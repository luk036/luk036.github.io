layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

# Bayesian Optimization: Theory and Practice in Python ğŸ

@luk036 ğŸ‘¨ğŸ»â€ğŸ«

2025-05-08 ğŸ“…

---

## Credit ğŸ™

*   **Title:** Bayesian Optimization: Theory and Practice Using Python ğŸ
*   **Based on:** Excerpts from the book by Peng Liu ğŸ“–
*   **Subtitle:** A Sample-Efficient Approach to Global Optimization ğŸ¯

---

## What is Bayesian Optimization (BO)?

*   A **class of methodology** for sample-efficient global optimization. ğŸ”„
*   Seeks the **optimal solution as fast as possible** âš¡.
*   Aims to find the global maximum (or minimum) of an unknown **black-box function** ğŸ“¦.
*   Provides a **unified framework** for sequential decision-making under uncertainty. ğŸ§©
*   Involves iteratively probing the unknown function to guide the search. ğŸ”

---

## Why Use Bayesian Optimization?

*   **Sample Efficiency:** Finds optimal solutions with significantly fewer function evaluations compared to methods like grid search or random search. ğŸ“‰
*   **Handles Black-Box Functions:** Effective when the objective function is expensive to evaluate, lacks derivatives, or its structure is unknown. ğŸ¤–
*   **Quantifies Uncertainty:** Provides probabilistic estimates of the objective function, crucial for guiding the search. ğŸ²
*   **Sequential Decision Making:** Builds upon Bayesian Decision Theory principles. âš–ï¸
*   Applicable to various problems, like hyperparameter tuning in machine learning. ğŸ¤–ğŸ”§

---

## Core Components of BO

*   **Surrogate Model:** Approximates the unknown objective function. ğŸ—ï¸
    *   Provides mean prediction and uncertainty estimates. ğŸ“Š
    *   **Gaussian Processes (GP)** are a powerful and common choice. ğŸŒŸ
*   **Acquisition Function:** A utility function that guides the sequential search. ğŸ§­
    *   Decides the **next best location to sample** ğŸ“.
    *   Balances **exploration** (sampling where uncertainty is high) ğŸ—ºï¸ and **exploitation** (sampling near current best values) ğŸ’.
    *   Examples: Expected Improvement (EI), Knowledge Gradient (KG). ğŸ“ˆ

---

## Quick Detour: Bayesian Statistics Basics for BO

*   **Bayesian Inference:** Updating prior beliefs with observed data to obtain a **posterior distribution**. ğŸ”„
    *   Prior + Likelihood â†’ Posterior. â•â¡ï¸ğŸ“Š
    *   Posterior distribution reflects belief *after* seeing data. Approaches normal distribution with more data. ğŸ“‰
*   **Prior and Posterior Predictive Distributions:** Distributions for *new* observations. ğŸ”®
    *   Prior Predictive: Expected distribution of new data based on prior belief. ğŸ”®
    *   Posterior Predictive: Expected distribution of new data based on updated (posterior) belief. Crucial for reasoning about sampling choices. ğŸ¯

---

## The Bayesian Optimization Workflow

*   An iterative process. ğŸ”„
*   **Starts with:** Initial dataset (can be small, sometimes generated randomly or with space-filling designs like Sobol sequences). ğŸ²
*   **Main Loop:
    1.  **Update Surrogate Model:** Fit or update the GP model based on all collected data. ğŸ”„
    2.  **Optimize Acquisition Function:** Find the location(s) that maximize the acquisition function. This proposes the next sampling point(s). ğŸ“
    3.  **Probe Environment:** Evaluate the true black-box function at the proposed location(s). ğŸ”
    4.  **Augment Dataset:** Add the new observation(s) to the dataset. â•
*   Repeats until budget exhausted (e.g., number of iterations). â³

---

## Component 1: The Surrogate Model - Gaussian Processes (GP)

*   Models the objective function as a **sample from a distribution of functions**. ğŸ²
*   A **nonparametric model** with high expressive capacity.
*   Naturally quantifies **uncertainty**. â“
*   Defined by a **mean function** and a **covariance (kernel) function**. ğŸ“Š
*   The **kernel function** encodes prior assumptions about the function (e.g., smoothness). ğŸŒ€
    *   Examples: Squared Exponential (RBF), Matern, Linear, Periodic. ğŸ”„

---

## GP Theory: Mean & Covariance

*   A Gaussian process is an infinite collection of random variables, where any finite subset follows a **multivariate Gaussian distribution**. ğŸ“Š
*   The mean function specifies the expected value of the function at each point. ğŸ“
*   The covariance (kernel) function `k(x_i, x_j)` defines the covariance between function values at any two points `x_i` and `x_j`. â†”ï¸
    *   It measures their similarity based on distance in the input space. Larger distance usually means smaller covariance/similarity. ğŸ“

---

## GP Prior and Posterior

*   **GP Prior:** Represents initial belief about the function before seeing data. Defined by the chosen mean and kernel functions. Can sample functions from this prior. ğŸ²
*   **GP Posterior:** The updated belief after observing data. ğŸ”„
    *   Conditioned on observed data points. ğŸ“Œ
    *   Provides a posterior mean function (often passing through or near observed points) and a posterior covariance function (uncertainty is reduced near observed points). ğŸ“‰
    *   Uncertainty is typically visualized with **credible intervals** (e.g., 95%). ğŸ“Š

---

## GP in Practice: Implementing with GPyTorch

*   **GPyTorch:** A state-of-the-art GP library built on **PyTorch**. ğŸ—ï¸
*   Leverages PyTorch features: **GPU acceleration** ğŸš€ and **auto-differentiation** (`autograd`). ğŸ”„
*   Allows for efficient **exact or approximate GP inference**. ğŸ“Š
*   Serves as the **backbone for BoTorch**. ğŸ¦´

---

## GPyTorch: Model Building & Inference

*   Building a GP model involves defining mean, kernel functions, and a likelihood. ğŸ—ï¸
    *   Classes like `gpytorch.means.ConstantMean()` and `gpytorch.kernels.RBFKernel()`. ğŸ› ï¸
    *   Likelihood (`gpytorch.likelihoods.GaussianLikelihood()`) models observation noise. ğŸ”Š
*   An `ExactGP` model can be defined by inheriting from `gpytorch.models.ExactGP`. ğŸ§¬
*   Fitting the model (`train` mode) and making predictions (`eval` mode). ğŸ”„
*   Obtain mean, lower, and upper confidence bounds for plotting. ğŸ“Š

---

## GPyTorch: Kernels & Hyperparameter Optimization

*   Different kernel functions capture different function properties (smoothness, periodicity, etc.). ğŸŒ€
*   Kernels can be **combined** via addition and multiplication for more complex functions. â•âœ–ï¸
*   Kernel and noise parameters (hyperparameters) are crucial for a good fit. ğŸ”§
*   These can be optimized by maximizing the **marginal log likelihood (MLL)** of the observed data. ğŸ“ˆ
*   GPyTorch and BoTorch provide utilities (`ExactMarginalLogLikelihood`, `fit_gpytorch_model`) to automate this optimization using PyTorch's `autograd`. ğŸ¤–

---

## Component 2: The Acquisition Function (AF)

*   Calculates the **utility** of sampling at a particular location `x`. ğŸ’°
*   Higher AF value suggests a more promising location. ğŸ“ˆ
*   Balancing Exploration vs. Exploitation:
    *   **Exploration:** Sample in areas with high uncertainty (large credible interval). ğŸ—ºï¸
    *   **Exploitation:** Sample near current best observed value. ğŸ’
*   Maximizing the AF determines the **next sampling point**. This is the "inner loop" optimization. ğŸ”„
*   Policies can be myopic (one-step lookahead) or nonmyopic (multi-step lookahead). ğŸ‘ï¸

---

## Acquisition Function: Expected Improvement (EI)

*   A widely used and effective acquisition function. ğŸ†
*   Measures the **expected gain** in the objective value compared to the **current best observed value**. ğŸ“ˆ
*   Has an **analytic, closed-form expression** in the noiseless setting. âœï¸
*   Calculates the expectation of `max(f(x) - f_best, 0)` under the GP posterior. ğŸ§®

---

## Implementing EI with Libraries

*   Can be implemented from scratch using NumPy/SciPy. ğŸ› ï¸
*   Libraries like `scikit-optimize` offer built-in `gp_minimize` function that uses EI. ğŸ“¦
*   **BoTorch** provides the `ExpectedImprovement` class for analytic EI and `qExpectedImprovement` for Monte Carlo (MC) EI. ğŸ²
*   Optimizing the acquisition function in BoTorch is done using `optimize_acqf`. ğŸ”„

---

## Monte Carlo (MC) Acquisition Functions

*   Used when acquisition function expectation **lacks a closed-form solution**. âŒ
*   Approximates the expectation operator (an integral) via **Monte Carlo simulation**. ğŸ²
*   Draws samples from the GP posterior predictive distribution and averages a function of these samples. ğŸ“Š
*   Example: **Monte Carlo Expected Improvement (MC EI)** or `qExpectedImprovement` in BoTorch. ğŸ“ˆ

---

## BoTorch: Efficient Optimization Tools

*   Finding the maximum of the acquisition function is a non-convex optimization problem. ğŸ”ï¸
*   BoTorch's `optimize_acqf` uses **multi-start optimization**. ğŸ”„
    *   Starts gradient ascent from multiple initial conditions. ğŸ“
*   Initial conditions for multi-start can be generated using **Sobol sequences**. ğŸ²
    *   Provide a **space-filling design** for better coverage than uniform random sampling. ğŸ—ºï¸
*   Leverages PyTorch's `autograd` for gradient calculation. ğŸ“Š

---

## Acquisition Function: Knowledge Gradient (KG)

*   A **nonmyopic** acquisition function. ğŸ‘ï¸
*   Quantifies the **expected increase in the maximum posterior mean** after taking a new observation. Represents the **value of information**. ğŸ’¡
*   Original formulation involves **nested optimization**. ğŸ—ï¸
    *   Requires optimizing over future ("fantasy") values. Computationally expensive. â³
*   Can be estimated using Monte Carlo simulations and gradient ascent. ğŸ²

---

## Knowledge Gradient: One-Shot KG (OKG)

*   Addresses the computational burden of nested KG. âš¡
*   Uses **Sample Average Approximation (SAA)** to convert the nested problem into a single, higher-dimensional deterministic optimization. ğŸ”„
*   Approximates KG using a **fixed set of base samples** (fantasy points). ğŸ²
*   Formulated as a single optimization problem over current and future ("fantasy") locations. ğŸ“
*   Implemented as `qKnowledgeGradient` in BoTorch. ğŸ“¦

---

## Case Study 1: Global Optimization of Hartmann Function

*   Problem: Seek the global maximum of the 6-dimensional Hartmann function. ğŸ¯
*   Demonstrates the **full BO loop** in practice. ğŸ”„
*   Steps: Generate initial data, initialize/update GP posterior, create MC acquisition functions (qEI, qKG), run the iterative BO loop. ğŸ“Š
*   Compare performance of qEI, qKG, and Random Search. ğŸ“‰
*   Shows how BO policies effectively find the optimum compared to random search. (Source suggests qKG is better than qEI for Hartmann). ğŸ†

---

## Case Study 2: Tuning CNN Learning Rate

*   Problem: Find the optimal learning rate for a CNN classifying MNIST digits. ğŸ¤–
*   Hyperparameter tuning treated as a global optimization problem. ğŸ¯
*   Involves: Loading data (MNIST with torchvision), defining CNN model (PyTorch), defining training/testing procedures.
*   Applies the **full BO loop** using qEI, qKG, and Random Search to the learning rate search space (typically log-scaled). ğŸ“Š
*   Demonstrates practical application of BO to a common ML task. (Source suggests qEI performed best for this task, but variability is high). ğŸ“ˆ

---

## Summary and Key Takeaways

*   Bayesian Optimization is a powerful **sample-efficient** approach for expensive black-box global optimization. ğŸ¯
*   Core idea: Build a **probabilistic surrogate model** (like GP) and use an **acquisition function** (like EI or KG) to guide the search sequentially. ğŸ§­
*   Libraries like **GPyTorch** and **BoTorch** provide the necessary tools for practical implementation in Python. ğŸ› ï¸
*   Advanced techniques (MC, OKG, multi-start optimization with Sobol sequences) improve computational efficiency and performance. âš¡
*   BO is versatile, applicable to synthetic problems and real-world tasks like hyperparameter tuning. ğŸŒ

---

count: false
class: nord-dark, middle, center

.pull-left[

# Q & A ğŸ¤

] .pull-right[

![Discussion](figs/questions-and-answers.svg)

]