layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

# Reinforcement Learning ðŸ¤–

@luk036 ðŸ‘¨ðŸ»â€ðŸ«

2025-05-16 ðŸ“…

---

## Introduction to Reinforcement Learning

*   Welcome! Today, we'll explore **Reinforcement Learning (RL)**. ðŸ‘‹
*   RL is fundamentally different from **supervised learning**. ðŸ”„
*   In supervised learning, agents learn a mapping from inputs to outputs using *provided training data* with known input and output values. A "teacher" or database defines the desired mapping. ðŸŽ“
*   In RL, the situation is more difficult because **no such training data is available**. âŒðŸ“Š

---

## What is Reinforcement Learning?

*   The agent learns through **trial and error (or success)** which actions are good in a certain situation. ðŸ”„ðŸŽ¯
*   It learns through **reinforcement**. ðŸ’ª
*   Successful attempts are **rewarded**. ðŸ†
*   Unsuccessful attempts are **penalized**. âŒ
*   This is similar to how humans learn, e.g., a child learning to walk or learning in sports. ðŸ‘¶ðŸš¶â€â™‚ï¸âš½
*   Positive and negative reinforcement are key factors. âž•âž–

---

## RL vs. Supervised Learning Summary

*   **Supervised Learning:**
    *   Mapping inputs to outputs. â†”ï¸
    *   Requires *complete training data* (input + desired output). ðŸ“‚
    *   Goal: Approximate the mapping and generalize. ðŸŽ¯
*   **Reinforcement Learning:**
    *   Finding good actions in a situation. ðŸ•µï¸â€â™‚ï¸
    *   *No training data* is initially available. âŒðŸ“Š
    *   Learns through *interaction* and *feedback* (rewards/penalties). ðŸ”„ðŸ’¬
    *   Goal: Learn a **policy** that maximizes long-term reward. ðŸ†ðŸ“ˆ

---

## A Robotics Example ðŸ¤–ðŸš¶â€â™‚ï¸

*   An illustrative example comes from robotics.
*   Consider a robot with a rectangular block body and an arm with two joints (gy and gx).
*   Actions are discrete movements of the joints (e.g., up/down for gy, right/left for gx). â¬†ï¸â¬‡ï¸â¬…ï¸âž¡ï¸
*   The task is to learn a **policy** to move **as quickly as possible to the right**. ðŸš€âž¡ï¸
*   This robot works analogously to a walking robot in the same two-dimensional state space.
*   Movements to the right are **rewarded** positively, movements to the left are **punished** negatively. âž•âž–

---

## Modeling the Task Mathematically ðŸ“

*   The robot's **state** is described by the position of its two joints, (gx, gy). Each joint has a finite number of discrete values. ðŸ”¢
*   The state is encoded as a vector (gx, gy).
*   The number of possible positions for gx is nx, and for gy is ny.
*   The **evaluation** of actions is based on the robot's horizontal â†”ï¸ position. Positive changes to x (movement right) are rewarded, negative changes (movement left) are penalized. âž•âž–
*   Figure 10.3 shows state spaces for 2x2 and 4x4 joint positions.

---

## Agent and Environment ðŸŒ

*   We distinguish between the **agent** and its **environment**. ðŸ¤–ðŸŒ¿
*   At time t, the world (agent + environment) is in a **state** st âˆˆ S. â±ï¸
*   S is an abstraction; the agent may have incomplete information due to measurement errors.
*   The agent performs an **action** at âˆˆ A at time t. âš¡
*   This action changes the world, resulting in state st+1 at time t+1. ðŸ”„
*   The state transition is determined by the environment's function Î´: st+1 = Î´(st, at). This is outside the agent's influence. ðŸŒ

---

## Reward and Policy â­ðŸ—ºï¸

*   After executing action at, the agent receives an **immediate reward** rt = r(st, at). ðŸ’°
*   The immediate reward depends on the current state st and executed action at.
*   rt > 0 means positive reinforcement; rt < 0 means negative reinforcement for action at in state st. rt = 0 means no immediate feedback. âž•âž–ðŸš«
*   A **policy Ï€ : S â†’ A** is a mapping from states to actions. It tells the agent what action to take in each state. ðŸ—ºï¸
*   The **goal of RL** is for the agent to learn an **optimal policy** based on its experiences. ðŸ†

---

## Maximizing Reward (Value Function) ðŸ“ˆ

*   An optimal policy maximizes reward **in the long run**, over many steps. â³
*   We define the **value** or **discounted reward** V Ï€(st) of a policy Ï€ starting in state st:
    *   V Ï€(st) = rt + Î³rt+1 + Î³Â²rt+2 + ... = âˆ‘ Î³^i * rt+i
    *   Here, **0 â‰¤ Î³ < 1** is a discount factor.
    *   **Î³** ensures future rewards are discounted; immediate reward is weighted strongest. âš–ï¸
*   An alternative is the **average reward** (though discounted is more common).

---

## Optimal Policy and Markov Decision Processes (MDP) âœ¨

*   A policy Ï€ is **optimal** if V Ï€(s) â‰¥ V Ï€(s) for all states s. It's at least as good as all other policies. ðŸ†
*   The optimal value function is denoted V.
*   The agents discussed typically use only information about the **current state st** to determine the next action, not prior history. â±ï¸
*   This is justified if reward depends only on the current state and action.
*   Processes with this property are called **Markov decision processes (MDP)**. ðŸ”„
*   If the agent's actual state is not exactly known (e.g., due to noisy sensors), it's a **partially observable Markov decision process (POMDP)**. â“

---

## Solving RL: Combinatorial Search? ðŸ¤”

*   The simplest approach to finding a successful policy is **combinatorial enumeration of all policies**. ðŸ”
*   However, even in the simple robot example (Example 10.1), there are **very many policies**.
*   The number of policies grows **exponentially** with the number of states if there's more than one possible action per state. ðŸ“ˆ
*   This makes combinatorial search associated with **extremely high computational cost** and impractical for practical applications. â³ðŸ’»
*   Heuristic search is also difficult because immediate reward is often zero and cannot serve as a heuristic. âŒ

---

## Solving RL: Value Iteration (Dynamic Programming) ðŸ§ 

*   The naive approach performs redundant work; many policies are similar. ðŸ”„
*   This suggests saving intermediate results for parts of policies and reusing them. ðŸ’¾
*   This is the approach of **dynamic programming**, introduced by Richard Bellman.
*   Bellman's principle: For an optimal policy, **all subsequent decisions must be optimal**, independent of the starting state and first action. ðŸ†
*   This principle allows finding a globally optimal policy through **local optimization** of individual actions. ðŸŽ¯

---

## The Bellman Equation for V ðŸ“œ

*   We want an optimal policy Ï€ that satisfies the optimal value definition.
*   This leads to a recursive characterization of the optimal value function V:
    *   **V(s) = max_a [r(s, a) + Î³V(Î´(s, a))]**
*   This is the **Bellman equation**.
*   It means that to calculate V(s), you add the immediate reward r(s, a) to the discounted optimal value of the successor state V(Î´(s, a)) for the *best possible action* 'a' in state 's'. âž•ðŸ’°

---

## Value Iteration Algorithm âš™ï¸

*   The Bellman equation (10.6) leads to an iteration rule for approximating V:
    *   **VÌ‚(s) = max_a [r(s, a) + Î³VÌ‚(Î´(s, a))]**
*   **Algorithm:**
    1.  Initialize approximate values VÌ‚(s) for all states (e.g., to zero). ðŸ
    2.  Repeatedly update VÌ‚(s) for each state using the iteration rule, relying on the VÌ‚ values of successor states. ðŸ”„
*   This process is called **value iteration**.
*   Value iteration **converges to V**. âœ…
*   Figure 10.7 shows the schematic algorithm.

---

## Value Iteration Example âœ¨

*   Figure 10.8 shows value iteration applied to the robot example with Î³ = 0.9 and 3x3 states.
*   The sequence shows the progression of learning. ðŸ“ˆ
*   The agent explores states, performs value iteration, and saves the policy via a tabulated V function.
*   To find the optimal action Ï€(s) from V, you use:
    *   **Ï€(s) = argmax_a [r(s, a) + Î³V(Î´(s, a))]**
    *   You **must** add the immediate reward r(s, a), not just choose the action leading to the state with maximum V value. ðŸ’°
*   Value iteration requires a **model of the environment** (knowing r and Î´ functions). ðŸŒ

---

## Robot Simulation & Real Robots ðŸ¤–ðŸ–¥ï¸

*   Simulators can be used to experiment with reinforcement learning.
*   A walking robot simulator allows observing learning, manually supplying feedback, and setting parameters. ðŸŽ®
*   Real crawling robots were also developed for teaching.
*   Learning can happen in simulation (faster) and then be transferred to the robot, or the robot can learn autonomously. ðŸ”„
*   A 5x5 state space takes about 30s to learn autonomously on the real robot. â±ï¸
*   **Differences exist between simulation and real robots**.
*   Real robots perceive subtle environmental effects (like grip on surface) through sensors, leading to different, often more efficient, learned policies. ðŸŒ
*   Real robots can show **adaptivity**, learning despite defects or changing environments. ðŸ”§

---

## Q-Learning: Model-Free RL ðŸŒ±

*   What if the agent **doesn't have a model** of the world (doesn't know r and Î´)? â“
*   This is common in robotics; a robot might not know the exact outcome of an action.
*   We need to evaluate an action *without* knowing the successor state.
*   We use an evaluation function **Q(s, a)** for state-action pairs.
*   The optimal action in state 's' is chosen by **Ï€(s) = argmax_a Q(s, a)** for an optimal function Q. ðŸ†

---

## The Q-Function and Bellman Equation for Q ðŸ§®

*   Similar to V, the value of action 'a' in state 's' is defined using discounted future rewards:
    *   **Q(st, at) = r(st, at) + Î³rt+1 + Î³Â²rt+2 + ...**
*   The optimal Q function, Q, is defined recursively:
    *   **Q(s, a) = r(s, a) + Î³ max_aâ€² Q(Î´(s, a), aâ€²)**
*   This looks similar to the Bellman equation for V, but the key advantage is that Q directly evaluates state-action pairs.
*   Knowing Q allows the agent to choose actions **without a model** of Î´ and r, simply by picking the action with the highest Q value in the current state. ðŸ†

---

## Q-Learning Algorithm ðŸ”„

*   An iterative algorithm for determining Q(s, a) is derived from its recursive form.
*   **QÌ‚(s, a) = r(s, a) + Î³ max_aâ€² QÌ‚(Î´(s, a), aâ€²)**
*   **Algorithm:**
    1.  Initialize a table QÌ‚(s, a) for all state-action pairs (e.g., with zeroes). ðŸ
    2.  Let the agent execute action 'a' in state 's' in the environment.
    3.  Observe the successor state s' = Î´(s, a) and the immediate reward r(s, a) from the environment. ðŸ‘€
    4.  Update QÌ‚(s, a) using the observed r and the *current* QÌ‚ value of the best action in the successor state s'. ðŸ”„
*   Figure 10.11 shows the schematic algorithm.

---

## Q-Learning Convergence (Deterministic) âœ…

*   Theorem 10.1 states that for a **deterministic MDP** with bounded reward and 0 â‰¤ Î³ < 1, QÌ‚n(s, a) **converges to Q(s, a)** for all s and a as n â†’ âˆž.
*   This convergence is guaranteed **if each state-action pair is visited infinitely often**. ðŸ”„
*   The proof shows that the maximum error in the QÌ‚ table is reduced by at least the factor Î³ in intervals where all state-action transitions occur at least once.
*   Thus, the error converges to zero.
*   Convergence is independent of the specific actions chosen *during learning*, as long as they are all visited infinitely often. The *speed* of convergence *does* depend on the paths taken. â±ï¸

---

## Q-Learning in Nondeterministic Environments ðŸŽ²

*   Many robotics environments are **nondeterministic**. An action in a state can lead to different successor states or rewards at different times.
*   A nondeterministic Markov process has probabilistic transition P(s'|s, a) and reward r(s, a).
*   Equation (10.16) is generalized by using the **expected value** over successor states:
    *   Q(s, a) = E(r(s, a)) + Î³ âˆ‘ sâ€² P(sâ€²|s, a) max_aâ€² Q(sâ€², aâ€²)
*   Directly using (10.17) in a nondeterministic environment does **not guarantee convergence**. Successive runs might give different rewards/successor states, causing values to jump. âŒ

---

## Stabilized Q-Learning (TD-Learning)

*   To stabilize Q values in nondeterministic environments, the iteration rule adds the old Q value, weighted by (1 - Î±n).
*   The learning rule becomes:
    *   **QÌ‚n(s, a) = (1 âˆ’ Î±n)QÌ‚nâˆ’1(s, a) + Î±n [ r(s, a) + Î³ max_aâ€² QÌ‚nâˆ’1(Î´(s, a), aâ€²) ]**
*   Here, Î±n is a time-varying weighting factor, e.g., Î±n = 1 / (1 + bn(s, a)), where bn(s, a) is how often action 'a' was executed in state 's'.
*   This stabilizes iteration, preventing excessive jumps in QÌ‚ values, especially later in learning when bn is large.
*   This equation can be rewritten as:
    *   QÌ‚n(s, a) = QÌ‚nâˆ’1(s, a) + Î± [ TD-error ]
*   The term [ r(s, a) + Î³ max_aâ€² QÌ‚nâˆ’1(Î´(s, a), aâ€²) âˆ’ QÌ‚nâˆ’1(s, a) ] is the **TD-error** (temporal difference error).
*   This update rule is a special case of **TD-Learning**. Î±=1 is the basic Q-learning; Î±=0 means no learning. ðŸ”„

---

## Exploration vs. Exploitation ðŸ§­ðŸ’°

*   How should the agent choose actions during learning?
*   **Possibility 1: Exploration**
    *   Choose actions randomly. ðŸŽ²
    *   Results in uniform exploration of possibilities.
    *   **Slow convergence**. Guarantees visiting all state-action pairs infinitely often (required for convergence). â³
*   **Possibility 2: Exploitation**
    *   Always choose the action with the highest *current* QÌ‚ value. ðŸ†
    *   Results in relatively **fast convergence of a specific trajectory**.
    *   Other paths may remain unvisited, potentially leading to **non-optimal policies**. âŒ
*   **Recommendation:** Use a **combination**, with high exploration initially, gradually reducing it over time. ðŸ”„

---

## Approximation and Generalization ðŸ§ âœ¨

*   Q-learning (and value iteration) as described use tables to store Q or V values. This only works for **finite, discrete state-action spaces**.
*   If the state space is **infinite** (e.g., continuous variables), tables are impossible, and visiting all pairs is impossible.
*   Solution: Replace the Q(s, a) table with a **function approximator**.
*   A **neural network** (e.g., backpropagation network) is a common choice. Input is (s, a), target output is the Q value. ðŸ§ 
*   Training examples update the network. Since there are finitely many examples for infinitely many inputs, this provides **generalization**.
*   Other approximators like SVMs or Gaussian processes can also be used.

---

## Challenges with Function Approximation & POMDPs ðŸš§

*   Q-learning with function approximation might **not converge**, as Theorem 10.1 requires visiting all state-action pairs infinitely often (which is impossible with continuous spaces). âŒ
*   Convergence problems can also occur with **POMDPs** (Partially Observable MDPs).
*   In a POMDP, the agent perceives multiple states as one due to noisy sensors or intentional mapping to 'observations'.
*   This mapping can reduce state space and prevent overfitting, but the agent cannot differentiate actual states within an observation.
*   An action in an observation might lead to different successor states depending on the true underlying state, causing convergence issues for value iteration or Q-learning.

---

## Alternative Approaches: Policy Improvement ðŸ’ª

*   **Policy improvement methods** and **policy gradient methods** are alternative approaches.
*   Instead of learning values (V or Q), they directly **change the policy**.
*   The goal is to find a policy in the space of all policies that maximizes the cumulative discounted reward (Eq. 10.1).
*   One way is to follow the **gradient of the cumulative reward** to a maximum. ðŸ“ˆ
*   This can significantly **speed up learning** in applications with large state spaces, like humanoid robots.

---

## Applications of Reinforcement Learning ðŸŽ®âš½âœˆï¸ðŸš—

*   RL has demonstrated practical utility many times.
*   **TD-Gammon:** A backgammon program using TD-learning and a neural network. Trained against itself, it defeated world-class players. â™Ÿï¸ðŸ†
*   **Robotics:** Used successfully in RoboCup Soccer Simulation League. Balancing a pole. ðŸ¤–âš½
*   **Model Airplane Landing:** A model airplane learned to land precisely, handling turbulent air currents (which are hard to model mathematically). As the quote says, "Birds don't solve Navierâ€“Stokes!" âœˆï¸ðŸ¦
*   **Car Control:** Learning to control a real car in 20 minutes using Q-learning and function approximation. Applicable to real industrial tasks with few measurements. ðŸš—
*   **Challenges in Real Robots:** Learning is slower than in simulation due to real-world feedback speed. Millions of training cycles are time-consuming. Need methods for offline learning. â³

---

## AlphaGo: A Breakthrough in Go â™Ÿï¸ðŸ¤¯

*   Go has a huge branching factor, making traditional search methods difficult.
*   **AlphaGo (Google DeepMind)** achieved a breakthrough.
*   It combines **Monte Carlo Tree Search (MCTS)** for generating data, **deep learning** for evaluating positions, and **reinforcement learning** for strategy improvement.
*   Learning goal: Learn a **policy p(a|s)** calculating the probability of winning for each move 'a' in position 's'. The best move is argmax_a p(a|s).
*   **Stage 1: Deep Learning** based on expert games (KGS Go Server). A CNN learns pÏƒ(a|s), predicting expert moves. It predicted expert moves up to 57% of the time. A simpler rollout policy pÏ€(a|s) was also trained.

---

## AlphaGo: Stages 2 & 3 and Final Play ðŸ†

*   **Stage 2: Reinforcement Learning** to improve the policy pÏƒ(a|s). AlphaGo plays against itself; the policy pÏ(a|s) is improved using stochastic gradient descent. Plays against earlier versions to avoid overfitting.
*   **Stage 3: Learning Value Function**. The policy pÏ(a|s) trains a board state evaluation function V(s) using value iteration.
*   **Final Play:** Uses a complex **MCTS algorithm**. Tree is expanded from the current position. Games are played to the end from leaf nodes using the fast rollout policy pÏ€(a|s). Position is evaluated using the game outcome + value function V(s).
*   Required massive computation (1202 CPUs, 176 GPUs) to defeat Lee Sedol.
*   AlphaGo was a major milestone, enabled by deep learning, RL, and engineering effort.

---

## AlphaGoZero: Learning from Scratch ðŸš€

*   **AlphaGoZero** learns Go completely **independently**, without human knowledge or expert games.
*   It learns by playing against itself.
*   Uses deep learning, reinforcement learning, and MCTS.
*   A single deep network outputs both move probabilities and win probability.
*   This network is trained using a new RL algorithm performing MCTS searches in games against itself to improve move probabilities. Chosen moves become stronger over time.

---

## The Curse of Dimensionality

*   RL remains challenging, especially for **high-dimensional state and action spaces**.
*   The huge computation time needed is known as the **"curse of dimensionality"**.
*   How to address this? Look to nature. ðŸŒ¿
*   Learning in nature happens on **multiple levels of abstraction** (hierarchical learning). Learned skills are encapsulated and used as higher-level actions, reducing the action space. States can also be abstracted.

---

## Overcoming the Curse: Modular & Initial Policies ðŸ§©ðŸ—ºï¸

*   **Distributed Learning / Multi-Agent Learning:** Complex control tasks (like a robot with many motors) can be broken down. Individual parts (e.g., each motor) get individual control, ideally independent of others, reducing complexity. Seen in insects (millipede legs).
*   **Starting with a Good Initial Policy:** The task is easier if the agent starts with a somewhat good policy.
    *   **Classical Programming:** Programmer provides an initial policy (a program). The agent follows this, leading it to "interesting" state-action areas, dramatically reducing the search space.
    *   **Demonstration Learning:** A human "proscribes" (teaches) the actions, e.g., via remote control. The robot saves state-action pairs and generalizes using supervised learning (backpropagation, decision trees). This provides an initial policy for subsequent RL.

---

## Summary and Outlook âœ¨ðŸ”®

*   Established learning algorithms like Value Iteration and Q-Learning exist.
*   Training for complex RL applications is still demanding, requiring experimentation. Tools like the Teaching-Box can help by integrating various learning algorithms, traditional programming, and demonstration learning.
*   RL is an **active and fascinating research area** with growing use.
*   Scaling to **high-dimensional state and action spaces** remains a significant challenge (curse of dimensionality).
*   Impressive results are achieved for small state/action spaces, but tasks with many degrees of freedom (like humanoid robots) are still very expensive to learn.
*   Many variations and new algorithms exist beyond those presented.
*   Further reading: Mitchell [Mit97], Sutton and Barto [SB98], Kaelbling, Littman, and Moore [KLM96].

---

count: false
class: nord-dark, middle, center

.pull-left[

# Q & A ðŸŽ¤

] .pull-right[

![Discussion](figs/questions-and-answers.svg)

]
