layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

# Energy-Efficient Neural Computing with Approximate Multipliers âš¡

@luk036 ğŸ‘¨â€ğŸ’»

2025-05-18 ğŸ“…

---

## Credit

*   Syed Shakib Sarwar, Swagath Venkataramani, Aayush Ankit, Anand Raghunathan, and Kaushik Roy ğŸ‘¨â€ğŸ’»
*   Purdue University ğŸ«
*   Based on J. Emerg. Technol. Comput. Syst. Article 16 (July 2018) ğŸ“š

---

## Introduction: The Challenge of Neural Networks ğŸ¤”ğŸ’­

*   Neural networks excel at deriving meaning from large, complicated data and extracting complex patterns. ğŸ§ 
*   They are used for tasks like recognition, classification, inference, pattern and sequence recognition, filtering, clustering, and robotics. ğŸ”
*   Examples include Google Image search, Google Now speech recognition, Apple's Siri, and Google Street View. ğŸŒ
*   Their computational requirements are considerable, stretching the capabilities of modern computing platforms. âš¡
*   Deep Learning Networks (DLNs), a descendant of ANNs, are seen as transformative for next-generation embedded devices. ğŸš€

---

## Hardware Implementation Challenges ğŸ”¥ğŸ”ŒğŸ’»

*   Due to compute-intensive workloads, hardware implementations of neuromorphic architectures are inefficient in terms of power consumption and area. â³
*   Traditional hardware approaches have explored altering network architecture or using emerging device technologies like memristors, phase-change memory, resistive RAM, and spin-based devices. ğŸ› ï¸
*   The testing phase of ANNs, though less compute-intensive than training (done off-line), requires significant computation on-chip for large networks.
*   The testing process involves multiplication, summation, and activation operations. â—
*   The most power-consuming operation is multiplication, which far outweighs summation and activation. âš¡
*   Specifically, the multipliers in neurons that multiply inputs and corresponding synaptic weights are the major power-hungry components of an ANN. ğŸ”Œ

---

## Exploiting Error Resilience âœ¨ğŸ›¡ï¸ğŸ”‹

*   Fortunately, neural networks and their applications exhibit intrinsic resilience to errors. ğŸ¯
*   This makes them appropriate candidates for approximate computations. âœ…
*   Exploiting this resilience, various approximate hardware and software techniques have been proposed to achieve computational efficiency. âš™ï¸
*   One technique is the reduction of bit precision for computation and storage. ğŸ”¢
*   Studies show NNs can function satisfactorily with 8- or 16-bit fixed-point numbers instead of 32- or 64-bit floating-point math. Google's TPU focuses on 8-bit integer math. ğŸ“Š
*   This work aims to go beyond 8 bits, proposing 12-, 8-, and even 4-bit neurons. ğŸ¯

---

## Proposed Solution: Approximate Multipliers ğŸ› ï¸ğŸ”¢â—

*   To address the issue of power-hungry multipliers, we propose an Alphabet Set Multiplier (ASM), which is approximate. ğŸ’¡
*   Unlike some other approximate multipliers, our approach applies uniform approximation throughout the network. ğŸ”„
*   ASM replaces conventional multiplication with simplified shift and add operations. âš™ï¸
*   The concept of Computation Sharing Multiplication (CSHM) is also utilized in conjunction with ASM for energy efficiency. ğŸ”—

---

## How ASM Works ğŸ—ï¸ğŸ”„âš™ï¸

*   A multiplication operation (like W Ã— I) can be decomposed into simple shift and add operations based on the synaptic weight "W". âœ–ï¸
*   This decomposition uses small bit sequences called alphabets (e.g., 0001â‚‚, 0011â‚‚, 0101â‚‚, 1011â‚‚) multiplied with the input "I". ğŸ”¤
*   Instead of conventional multiplication, ASM uses shifted and added lower-order multiples of the input. ğŸ”„
*   An ASM consists of: ğŸ§©
    *   A pre-computer bank that generates products of the input and pre-specified alphabets (the alphabet set, e.g., {1,3,5,...}). ğŸ¦
    *   "Select" units to choose a product from the bank. ğŸšï¸
    *   "Shift" units to shift the selected product. â©
    *   An "adder" to sum shifted products. â•
    *   "Control logic" to manage select, shift, and add operations based on the multiplicand. ğŸ›ï¸
*   For performing a general multiplication with a 4-bit sequence size, an alphabet set of eight alphabets {1,3,5,7,9,11,13,15} is required. ğŸ”¢

---

## ASM for Energy Benefits & Approximation ğŸ“‰âš¡ğŸ”‹

*   Energy benefits are achieved by using a reduced number of alphabets (less than eight) in the ASM pre-computer bank. This reduces power dissipation and routing complexity. ğŸ’°
*   Using fewer alphabets means the ASM may not support all multiplication combinations, introducing approximations. â“
*   For example, a four-alphabet {1,3,5,7} ASM for 4-bit sequences can generate 12 out of 16 combinations but cannot produce products involving 9, 11, 13, 15 as quartets from weights. ğŸ“
*   The pre-computer bank can be shared between multiple ASM units (e.g., four units processing the same input with different weights), utilizing the CSHM architecture. ğŸ”„

---

## Addressing Accuracy Loss: Weight Constraints & Retraining ğŸ¯ğŸ”„ğŸ¯

*   To guarantee proper functioning of the neural network despite unsupported multiplication combinations, it must be ensured these combinations do not lead to significant errors. ğŸ¯
*   This is achieved by introducing constrained training. âš–ï¸
*   Synaptic weights corresponding to unsupported combinations are restricted to the nearest supported values (e.g., 9, 11, 13, 15 restricted to 8, 10, 12, 14 for a {1,3,5,7} ASM). This is similar to quantization. ğŸ”„
*   This restriction results in some accuracy loss. ğŸ“‰
*   To recover lost accuracy, the network is retrained with the imposed constraints. ğŸ”„
*   The retraining overhead is marginal compared to the original training without constraints. â³

---

## Retraining Methodology ğŸ“ˆğŸ“ŠğŸ”„

*   The overall process involves: ğŸ”„
    1.  Train NN without constraints until near saturation. 1ï¸âƒ£
    2.  Test network accuracy (J) and create a restore point. 2ï¸âƒ£
    3.  Retrain network with constraints (starting with minimum alphabets) and a lower learning rate until near saturation. 3ï¸âƒ£
    4.  Test retrained network accuracy (K). If K â‰¥ J Ã— Q (quality constraint), end training. Else, restore point and repeat with more alphabets. 4ï¸âƒ£
*   Precise adjustment of the learning rate is crucial during retraining due to the non-uniform distance between allowed weight levels caused by approximations. ğŸ›ï¸
*   Too low a learning rate can cause weights to get stuck; too high can cause oscillation and accuracy deterioration. â¬
*   For aggressive bit precision scaling (e.g., 4 bits), assisted training is used: train with high precision, then round down during retraining with an increased learning rate. ğŸš€

---

## Multiplier-Less Artificial Neuron (MAN) ğŸ’¡âš™ï¸â–

*   Based on accuracy results, it was observed that accuracy is still within âˆ¼0.5% of conventional implementations even with only one alphabet {1} in all layers. ğŸ“Š
*   Using just the alphabet {1} means the input itself is sufficient, eliminating the need to generate other alphabet sets. ğŸ”¢
*   This removes the need for multiplication in the traditional sense, requiring only shifting and adding. âœ–ï¸
*   Consequently, the pre-computer bank and alphabet "select" unit are eliminated. ğŸ—ï¸
*   This leads to a "Multiplier-less" neuron (MAN). ğŸ§ 
*   Figure 8 in the source illustrates an 8-bit one-alphabet {1} ASM (MAN). ğŸ“Š

---

## Contracted Multiplier-Less Artificial Neuron (CMAN) ğŸ¤ğŸ’¨âš¡

*   Reducing synaptic weight and input bit width to 4 bits allows using the Alphabet Set Multiplier with a 4-bit sequence size, removing the need for the final addition step. ğŸ”¢
*   Combining 4-bit synapse precision with only one alphabet {1} means only shifting is needed â€“ the pre-computer bank and select unit are eliminated. ğŸ”„
*   This design represents the most simplified multiplier for artificial neurons. âš¡
*   This is termed the Contracted Multiplier-Less Artificial Neuron (CMAN). ğŸ·ï¸
*   For MNIST, using 4-bit synapse and one alphabet {1}, accuracy is within âˆ¼1% of conventional implementation. ğŸ¯
*   CMAN is faster, more compact, and consumes less power. âš¡

---

## Evaluation & Results: Accuracy

*   The proposed approach was evaluated on various benchmark applications and datasets like MNIST, SVHN, TiCH, CIFAR10, and CIFAR100. ğŸ§ª
*   Accuracy is compared to conventional multiplier-based neurons and normalized to a 64-bit fully accurate NN implementation. ğŸ¯
*   For ANNs (MNIST, SVHN, TiCH), maximum accuracy loss compared to conventional neurons of equivalent bit precision was: ğŸ“‰
    *   âˆ¼0.63% for 12-bit.
    *   âˆ¼0.84% for 8-bit.
    *   âˆ¼2.4% for 4-bit.
*   The 4-bit one-alphabet {1} version (CMAN) gave 96.31% accuracy for MNIST.
*   CNNs on CIFAR10/100 also performed well for 12- and 8-bit, with higher degradation for 4-bit. ğŸ–¼ï¸
*   Accuracy results for Face Detection and Digit Recognition are presented in Tables 2 and 3. ğŸ“‹

---

## Evaluation & Results: Power & Area âš¡ğŸ“‰ğŸ¢

*   Significant reductions in power consumption and area were observed compared to conventional neurons of equivalent bit precision under iso-speed conditions. ğŸ“‰
*   For MAN (one-alphabet {1}): âš¡
    *   Power Reduction: âˆ¼33% (12-bit), âˆ¼32% (8-bit), âˆ¼25% (4-bit).
    *   Area Reduction: âˆ¼33% (12-bit), âˆ¼34% (8-bit), âˆ¼27% (4-bit).
*   Using two alphabets {1,3} also provides benefits (e.g., up to âˆ¼16% power, âˆ¼18% area reduction). ğŸ”¢
*   ASM-based neurons can achieve slightly better maximum operational speed with much lower power and area than conventional neurons. âš¡

---

## Energy-Accuracy Trade-off âš–ï¸ğŸ“ŠğŸ”„

*   A central aspect of this work is demonstrating the trade-off between energy/area savings and accuracy loss. ğŸ”„
*   This trade-off depends on the application, neuron bit precision (12, 8, or 4 bits), and the number of alphabets used (conventional, 4, 2, or 1). ğŸ“Š
*   Generally, reducing bit precision and the number of alphabets increases energy/area benefits but also increases accuracy loss. âš–ï¸
*   12-bit neurons maintain better accuracy with fewer alphabets compared to 8- and 4-bit synapses due to greater flexibility. ğŸ¯
*   For NNs with neuron-size constraints (like 4-bit), using a mixed ASM approach (more alphabets in significant, smaller concluding layers) can improve accuracy with minimal energy overhead. ğŸ”„

---

## System-Level Benefits ğŸ’¾ğŸ’¡ğŸ–¥ï¸

*   System-level analysis considers both neuron computation energy and memory access energy. ğŸ¢
*   In systems with large on-chip memory sized for the largest network, memory energy dominates (e.g., 12x computation energy for 12-bit, 32x for 4-bit). ğŸ§ 
*   In such systems, computation energy reduction from approximate neurons translates to smaller overall system energy savings (1-2.5%). âš¡
*   However, for systems using compressed neural networks (e.g., 50x compression), memory is smaller and memory energy is less dominant. ğŸ”„
*   In these cases (memory sized for compressed networks), approximate neurons project higher system-level energy benefits (4â€“9%). ğŸš€
*   The approximate neuron design is compatible with hardware enhancements for compression. ğŸ”—
*   ASM-based neurons themselves do not change memory requirements compared to conventional neurons of the same bit precision. ğŸ’¾

---

## Conclusions âœ¨ğŸš€ğŸ¯

*   Deep Learning Networks are powerful but computationally demanding. ğŸ§ 
*   We exploited the inherent error resilience of NNs to design highly energy-efficient, approximate ASM-based neurons. âš¡
*   Introduced MAN (Multiplier-less Artificial Neuron), replacing conventional multiplication with only shift and add operations. ğŸ”„
*   Proposed aggressive bit precision scaling to 4 bits using assisted training and the CMAN concept for 4-bit neurons. ğŸ¯
*   A methodology for retraining approximate networks with weight constraints was developed to mitigate accuracy loss. ğŸ”„
*   Experimental results demonstrated significant improvements in energy consumption and reduction in area with only a negligible loss in classification accuracy. ğŸ“Š

---

count: false
class: nord-dark, middle, center

.pull-left[

## Q & A ğŸ¤

] .pull-right[

![Discussion](figs/questions-and-answers.svg)

]