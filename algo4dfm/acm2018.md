layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

# Energy-Efficient Neural Computing with Approximate Multipliers ⚡

@luk036 👨‍💻

2025-05-18 📅

---

## Credit

*   Syed Shakib Sarwar, Swagath Venkataramani, Aayush Ankit, Anand Raghunathan, and Kaushik Roy 👨‍💻
*   Purdue University 🏫
*   Based on J. Emerg. Technol. Comput. Syst. Article 16 (July 2018) 📚

---

## Introduction: The Challenge of Neural Networks 🤔💭

*   Neural networks excel at deriving meaning from large, complicated data and extracting complex patterns. 🧠
*   They are used for tasks like recognition, classification, inference, pattern and sequence recognition, filtering, clustering, and robotics. 🔍
*   Examples include Google Image search, Google Now speech recognition, Apple's Siri, and Google Street View. 🌍
*   Their computational requirements are considerable, stretching the capabilities of modern computing platforms. ⚡
*   Deep Learning Networks (DLNs), a descendant of ANNs, are seen as transformative for next-generation embedded devices. 🚀

---

## Hardware Implementation Challenges 🔥🔌💻

*   Due to compute-intensive workloads, hardware implementations of neuromorphic architectures are inefficient in terms of power consumption and area. ⏳
*   Traditional hardware approaches have explored altering network architecture or using emerging device technologies like memristors, phase-change memory, resistive RAM, and spin-based devices. 🛠️
*   The testing phase of ANNs, though less compute-intensive than training (done off-line), requires significant computation on-chip for large networks.
*   The testing process involves multiplication, summation, and activation operations. ➗
*   The most power-consuming operation is multiplication, which far outweighs summation and activation. ⚡
*   Specifically, the multipliers in neurons that multiply inputs and corresponding synaptic weights are the major power-hungry components of an ANN. 🔌

---

## Exploiting Error Resilience ✨🛡️🔋

*   Fortunately, neural networks and their applications exhibit intrinsic resilience to errors. 🎯
*   This makes them appropriate candidates for approximate computations. ✅
*   Exploiting this resilience, various approximate hardware and software techniques have been proposed to achieve computational efficiency. ⚙️
*   One technique is the reduction of bit precision for computation and storage. 🔢
*   Studies show NNs can function satisfactorily with 8- or 16-bit fixed-point numbers instead of 32- or 64-bit floating-point math. Google's TPU focuses on 8-bit integer math. 📊
*   This work aims to go beyond 8 bits, proposing 12-, 8-, and even 4-bit neurons. 🎯

---

## Proposed Solution: Approximate Multipliers 🛠️🔢➗

*   To address the issue of power-hungry multipliers, we propose an Alphabet Set Multiplier (ASM), which is approximate. 💡
*   Unlike some other approximate multipliers, our approach applies uniform approximation throughout the network. 🔄
*   ASM replaces conventional multiplication with simplified shift and add operations. ⚙️
*   The concept of Computation Sharing Multiplication (CSHM) is also utilized in conjunction with ASM for energy efficiency. 🔗

---

## How ASM Works 🏗️🔄⚙️

*   A multiplication operation (like W × I) can be decomposed into simple shift and add operations based on the synaptic weight "W". ✖️
*   This decomposition uses small bit sequences called alphabets (e.g., 0001₂, 0011₂, 0101₂, 1011₂) multiplied with the input "I". 🔤
*   Instead of conventional multiplication, ASM uses shifted and added lower-order multiples of the input. 🔄
*   An ASM consists of: 🧩
    *   A pre-computer bank that generates products of the input and pre-specified alphabets (the alphabet set, e.g., {1,3,5,...}). 🏦
    *   "Select" units to choose a product from the bank. 🎚️
    *   "Shift" units to shift the selected product. ⏩
    *   An "adder" to sum shifted products. ➕
    *   "Control logic" to manage select, shift, and add operations based on the multiplicand. 🎛️
*   For performing a general multiplication with a 4-bit sequence size, an alphabet set of eight alphabets {1,3,5,7,9,11,13,15} is required. 🔢

---

## ASM for Energy Benefits & Approximation 📉⚡🔋

*   Energy benefits are achieved by using a reduced number of alphabets (less than eight) in the ASM pre-computer bank. This reduces power dissipation and routing complexity. 💰
*   Using fewer alphabets means the ASM may not support all multiplication combinations, introducing approximations. ❓
*   For example, a four-alphabet {1,3,5,7} ASM for 4-bit sequences can generate 12 out of 16 combinations but cannot produce products involving 9, 11, 13, 15 as quartets from weights. 📝
*   The pre-computer bank can be shared between multiple ASM units (e.g., four units processing the same input with different weights), utilizing the CSHM architecture. 🔄

---

## Addressing Accuracy Loss: Weight Constraints & Retraining 🎯🔄🎯

*   To guarantee proper functioning of the neural network despite unsupported multiplication combinations, it must be ensured these combinations do not lead to significant errors. 🎯
*   This is achieved by introducing constrained training. ⚖️
*   Synaptic weights corresponding to unsupported combinations are restricted to the nearest supported values (e.g., 9, 11, 13, 15 restricted to 8, 10, 12, 14 for a {1,3,5,7} ASM). This is similar to quantization. 🔄
*   This restriction results in some accuracy loss. 📉
*   To recover lost accuracy, the network is retrained with the imposed constraints. 🔄
*   The retraining overhead is marginal compared to the original training without constraints. ⏳

---

## Retraining Methodology 📈📊🔄

*   The overall process involves: 🔄
    1.  Train NN without constraints until near saturation. 1️⃣
    2.  Test network accuracy (J) and create a restore point. 2️⃣
    3.  Retrain network with constraints (starting with minimum alphabets) and a lower learning rate until near saturation. 3️⃣
    4.  Test retrained network accuracy (K). If K ≥ J × Q (quality constraint), end training. Else, restore point and repeat with more alphabets. 4️⃣
*   Precise adjustment of the learning rate is crucial during retraining due to the non-uniform distance between allowed weight levels caused by approximations. 🎛️
*   Too low a learning rate can cause weights to get stuck; too high can cause oscillation and accuracy deterioration. ⏬
*   For aggressive bit precision scaling (e.g., 4 bits), assisted training is used: train with high precision, then round down during retraining with an increased learning rate. 🚀

---

## Multiplier-Less Artificial Neuron (MAN) 💡⚙️➖

*   Based on accuracy results, it was observed that accuracy is still within ∼0.5% of conventional implementations even with only one alphabet {1} in all layers. 📊
*   Using just the alphabet {1} means the input itself is sufficient, eliminating the need to generate other alphabet sets. 🔢
*   This removes the need for multiplication in the traditional sense, requiring only shifting and adding. ✖️
*   Consequently, the pre-computer bank and alphabet "select" unit are eliminated. 🏗️
*   This leads to a "Multiplier-less" neuron (MAN). 🧠
*   Figure 8 in the source illustrates an 8-bit one-alphabet {1} ASM (MAN). 📊

---

## Contracted Multiplier-Less Artificial Neuron (CMAN) 🤏💨⚡

*   Reducing synaptic weight and input bit width to 4 bits allows using the Alphabet Set Multiplier with a 4-bit sequence size, removing the need for the final addition step. 🔢
*   Combining 4-bit synapse precision with only one alphabet {1} means only shifting is needed – the pre-computer bank and select unit are eliminated. 🔄
*   This design represents the most simplified multiplier for artificial neurons. ⚡
*   This is termed the Contracted Multiplier-Less Artificial Neuron (CMAN). 🏷️
*   For MNIST, using 4-bit synapse and one alphabet {1}, accuracy is within ∼1% of conventional implementation. 🎯
*   CMAN is faster, more compact, and consumes less power. ⚡

---

## Evaluation & Results: Accuracy

*   The proposed approach was evaluated on various benchmark applications and datasets like MNIST, SVHN, TiCH, CIFAR10, and CIFAR100. 🧪
*   Accuracy is compared to conventional multiplier-based neurons and normalized to a 64-bit fully accurate NN implementation. 🎯
*   For ANNs (MNIST, SVHN, TiCH), maximum accuracy loss compared to conventional neurons of equivalent bit precision was: 📉
    *   ∼0.63% for 12-bit.
    *   ∼0.84% for 8-bit.
    *   ∼2.4% for 4-bit.
*   The 4-bit one-alphabet {1} version (CMAN) gave 96.31% accuracy for MNIST.
*   CNNs on CIFAR10/100 also performed well for 12- and 8-bit, with higher degradation for 4-bit. 🖼️
*   Accuracy results for Face Detection and Digit Recognition are presented in Tables 2 and 3. 📋

---

## Evaluation & Results: Power & Area ⚡📉🏢

*   Significant reductions in power consumption and area were observed compared to conventional neurons of equivalent bit precision under iso-speed conditions. 📉
*   For MAN (one-alphabet {1}): ⚡
    *   Power Reduction: ∼33% (12-bit), ∼32% (8-bit), ∼25% (4-bit).
    *   Area Reduction: ∼33% (12-bit), ∼34% (8-bit), ∼27% (4-bit).
*   Using two alphabets {1,3} also provides benefits (e.g., up to ∼16% power, ∼18% area reduction). 🔢
*   ASM-based neurons can achieve slightly better maximum operational speed with much lower power and area than conventional neurons. ⚡

---

## Energy-Accuracy Trade-off ⚖️📊🔄

*   A central aspect of this work is demonstrating the trade-off between energy/area savings and accuracy loss. 🔄
*   This trade-off depends on the application, neuron bit precision (12, 8, or 4 bits), and the number of alphabets used (conventional, 4, 2, or 1). 📊
*   Generally, reducing bit precision and the number of alphabets increases energy/area benefits but also increases accuracy loss. ⚖️
*   12-bit neurons maintain better accuracy with fewer alphabets compared to 8- and 4-bit synapses due to greater flexibility. 🎯
*   For NNs with neuron-size constraints (like 4-bit), using a mixed ASM approach (more alphabets in significant, smaller concluding layers) can improve accuracy with minimal energy overhead. 🔄

---

## System-Level Benefits 💾💡🖥️

*   System-level analysis considers both neuron computation energy and memory access energy. 🏢
*   In systems with large on-chip memory sized for the largest network, memory energy dominates (e.g., 12x computation energy for 12-bit, 32x for 4-bit). 🧠
*   In such systems, computation energy reduction from approximate neurons translates to smaller overall system energy savings (1-2.5%). ⚡
*   However, for systems using compressed neural networks (e.g., 50x compression), memory is smaller and memory energy is less dominant. 🔄
*   In these cases (memory sized for compressed networks), approximate neurons project higher system-level energy benefits (4–9%). 🚀
*   The approximate neuron design is compatible with hardware enhancements for compression. 🔗
*   ASM-based neurons themselves do not change memory requirements compared to conventional neurons of the same bit precision. 💾

---

## Conclusions ✨🚀🎯

*   Deep Learning Networks are powerful but computationally demanding. 🧠
*   We exploited the inherent error resilience of NNs to design highly energy-efficient, approximate ASM-based neurons. ⚡
*   Introduced MAN (Multiplier-less Artificial Neuron), replacing conventional multiplication with only shift and add operations. 🔄
*   Proposed aggressive bit precision scaling to 4 bits using assisted training and the CMAN concept for 4-bit neurons. 🎯
*   A methodology for retraining approximate networks with weight constraints was developed to mitigate accuracy loss. 🔄
*   Experimental results demonstrated significant improvements in energy consumption and reduction in area with only a negligible loss in classification accuracy. 📊

---

count: false
class: nord-dark, middle, center

.pull-left[

## Q & A 🎤

] .pull-right[

![Discussion](figs/questions-and-answers.svg)

]