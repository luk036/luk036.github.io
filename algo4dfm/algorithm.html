<!doctype html>
<html>
  <head>
    <title>Lecture 2e: Algorithmic Paradigms</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"
    />
    <link rel="stylesheet" type="text/css" href="../katex/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../css/spaces.css" />
    <link rel="stylesheet" type="text/css" href="../css/slides.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-dark.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-light.css" />
    <link rel="stylesheet" type="text/css" href="../css/font-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/bg-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/style.css" />
  </head>
  <body>
    <textarea id="source">

layout: true
class: typo, typo-selection

---

class: nord-dark, middle, center

# Lecture 2e: Algorithmic Paradigms

@luk036 üë®üèª‚Äçüè´

2024-11-13 üìÖ

---

## üìù Abstract

Throughout this lecture, we'll examine various approaches to solving computational problems, from greedy algorithms to simulated annealing. Each paradigm offers unique advantages and limitations that make them suitable for different problem domains.

We'll analyze how these paradigms perform on classic problems like the Knapsack Problem and Weighted Vertex Cover, and provide implementation insights using modern programming techniques. By the end of this lecture, you'll have a deeper understanding of when and how to apply these powerful algorithmic tools.

---

## Overview üìã

- Greedy approach
- Mathematical programming
- Primal-dual algorithm
- Randomized method
- Dynamic programming
- Local search
- Simulated annealing
- Books and online resource

---

## Algorithmic Approaches Overview

- Diverse Problem-Solving Strategies

  From greedy approaches to mathematical programming, each paradigm offers unique advantages for specific problem domains.

- Implementation Techniques

  Modern programming approaches like generic programming enhance flexibility and performance across algorithm implementations.

- Theoretical Foundations

  Understanding the mathematical underpinnings helps predict algorithm performance and limitations.

---

## ü§ë The Greedy Approach

The greedy approach makes locally optimal choices at each step, hoping to find a global optimum. While this works perfectly for some problems, it can fail dramatically for others. 

- Excellent for Minimum Spanning Tree (MST)

  Achieves optimal solutions for MST and Channel Routing Problems

- Decent for Knapsack problem

  Guarantees at least half of the optimal solution value

- Poor for Feedback Arc Removal problem
  
  Performs worse than even naive random methods: randomly remove edges when traversing a graph, then reverses the set if $|E'|$ is greater than 0.5$|E|$.

The key question remains: can we predict when greedy algorithms will perform well?

---

## üéí Knapsack Problem

.pull-left[

- Problem Definition

  A thief considers taking $b$ pounds of loot üí∞. The loot consists of $n$ items, each with weight $a_i$ and value $p_i$. Any amount of an item can be taken as long as the weight limit $b$ is not exceeded.

- The greedy approach

  The greedy approach takes items in order of highest value-to-weight ratio ($p_i/a_i$) until the knapsack is full.

] .pull-right[

![knapsack](lec02.files/knapsack.png)

]

---

## Program 1: Greedy Knapsack

- **Input**: Set of $n$ items, for each $x_i \in X$, values $p_i$,
  $a_i$, positive integer $b$;
- **Output**: Subset $Y \subset X$ such that $\sum a_i \leq b$;
- Sort $X$ in non-increasing order with respect to the ratio
  $p_i$/$a_i$;
- Let ($x_1$, $x_2$, ..., $x_n$) be the sorted sequence
- $Y$ := $0$;
- **for** $i$:=1 **to** $n$ **do**
  - **if** $b \geq a_i$ **do**
      - $Y$ := $Y \cup \{ x_i \}$;
      - $b$ := $b - a_i$;
- **return** $Y$

---

## C++ Template Implementation

```cpp
template <class InputIt, typename T, typename F1, typename F2>
InputIt greedy_knapsack(InputIt first, InputIt last,
                        const T& b, F1&& price, F2&& weight)
{
    using Item = typename InputIt::value_type;
    std::sort(first, last, [&](const Item& i1, const Item& i2) {
        return weight(i1) * price(i2) < weight(i2) * price(i1);
    });
    T init(0);
    InputIt it = std::find_if(first, last, [&](Item& i) {
        return (init += weight(i)) > b;
    });
    return it;
}
```

- Test program can be found in http://ideone.com/9ZK6ol.

---

## Why Generic Programming

- ü§∏ Increased Flexibility

  Adapt code to various data types without modification, allowing the same algorithm to work with different problem instances.
  
- ‚ôªÔ∏è Reduced Duplication

  Write once, use for multiple types, significantly reducing development and maintenance effort.

- üõ°Ô∏è Enhanced Type Safety

  Catch errors at compile-time rather than runtime, improving reliability and reducing debugging time.

- üöÄ Improved Performance

  Optimize code for specific types at compile-time, eliminating runtime overhead of virtual functions or type checking.

---

## Can the thief do better?

- Theorem 1

  Let m<sub>H</sub>($x$) = max($p$<sub>max</sub>, m<sub>GR</sub>($x$)), where $p$<sub>max</sub> is the maximum profit of an item üíç in $x$. Then m<sub>H</sub>($x$) satisfies the inequality: m($x$)/m<sub>H</sub>($x$) < 2. (p.42) (m($x$) is the optimal solution)

- Performance Guarantee üíØ

  Theorem 1 states that if we take the maximum of the greedy solution and the highest-value single item, we achieve a solution that is at least half of the optimal value.

  This hybrid approach provides a provable performance bound, making it a reliable choice for practical implementations.

HW: Implement the algorithm using C++ Template technique and iterators (generic programming style)

---

## Mathematical Programming Approaches

- Integer Linear Programming

  Exact formulation with integer constraints

- LP Relaxation

  Remove integrality constraints for polynomial-time solving

- Primal-Dual Method

  Iteratively improve both primal and dual solutions

Mathematical programming provides a powerful framework for solving optimization problems. By relaxing integer constraints, we can solve problems in polynomial time, then round the solution to obtain approximate integer solutions. The primal-dual method offers a faster alternative that doesn't require solving the LP optimally while still providing performance guarantees.

---

## Weighted Vertex Cover Problem

- ILP Formulation

  Given a weighted graph $G=(V, E)$, Minimum Weighted Vertex Cover(MWVC) can be formulated as the following integer program ILP<sub>VC</sub>($G$):

$$\begin{array}{lll}
  \text{minimize} & \sum_{v \in V} c_v x_v \\
  \text{subject to} & x_u + x_v \geq 1 & \forall (u, v) \in E \\
    & x_v \in \{0, 1\} & \forall v \in V
\end{array}$$

- LP Relaxation

  Relax integrality constraints for polynomial-time solving

- Rounding Approach

  Round variables ‚â• 0.5 to 1, guaranteeing 2-approximation.

---

## Program 2.6 Rounding WVC

- **Input** Graph $G=(V, E)$ with non-negative vertex weights;
- **Output** Vertex cover $V$' of $G$;
- Let ILP<sub>VC</sub> be the linear integer
  programming formulation of the problem;
- Let LP<sub>VC</sub> be the problem obtained
  from ILP<sub>VC</sub> by relaxing the
  integrality constraints;
- Let $x(G^*)$ be the optimal solution for
  LP<sub>VC</sub>;
- $V'$ := \{$v \mid x_v(G^*) \geq 0.5$\};
- **return** $V$'

---

## LP Relaxation

- Theorem 2.15. 

  Given a graph $G$ with non-negative vertex weights, Program 2.6 finds a feasible solution of MWVC with value m<sub>LP</sub>($G$) such that m<sub>LP</sub>($G$)/m($G^*$) $\leq 2$.

  Problem: need to solve the LP optimally.

- Primal-Dual Method

  Linear-time algorithm with 2-approximation guarantee

---

## ‚òØ Primal-dual WVC

- **Input** Graph $G = (V, E)$ with non-negative vertex weights;
- **Output** Vertex cover $V'$ of $G$;
- Let DLP<sub>VC</sub> be the dual of the LP
  relaxation of ILP<sub>VC</sub>;
- ~~**for** each dual variable $y$ of DLP<sub>VC</sub> **do** $y$ := 0;~~
- $V' := 0$;
- **while** $V'$ is not a vertex cover **do**
  - Let $(v_i, v_j)$ be an edge not covered by $V'$;
  - Increase $y_{ij}$ until a constraint of
    DLP<sub>VC</sub> becomes tight;
  - **if** sum$(y_{ij} | (i, j) \in E )$ is tight **then**
      - $V' := V' \cup \{v_i\}$ (\* the i-th dual constraint is
      tight \*)
  - **else**
      - $V' := V' \cup \{v_j\}$ (\* the j-th dual constraint is
      tight \*)
- **return** $V'$

---

## ‚òØ Primal-dual WVC

- Theorem 2.16. 

  Given a graph $G$ with non-negative weights, Program 2.7 finds a feasible solution of MWVC such that $m_\text{PD}(G)/m(G^*) \leq 2$. (p.¬†69)

- Much faster than Program 2.6 (only take linear time) because we
  don't need to solve the LP optimally.

- Bonus: Sum of dual variables $y_{ij}$ gives the lower bound of the
  optimal solution.

---

## üé≤ Randomized Algorithms

Randomized algorithms introduce probability to achieve simpler or faster solutions. For Weighted Vertex Cover, the randomized approach selects edges and endpoints with probability proportional to weights, achieving an expected 2-approximation ratio.

These algorithms don't guarantee good solutions every time but perform well on average.

---

## Program - Random WVC

- **Input** Graph $G= (V, E)$, weight function $w: V \mapsto N$;
- **Output** Vertex cover $U$;
- $U$ := $\emptyset$;
- **while** $E$ is not empty **do**
  - Select an edge $e = (v,t) \in E$;
  - Randomly choose $x$ from $\{v,t\}$ with Pr$\{x=v\}$ =
    $w(t) / (w(v) + w(t))$;
  - $U$ := $U \cup \{x\}$;
  - $E$ := $E - \{e \mid x \text{ is an endpoint of } e\}$
- **return** $U$

---

## üé≤ Randomized Algorithms

- Theorem 5.1. 

  The expect measure of the solution returned by the previous algorithm satisfied the following inequality:

  $$E[m_\text{RWVC}(x)] \leq 2 m^*(x)$$

HW: Implement MWVC solvers using all the above methods. Also extend all the methods to handle hypergraph.

---

## Dynamic Programming (I)

- Dynamic programming solves problems by breaking them into overlapping subproblems. 

- This approach works by solving subproblems once and storing their solutions to avoid redundant computation.

- üìö Example:

  One passenger wants to go from city A to city H through the _shortest path_ according to the map on the right, where number of indicate distance between corresponding cities.

- Reference

  Pablo Pedregal, _Introduction to Optimization_, chapter 5.8, Springer, 2003

---

## Dynamic Programming (II)

- Proposition 5.24 (Fundamental property of dynamic programming)
  
  - If $S(t_j, x)$ denotes the optimal cost from $(t_0, x)$ to $(t_j, x)$
  
  - then we must have S($t_{j+1}$, $y$) = min<sub>j</sub> \[S($t_j$, $x$) + c($j$,$x$,$y$)\]

---

## Dynamic Programming (III)

- According to Proposition 5.24, we must proceed successively to
  determine S($t_j, x$) for each $x$ in
  A<sub>j</sub> to end with S($t_n, x_n$). In the
  proposed example, we have four stages $t_0$, $t_1$, $t_2$, $t_3$
  with associated sets of feasible states

  - A<sub>0</sub> = {A},
    A<sub>1</sub> = {B, C, D},
    A<sub>2</sub> = {E,F,G},
    A<sub>3</sub> = {H}

- For each city in A<sub>1</sub>, there is a unique
  path from A, so that it must be optimal, and

  - S($t_1$, B) = 7, S($t_1$, C) = 4, S($t_1$, D) = 1.

- For each city in A<sub>2</sub>, we determine the
  optimal cost based on the fundamental property of dynamic
  programming,

  - S($t_{j+1}$, $y$) = min<sub>j</sub> \[S($t_j$,
    $x$) + c($j$,$x$,$y$)\]

---

## Local Search and Simulated Annealing

- ‚åï Local Search

  Iteratively improve by exploring neighboring solutions

- üå° Simulated Annealing

  Accept worse solutions with decreasing probability over time

Local search methods explore the solution space by moving between neighboring solutions. While basic local search can get trapped in local optima, simulated annealing introduces randomness that allows occasional uphill moves, helping escape local minima. The algorithm gradually reduces the probability of accepting worse solutions, eventually converging to a good solution.

---

## ‚åï Local Search

- **Input**: Instance $x$;
- **Output**: Solution $s$
- $s$ := initial feasible solution $s_0$;
- (\* $\mathcal{N}$ denotes the neighborhood function \*)
- **repeat**
  - Select any $s' \in \mathcal{N}(x, s)$ not yet considered;
  - **if** $m(x,s')$ < $m(x, s)$ **then**
      - $s$ := $s'$;
- **until** all solutions in $\mathcal{N}(x, s)$ have been
  visited;
- **return** $s$;

---

## üå° Simulated Annealing

- **Input**: Instance $x$;
- **Output**: Solution $s$
- $œÑ$ := $t$;
- $s$ := initial feasible solution $s_0$;
- **repeat**
  - **for** $l$ times **do**
    - Select any unvisited $s' \in \mathcal{N}(x, s)$
    - **if** ($m(x, s')$ < $m(x, s)$)
      - $s$ := $s'$;
    - **else**
      - $Œ¥$ := $m(x, s') - m(x, s)$;
      - $s$ := $s'$ with probability exp($-Œ¥/t$);
  - $œÑ$ := $r \cdot œÑ$; (\* update of temperature \*)
- **until** FROZEN;
- **return** $s$;

---

## Other Heuristic Methods

- üßó Hill Climbing
  - Reference: _Hill Climbing_ by Sutton and Barto, MIT Press, 1983

- üêú Ant Colony Optimization (ACO)
  - Reference: _Ant Colony Optimization_, D. E. Kirkpatrick, C.
    Storn, Journal of Global Optimization, 1992

- üß¨ Genetic Algorithm (GA)
  - Reference: _Genetic Algorithms_, M. Mitchell, McGraw Hill, 1989

- üôÖ Tabu Search (TS)
  - Reference: _Tabu search_, Kirkpatrick, Storn, 1983

- üèòÔ∏è Variable Neighborhood Descent (VND)
  - Reference: _Variable neighborhood descent_, Kirkpatrick, Storn, 1983

---

## üìö Books and Online Resources

To deepen your understanding of algorithmic paradigms, explore these valuable resources:

- Complexity and Approximation

  G. Ausiello et al., Springer, 1999 - Covers combinatorial optimization problems and their approximability properties.

- Computers and Intractability

  M. R. Garey and D. S. Johnson, Freeman, 1979 - The definitive guide to the theory of NP-completeness.

- Introduction to Optimization

  Pablo Pedregal, Springer, 2003 - Provides fundamental concepts and techniques in optimization theory.

---

class: nord-dark, middle, center

# Q & A üôã

    </textarea>
    <script src="../js/remark.min.js"></script>
    <script src="../katex/katex.min.js" type="text/javascript"></script>
    <script
      src="../katex/contrib/auto-render.min.js"
      type="text/javascript"
    ></script>
    <script type="text/javascript">
      renderMathInElement(document.getElementById("source"), {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
        ],
      });
      var slideshow = remark.create({
        ratio: "4:3", // Á™óÂè£ÊØî‰æã
        // ÂèØÈÄâÔºöarta, ascetic, dark, default, far, github, googlecode, idea,
        // ir-black, magula, monokai, rainbow, solarized-dark, solarized-light,
        // sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright,
        // tomorrow-night, tomorrow-night-eighties, vs, zenburn.
        highlightStyle: "tomorrow-night-eighties",
        highlightLines: true,
        countIncrementalSlides: false, // Â¢ûÈáèÂÜÖÂÆπÊòØÂê¶ÁÆó‰∏ÄÈ°µ
        // slideNumberFormat: "", // Ëã•Â∞ÜÊ≠§ÂèÇÊï∞ËÆæÁΩÆ‰∏∫ ""ÔºåÂ∞Ü‰∏çÊòæÁ§∫È°µÁ†Å
        navigation: {
          scroll: false, // ÊòØÂê¶ÂÖÅËÆ∏‰ΩøÁî®Èº†Ê†áÊªöËΩÆÁøªÈ°µ
          touch: true, // ÔºàÂ¶ÇÊûúÊòØËß¶Êë∏Â±èÔºâÊòØÂê¶ÂÖÅËÆ∏ÁÇπÂáªÂ±èÂπïÂ∑¶ËæπÊàñÂè≥ËæπÂâçÂêéÁøªÈ°µ
          click: false, // ÊòØÂê¶ÂÖÅËÆ∏Èº†Ê†áÁÇπÂáªÂ±èÂπïÂ∑¶ËæπÊàñÂè≥ËæπÂâçÂêéÁøªÈ°µ
        },
      });
    </script>
  </body>
</html>
