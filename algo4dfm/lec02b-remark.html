<!DOCTYPE html>
<html>
  <head>
    <title>Lecture 2c: Introduction to Optimization</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" type="text/css" href="../katex/katex.min.css"/>
    <link rel="stylesheet" type="text/css" href="slides.css"/>
    <style type="text/css">
      @import url(http://fonts.lug.ustc.edu.cn/css?family=Droid+Serif);
      @import url(http://fonts.lug.ustc.edu.cn/css?family=Yanone+Kaffeesatz);
      /* Slideshow styles */
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

Lecture 2c: Introduction to Optimization
=======================================

Wai-Shing Luk

2020-09-30

---

Agenda
------

-   Introduction
-   Linear programming
-   Nonlinear programming
-   Duality and Convexity
-   Approximation techniques
-   Convex Optimization
-   Books and online resources.

---

Classification of Optimizations
-------------------------------

-   Continuous
    -   Linear vs Non-linear
    -   Convex vs Non-convex
-   Discrete
    -   Polynomial time Solvable
    -   NP-hard
        -   Approximatable
        -   Non-approximatable
-   Mixed

-   Unconstrained vs constrained

---

![classification](lec02.files/class.svg)

---

Linear Programming Problem
--------------------------

-   An LPP in standard form is:
    $$\min\{ c^\mathsf{T} x \mid A x = b, x \ge 0\}.$$
-   The ingredients of LPP are:
    -   An $m \times n$ matrix $A$, with $n > m$
    -   A vector $b \in \mathbb{R}^m$
    -   A vector $c \in \mathbb{R}^n$

---

Example
-------

$$\begin{array}{lll}
  \text{minimize}      & 0.4 x_1 + 3.4 x_2 - 3.4 x_3 \\
  \text{subject to}    & 0.5 x_1 + 0.5 x_2           & = 3.5 \\
                       & 0.3 x_1 - 0.8 x_2 + 8.4 x_2 & = 4.5 \\
                       & x_1, x_2, x_3 \ge 0
\end{array}$$

---

Transformations to Standard Form
--------------------------------

-   Theorem: Any LPP can be transformed into the standard form.
-   Variables not restricted in sign:
    -   Decompose $x$ to two new variables
        $x = x_1 - x_2, x_1, x_2 \geq 0$
-   Transforming inequalities into equalities:
    -   By putting slack variable $y = b - A x \geq 0$
    -   Set $x' = (x, y), A' = (A, 1)$
-   Transforming a max into a min
    -   max(expression) = min($-$expression);

---

Duality of LPP
--------------

-   If the primal problem of the LPP:
    $\min\{ c^\mathsf{T} x \mid A x \ge b, x \ge 0\}$.
-   Its dual is:
    $\max\{ y^\mathsf{T} b \mid A^\mathsf{T} y \leq c, y \ge 0\}$.
-   If the primal problem is:
    $\min\{ c^\mathsf{T} x \mid A x = b, x \ge 0\}$.
-   Its dual is: $\max\{ y^\mathsf{T} b \mid A^\mathsf{T} y \leq c\}$.

---

Nonlinear Programming
---------------------

-   The standard form of an NLPP is
    $$\min\{f(x) \mid g(x) \leq 0, h(x)=0 \}.$$
-   Necessary conditions of optimality, Karush- Kuhn-Tucker (KKT)
    conditions:
    -   $\nabla f(x) + \mu \nabla g(x) + \lambda \nabla h(x) = 0$,
    -   $\mu g(x) = 0$,
    -   $\mu \geq 0, g(x) \leq 0, h(x) = 0$

---

Convexity
---------

-   A function $f$: $K \subseteq \mathbb{R}^n \mapsto R$ is convex
    if $K$ is a convex set and
    $f(y) \ge f(x) + \nabla f(x) (y - x), \; y,x \in K$.

-   **Theorem**: Assume that $f$ and $g$ are convex differentiable
    functions. If the pair $(x, m)$ satisfies the KKT conditions above,
    $x$ is an optimal solution of the problem. If in addition, $f$ is
    strictly convex, $x$ is the only solution of the problem.

    **(Local minimum = global minimum)**

---

Duality and Convexity
---------------------

-   Dual is the NLPP: $$\max\{\theta(\mu, \lambda) \mid \mu \geq 0\},$$
    where
    $\theta(\mu, \lambda) = \inf_x [ f(x) + \mu g(x) + \lambda h(x) ]$

-   Dual problem is always convex.

-   Useful for computing the lower/upper bound.

---

Unconstraint Techniques
-----------------------

-   Line search methods
-   Fixed or variable step size
-   Interpolation
-   Golden section method
-   Fibonacci's method
-   Gradient methods
-   Steepest descent
-   Quasi-Newton methods
-   Conjugate Gradient methods

---

General Descent Method
----------------------

1.  **Input**: a starting point $x \in$ dom $f$
2.  **Output**: $x^*$
3.  **repeat**
    1.  Determine a descent direction $p$.
    2.  Line search. Choose a step size $\alpha > 0$.
    3.  Update. $x := x + \alpha p$
4.  **until** stopping criterion satisfied.

---

Some Common Descent Directions
------------------------------

-   Gradient descent: $p = -\nabla f(x)^\mathsf{T}$
-   Steepest descent:
    -   $\triangle x_{nsd}$ =
        argmin$\{\nabla f(x)^\mathsf{T} v \mid \|v\|=1 \}$
    -   $\triangle x$ = $\|\nabla f(x)\| \triangle x_{nsd}$
        (un-normalized)
-   Newton's method: 
    -   $p = -\nabla^2 f(x)^{-1} \nabla f(x)$
-   Conjugate gradient method: 
    -   $p$ is "orthogonal" to all previous $p$'s
-   Stochastic subgradient method:
    -   $p$ is calculated from a set of sample data (instead of using all data)
-   Network flow problems:
    -   $p$ is given by a "negative cycle" (or "negative cut").

---

Approximation Under Constraints
-------------------------------

-   Penalization and barriers
-   Dual method
-   Interior Point method
-   Augmented Lagrangian method

---

üìö Books and Online Resources
----------------------------

-   Pablo Pedregal. Introduction to Optimization, Springer. 2003 (O224
    P371)
-   Stephen Boyd and Lieven Vandenberghe, Convex Optimization, Dec.¬†2002
-   Mittlemann, H. D. and Spellucci, P. Decision Tree for Optimization
    Software, World Wide Web, http://plato.la.asu.edu/guide.html, 2003

---

class: center, middle

Q & A üó£Ô∏è
========

    </textarea>
    <script src="../remark-latest.min.js"></script>
    <script src="../katex/katex.min.js" type="text/javascript"></script>
    <script src="../katex/contrib/auto-render.min.js" type="text/javascript"></script>
    <script type="text/javascript">
      renderMathInElement(
          document.getElementById("source"),
          {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false}
              ]
          }
      );
      var slideshow = remark.create({
        ratio: '16:9',
        highlightStyle: 'googlecode'
      });
    </script>
  </body>
</html>
