<!doctype html>
<html>
  <head>
    <title>Lecture 2c: Introduction to Convex Optimization</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"
    />
    <link rel="stylesheet" type="text/css" href="../katex/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-dark.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-light.css" />
    <link rel="stylesheet" type="text/css" href="../css/font-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/bg-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/slides.css" />
  </head>
  <body>
    <textarea id="source">

layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

Lecture 2c: Introduction to Convex Programming
==============================================

@luk036
-------

2024-09-25

---

📝 Abstract
----------

This lecture introduces convex programming and covers various optimization aspects. It commences with an optimization overview, including linear and nonlinear programming, duality, convexity, and approximation techniques. The lecture then proceeds with specific topics within continuous optimization, such as linear programming problems and their standard form, standard form transformations, and duality. The lecture covers nonlinear programming, examining the standard format of an NLPP (nonlinear programming problem) and the essential optimality requirements referred to as the Karush-Kuhn-Tucker (KKT) conditions. It also delves into the topic of convexity, defining convex functions and their characteristics. Moreover, the lecture covers the duality of convex optimization problems and their value in computation. Finally, the document discusses several optimization techniques, including unconstrained optimization, descent methods, and approximation methods when working with constraints.

---

🗺️ Overview
-----------

.pull-left[

-   Introduction
-   Linear programming
-   Nonlinear programming
-   Duality and Convexity
-   Approximation techniques
-   Convex Optimization
-   Books and online resources.

] .pull-right[

![image](figs/dfm.svg)

]

---

Classification of Optimizations
-------------------------------

-   Continuous
    -   Linear vs Non-linear
    -   Convex vs Non-convex
-   Discrete
    -   Polynomial time Solvable
    -   NP-hard
        -   Approximatable
        -   Non-approximatable
-   Mixed

---

Venn Diagram of Continuous Optimization
-----------------------

![classification](lec02.files/class.svg)

---

Linear Programming Problem
--------------------------

-   An LPP in standard form is:
    $$\min\{ c^\mathsf{T} x \mid A x = b, x \ge 0\}.$$
-   The ingredients of LPP are:
    -   An $m \times n$ matrix $A$, with $n > m$
    -   A vector $b \in \mathbb{R}^m$
    -   A vector $c \in \mathbb{R}^n$

---

Example
-------

$$\begin{array}{lll}
  \text{minimize} & 0.4 x_1 + 3.4 x_2 - 3.4 x_3 \\
  \text{subject to} & 0.5 x_1 + 0.5 x_2  & = 3.5 \\
  & 0.3 x_1 - 0.8 x_2 + 8.4 x_2 & = 4.5 \\
  & x_1, x_2, x_3 \ge 0
\end{array}$$

---

Transformations to Standard Form
--------------------------------

-   Theorem: Any LPP can be transformed into the standard form.
-   Variables not restricted in sign:
    -   Decompose $x$ to two new variables
        $x = x_1 - x_2, x_1, x_2 \geq 0$
-   Transforming inequalities into equalities:
    -   By putting slack variable $y = b - A x \geq 0$
    -   Set $x' = (x, y), A' = (A, 1)$
-   Transforming a max into a min
    -   max(expression) = min($-$expression);

---

## Linearize the non-linear's

**Original Problem:**
$$(a +  b \cdot {\color{red} y}) x \leq 0, \; x > 0$$

**Transformation:**
Let $z = y \cdot x$. The problem becomes:
$$a \cdot x + b \cdot {\color{green} z} \leq 0, \; x > 0$$

**Post-proccessing:**
$$y_\text{opt} = z_\text{opt} x^{-1}_\text{opt}$$

---

## Logarithmic Transformation

**Original Problem:**

$$\pi \leq {\color{red} x / y} \leq \phi, \; x > 0, y > 0$$

**Transformation:**
Let $z' = \log(z)$. The problem becomes:

$$\pi' \leq {\color{green} x' - y'} \leq \phi'$$

**Post-proccessing:**

$$z_\text{opt} = \exp(z'_\text{opt}).$$

---

Duality of LPP
--------------

-   If the primal problem of the LPP:
    $\min\{ c^\mathsf{T} x \mid A x \ge b, x \ge 0\}$.
-   Its dual is:
    $\max\{ y^\mathsf{T} b \mid A^\mathsf{T} y \leq c, y \ge 0\}$.
-   If the primal problem is:
    $\min\{ c^\mathsf{T} x \mid A x = b, x \ge 0\}$.
-   Its dual is: $\max\{ y^\mathsf{T} b \mid A^\mathsf{T} y \leq c\}$.

---

Nonlinear Programming
---------------------

-   The standard form of an NLPP is
    $$\min\{f(x) \mid g(x) \leq 0, h(x)=0 \}.$$
-   Necessary conditions of optimality, Karush- Kuhn-Tucker (KKT)
    conditions:
    -   Gradient Condition: ∇f(x) + µ∇g(x) + λ∇h(x) = 0, where ∇f(x),
        ∇g(x), and ∇h(x) are the gradients of the objective function,
        inequality constraints, and equality constraints, respectively.
        This condition states that the sum of the directional
        derivatives of the objective function and the constraints must
        be zero at the optimal solution.

    -   Complementary Slackness Condition: µg(x) = 0, where µ is a
        Lagrange multiplier associated with the inequality constraints.
        This condition implies that either the constraint is inactive
        (g(x) ≤ 0) or its corresponding Lagrange multiplier is zero.

    -   Feasibility Condition: µ ≥ 0, g(x) ≤ 0, h(x) = 0. This condition
        ensures that the inequality and equality constraints are
        satisfied at the optimal solution.

---

What is the significance of the KKT conditions mentioned?
---------------------------------------------------------

The significance of the KKT conditions lies in their ability to provide
necessary conditions for a solution to be optimal in nonlinear
programming problems. By satisfying these conditions, a point can be
determined as a possible optimal solution. Moreover, if the objective
function is strictly convex, and the KKT conditions are satisfied, then
the solution obtained is the unique optimal solution. In essence, the
KKT conditions serve as a powerful mathematical tool for analyzing and
solving optimization problems.

---

Convexity
---------

.pull-left[

-   A function $f$: $K \subseteq \mathbb{R}^n \mapsto R$ is convex if
    $K$ is a convex set and
    $f(y) \ge f(x) + \nabla f(x) (y - x), \; y,x \in K$.

-   **Theorem**: Assume that $f$ and $g$ are convex differentiable
    functions. If the pair $(x, m)$ satisfies the KKT conditions above,
    $x$ is an optimal solution of the problem. If in addition, $f$ is
    strictly convex, $x$ is the only solution of the problem.

] .pull-right[

![image](figs/local-global.png)

**(Local minimum == global minimum)**

]

---

Duality and Convexity
---------------------

-   Dual is the NLPP: $$\max\{\theta(\mu, \lambda) \mid \mu \geq 0\},$$
    where
    $\theta(\mu, \lambda) = \inf_x [ f(x) + \mu g(x) + \lambda h(x) ]$

-   Dual problem is always convex.

-   Useful for computing the lower/upper bound.

---

Applications
------------

-   Statistics
-   Filter design
-   Power control
-   Machine learning
    -   SVM classifier
    -   logistic regression

---

class: nord-light, middle, center

Convexify the non-convex's
==========================

---

## Example: Convexifying a Simple Non-Convex Function

Consider the non-convex function:
$$f(x) = x^4 - 4x^2 + 4$$

**Transformation:**
Let $y = x^2$. The problem becomes:
$$f(y) = y^2 - 4y + 4$$

**Post-proccessing:**
$$x_\text{opt} = \pm\sqrt{y_\text{opt} }$$

---

## Change of curvature: square

**Original Problem:**
$${\color{red} x^2 } + {\color{red} y^2 } \geq 0.16 \quad \text{(non-convex)}$$

**Transformation:**
Let $x' = x^2$, $y' = y^2$. The problem becomes:
$${\color{green} x'} + {\color{green} y'} \geq 0.16, \quad x', y' \geq 0$$

**Post-proccessing:**
$$x_\text{opt} = \pm\sqrt{x'_\text{opt} }, \quad y_\text{opt} = \pm\sqrt{y'_\text{opt} }.$$

**Generalization:**
- Quadratically Constrainted Quadratic Programming (QCQP)

---

## Change of curvature: sine

**Original Problem:**
$${\color{red} \sin^2{x} } \leq 0.4, \quad 0 \leq x \leq \pi/2$$

**Transformation:**
$${\color{green} y} \leq 0.4, \quad 0 \leq y \leq 1$$

**Post-proccessing:**
$$x_\text{opt} = \sin^{-1}(\sqrt{y_\text{opt} }).$$

👉 Note that $\sin(\cdot)$ are monotonic concave functions in $(0, \pi/2)$.

---

## Change of curvature: square

**Original Problem:**
$$0.3 \leq {\color{red} \sqrt{x} } \leq 0.4$$

**Transformation:**
$$0.09 \leq {\color{green} x} \leq 0.16 \, .$$

👉 Note that $\sqrt{\cdot}$ are **monotonic** **concave** functions in $(0, +\infty)$.

**Generalization:**

- Consider $|H(\omega)|^2$ (power) instead of $|H(\omega)|$ (magnitude).
- Square root → Spectral factorization

---

## Logarithmic Transformation

**Original Problem:**

$${\color{red} 1 / x + 1 / y} \leq 1, \; x > 0, y > 0$$

**Transformation:**
Let $z' = \log(z)$. The problem becomes:

$${\color{green} \log(e^{-x'} + e^{-y'})} \leq 0$$

**Post-proccessing:**

$$z_\text{opt} = \exp(z'_\text{opt}).$$

**Generalization:**

- Geometric programming

---

## Reciprocal Transformation

**Original Problem:**
$${\color{red} \log(x)} + 0.4 \leq 0, \; x > 0$$

**Transformation:**
Let $y = 1 / x$. The problem becomes:
$${\color{green} -\log(y)} + 0.4 \leq 0, \; y > 0 \, .$$

**Post-proccessing:**
$$x_\text{opt} = y^{-1}_\text{opt}.$$

👉 Note that $\sqrt{\cdot}$, $\log(\cdot)$, and $(\cdot)^{-1}$ are monotonic functions.

---

## Generalize to Matrix Inequalities

**Original Problem:**

$${\color{red} \log(\det X)} + \text{Tr}(X^{-1} C) \leq 0.3, \; X \succ 0$$

**Transformation:**
Let $Y = X^{-1}$. The problem becomes:
$${\color{green} -\log(\det Y)} + \text{Tr}(Y \cdot C) \leq 0.3, \; Y \succ 0$$

**Post-proccessing:**
$$X_\text{opt} = Y^{-1}_\text{opt}.$$

---

## Exponential Transformation

### Example: Exponential Utility Functions

For problems involving exponential utility functions, an exponential transformation can be useful.

**Original Problem:**
$$\text{minimize} \quad -\exp(f(x))$$

**Transformation:**
Let $y = \exp(f(x))$. The problem becomes:
$$\text{minimize} \quad -y \quad \text{subject to} \quad y = \exp(f(x))$$

This can help in transforming the problem into a convex form.

---

## Change of Variables

### Example: Power Transformation

For certain power functions, a change of variables can help.

**Original Problem:**
$$
\text{minimize} \quad f(x) = x^p \quad \text{for} \quad p \lt 1
$$

**Transformation:**
Let $y = x^p$. The problem becomes:
$$\text{minimize} \quad y \quad \text{subject to} \quad y = x^p$$

This can help in transforming the problem into a convex form.

---

## Perspective Transformation 🤔

### Example: Perspective Function

For certain functions, applying a perspective transformation can help.

**Original Problem:**
$$\text{minimize} \quad \frac{f(x)}{t}$$

**Transformation:**
Let $y = \frac{x}{t}$. The problem becomes:
$$\text{minimize} \quad f(y) \quad \text{subject to} \quad t = 1$$

This can help in transforming the problem into a convex form.

---

## Change of variables

**Original Problem:**
$$(a +  b \cdot {\color{red} y}) x \leq 0, \; x > 0$$

**Transformation:**
Let $z = y \cdot x$. The problem becomes:
$$a \cdot x + b \cdot {\color{green} z} \leq 0, \; x > 0$$

**Post-proccessing:**
$$y_\text{opt} = z_\text{opt} x^{-1}_\text{opt}$$

---

## Generalize to matrix inequalities

**Original Problem:**
$$(A + B \cdot {\color{red} Y}) X + X (A + B \cdot {\color{red} Y})^T  \prec 0, \; X \succ 0$$

**Transformation:**
Let $Z = Y \cdot X$. The problem becomes:
$$A \cdot X + X \cdot A^T + B \cdot {\color{green} Z} + {\color{green} Z}^T \cdot B^T \prec 0, \; X \succ 0$$

**Post-proccessing:**
$$Y_\text{opt} = Z_\text{opt} X^{-1}_\text{opt}$$

---

Unconstraint Techniques
-----------------------

-   Line search methods
-   Fixed or variable step size
-   Interpolation
-   Golden section method
-   Fibonacci's method
-   Gradient methods
-   Steepest descent
-   Quasi-Newton methods
-   Conjugate Gradient methods

---

General Descent Method
----------------------

1.  **Input**: a starting point $x \in$ dom $f$
2.  **Output**: $x^*$
3.  **repeat**
    1.  Determine a descent direction $p$.
    2.  Line search. Choose a step size $\alpha > 0$.
    3.  Update. $x := x + \alpha \cdot p$
4.  **until** stopping criterion satisfied.

---

Common Descent Directions
------------------------------

-   Gradient descent: $p = -\nabla f(x)^\mathsf{T}$
-   Steepest descent:
    -   $\triangle x_{nsd}$ =
        argmin$\{\nabla f(x)^\mathsf{T} v \mid \|v\|=1 \}$
    -   $\triangle x$ = $\|\nabla f(x)\| \triangle x_{nsd}$
        (un-normalized)
-   Newton's method:
    -   $p = -\nabla^2 f(x)^{-1} \nabla f(x)$
-   Levenberg-Marquardt:
    - Hybrid of Newton and Gradient Descent
    -   $p = -(\nabla^2 f(x) + \lambda I)^{-1} \nabla f(x)$
-   Conjugate gradient method:
    -   $p$ is "orthogonal" to all previous $p$'s
-   Stochastic subgradient method:
    -   $p$ is calculated from a set of sample data (instead of using
        all data)
-   Network flow problems:
    -   $p$ is given by a "negative cycle" (or "negative cut").

---

Approximation Under Constraints
-------------------------------

-   Penalization and barriers
-   Dual method
-   Interior Point method
-   Augmented Lagrangian method

---

📚 Books and Online Resources
----------------------------

-   Pablo Pedregal. Introduction to Optimization, Springer. 2003 (O224
    P371)
-   Stephen Boyd and Lieven Vandenberghe, Convex Optimization, Dec. 2002
-   Mittlemann, H. D. and Spellucci, P. Decision Tree for Optimization
    Software, World Wide Web, http://plato.la.asu.edu/guide.html, 2003

---

## Other thoughts

- Minimizing any quasi-convex function subject to convex constraints can easily be
  transformed into a convex programming.
- Alternating minimization
- Replace a non-convex constraint with a sufficient condition
  (such as its lower bound). Less optimal.
- Relaxation + heuristic
- Decomposition

---

class: nord-dark, middle, center

.pull-left[

Q & A 🙋️
========

] .pull-right[

![image](figs/questions-and-answers.svg)

]

    </textarea>
    <script src="../js/remark.min.js"></script>
    <script src="../katex/katex.min.js" type="text/javascript"></script>
    <script
      src="../katex/contrib/auto-render.min.js"
      type="text/javascript"
    ></script>
    <script type="text/javascript">
      renderMathInElement(document.getElementById("source"), {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
        ],
      });
      var slideshow = remark.create({
        ratio: "4:3", // 窗口比例
        // 可选：arta, ascetic, dark, default, far, github, googlecode, idea,
        // ir-black, magula, monokai, rainbow, solarized-dark, solarized-light,
        // sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright,
        // tomorrow-night, tomorrow-night-eighties, vs, zenburn.
        highlightStyle: "tomorrow-night-eighties",
        highlightLines: true,
        countIncrementalSlides: false, // 增量内容是否算一页
        // slideNumberFormat: "", // 若将此参数设置为 ""，将不显示页码
        navigation: {
          scroll: false, // 是否允许使用鼠标滚轮翻页
          touch: true, // （如果是触摸屏）是否允许点击屏幕左边或右边前后翻页
          click: false, // 是否允许鼠标点击屏幕左边或右边前后翻页
        },
      });
    </script>
  </body>
</html>
