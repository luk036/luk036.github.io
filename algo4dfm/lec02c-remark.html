<!doctype html>
<html>
  <head>
    <title>Lecture 2c: Introduction to Convex Optimization</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"
    />
    <link rel="stylesheet" type="text/css" href="../katex/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-dark.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-light.css" />
    <link rel="stylesheet" type="text/css" href="../css/font-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/bg-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/slides.css" />
  </head>
  <body>
    <textarea id="source">

layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

Lecture 2c: Introduction to Convex Programming
==============================================

.pull-left[

@luk036
-------

2023-09-27

] .pull-right[

![image](figs/dfm.svg)

]

---

📝 Abstract
----------

This lecture introduces convex programming and covers various optimization aspects. It commences with an optimization overview, including linear and nonlinear programming, duality, convexity, and approximation techniques. The lecture then proceeds with specific topics within continuous optimization, such as linear programming problems and their standard form, standard form transformations, and duality. The lecture covers nonlinear programming, examining the standard format of an NLPP (nonlinear programming problem) and the essential optimality requirements referred to as the Karush-Kuhn-Tucker (KKT) conditions. It also delves into the topic of convexity, defining convex functions and their characteristics. Moreover, the lecture covers the duality of convex optimization problems and their value in computation. Finally, the document discusses several optimization techniques, including unconstrained optimization, descent methods, and approximation methods when working with constraints.

---

🗺️ Overview
-----------

-   Introduction
-   Linear programming
-   Nonlinear programming
-   Duality and Convexity
-   Approximation techniques
-   Convex Optimization
-   Books and online resources.

---

Classification of Optimizations
-------------------------------

-   Continuous
    -   Linear vs Non-linear
    -   Convex vs Non-convex
-   Discrete
    -   Polynomial time Solvable
    -   NP-hard
        -   Approximatable
        -   Non-approximatable
-   Mixed

---

Continuous Optimization
-----------------------

![classification](lec02.files/class.svg)

---

Linear Programming Problem
--------------------------

-   An LPP in standard form is:
    $$\min\{ c^\mathsf{T} x \mid A x = b, x \ge 0\}.$$
-   The ingredients of LPP are:
    -   An $m \times n$ matrix $A$, with $n > m$
    -   A vector $b \in \mathbb{R}^m$
    -   A vector $c \in \mathbb{R}^n$

---

Example
-------

$$\begin{array}{lll}
  \text{minimize} & 0.4 x_1 + 3.4 x_2 - 3.4 x_3 \\
  \text{subject to} & 0.5 x_1 + 0.5 x_2  & = 3.5 \\
  & 0.3 x_1 - 0.8 x_2 + 8.4 x_2 & = 4.5 \\
  & x_1, x_2, x_3 \ge 0
\end{array}$$

---

Transformations to Standard Form
--------------------------------

-   Theorem: Any LPP can be transformed into the standard form.
-   Variables not restricted in sign:
    -   Decompose $x$ to two new variables
        $x = x_1 - x_2, x_1, x_2 \geq 0$
-   Transforming inequalities into equalities:
    -   By putting slack variable $y = b - A x \geq 0$
    -   Set $x' = (x, y), A' = (A, 1)$
-   Transforming a max into a min
    -   max(expression) = min($-$expression);

---

Duality of LPP
--------------

-   If the primal problem of the LPP:
    $\min\{ c^\mathsf{T} x \mid A x \ge b, x \ge 0\}$.
-   Its dual is:
    $\max\{ y^\mathsf{T} b \mid A^\mathsf{T} y \leq c, y \ge 0\}$.
-   If the primal problem is:
    $\min\{ c^\mathsf{T} x \mid A x = b, x \ge 0\}$.
-   Its dual is: $\max\{ y^\mathsf{T} b \mid A^\mathsf{T} y \leq c\}$.

---

Nonlinear Programming
---------------------

-   The standard form of an NLPP is
    $$\min\{f(x) \mid g(x) \leq 0, h(x)=0 \}.$$
-   Necessary conditions of optimality, Karush- Kuhn-Tucker (KKT)
    conditions:
    -   Gradient Condition: ∇f(x) + µ∇g(x) + λ∇h(x) = 0, where ∇f(x),
        ∇g(x), and ∇h(x) are the gradients of the objective function,
        inequality constraints, and equality constraints, respectively.
        This condition states that the sum of the directional
        derivatives of the objective function and the constraints must
        be zero at the optimal solution.

    -   Complementary Slackness Condition: µg(x) = 0, where µ is a
        Lagrange multiplier associated with the inequality constraints.
        This condition implies that either the constraint is inactive
        (g(x) ≤ 0) or its corresponding Lagrange multiplier is zero.

    -   Feasibility Condition: µ ≥ 0, g(x) ≤ 0, h(x) = 0. This condition
        ensures that the inequality and equality constraints are
        satisfied at the optimal solution.

---

What is the significance of the KKT conditions mentioned?
---------------------------------------------------------

The significance of the KKT conditions lies in their ability to provide
necessary conditions for a solution to be optimal in nonlinear
programming problems. By satisfying these conditions, a point can be
determined as a possible optimal solution. Moreover, if the objective
function is strictly convex, and the KKT conditions are satisfied, then
the solution obtained is the unique optimal solution. In essence, the
KKT conditions serve as a powerful mathematical tool for analyzing and
solving optimization problems.

---

Convexity
---------

-   A function $f$: $K \subseteq \mathbb{R}^n \mapsto R$ is convex if
    $K$ is a convex set and
    $f(y) \ge f(x) + \nabla f(x) (y - x), \; y,x \in K$.

-   **Theorem**: Assume that $f$ and $g$ are convex differentiable
    functions. If the pair $(x, m)$ satisfies the KKT conditions above,
    $x$ is an optimal solution of the problem. If in addition, $f$ is
    strictly convex, $x$ is the only solution of the problem.

    **(Local minimum = global minimum)**

---

Duality and Convexity
---------------------

-   Dual is the NLPP: $$\max\{\theta(\mu, \lambda) \mid \mu \geq 0\},$$
    where
    $\theta(\mu, \lambda) = \inf_x [ f(x) + \mu g(x) + \lambda h(x) ]$

-   Dual problem is always convex.

-   Useful for computing the lower/upper bound.

---

Applications
------------

-   Statistics
-   Filter design
-   Power control
-   Machine learning
    -   SVM classifier
    -   logistic regression

---

class: nord-light, middle, center

Convexify the non-convex's
==========================

---

Change of curvature: square
---------------------------

Transform: $$0.3 \leq {\color{red} \sqrt{x} } \leq 0.4$$ into:
$$0.09 \leq {\color{green} x} \leq 0.16 \, .$$

👉 Note that $\sqrt{\cdot}$ are **monotonic** **concave** functions in
$(0, +\infty)$.

Generalization:
-   Consider $|H(\omega)|^2$ (power) instead of $|H(\omega)|$ (magnitude).
-   square root -> Spectral factorization

---

Change of curvature: square
---------------------------

Transform:
$${\color{red} x^2 } + {\color{red} y^2 } \geq 0.16, \quad \text{(non-convex)}$$
into:
$${\color{green} x'} + {\color{green} y'} \geq 0.16, \quad x', y' \geq 0$$
Then:
$$x_\text{opt} = \pm\sqrt{x'_\text{opt} }, \quad y_\text{opt} = \pm\sqrt{y'_\text{opt} }.$$

---

Change of curvature: sine
-------------------------

Transform:
$${\color{red} \sin^2{x} } \leq 0.4, \quad 0 \leq x \leq \pi/2$$ into:
$${\color{green} y} \leq 0.4, \quad 0 \leq y \leq 1$$ Then:
$$x_\text{opt} = \sin^{-1}(\sqrt{y_\text{opt} }).$$

👉 Note that $\sin^2(\cdot)$ is a monotonic increasing function in
$(0, \pi/2)$.

---

Change of curvature: log
------------------------

Transform: $$\pi \leq {\color{red} x / y} \leq \phi$$ into:
$$\pi' \leq {\color{green} x' - y'} \leq \phi'$$ where $z' = \log(z)$.

Then: $$z_\text{opt} = \exp(z'_\text{opt}).$$

Generalization: - Geometric programming

---

Change of curvature: inverse
----------------------------

Transform: $${\color{red} \log(x) + \frac{0.2}{x} } \leq 0.3, \; x > 0$$
into: $${\color{green} -\log(z) + 0.2 \cdot z } \leq 0.3, \; z > 0 \, .$$

Then: $$x_\text{opt} = z^{-1}_\text{opt}.$$

👉 Note that $\sqrt{\cdot}$, $\log(\cdot)$, and $(\cdot)^{-1}$ are
monotonic functions.

---

Generalize to matrix inequalities
---------------------------------

Transform:
$${\color{red} \log(\det X) + \text{Tr}(X^{-1} C)} \leq 0.3, \; X \succ 0$$
into:
$${\color{green} -\log(\det Z) + \text{Tr}(Z \cdot C)} \leq 0.3, \; Z \succ 0$$

Then: $$X_\text{opt} = Z^{-1}_\text{opt}.$$

---

Change of variables
-------------------

Transform: $$(a +  b \cdot {\color{red} y}) x \leq 0, \; x > 0$$

into: $$a \cdot x + b \cdot {\color{green} z} \leq 0, \; x > 0$$ where
$z = y x$.

Then: $$y_\text{opt} = z_\text{opt} x^{-1}_\text{opt}$$

---

Generalize to matrix inequalities
---------------------------------

Transform:
$$(A + B {\color{red} Y}) X + X (A + B {\color{red} Y})^T  \prec 0, \; X \succ 0$$

into:
$$A X + X A^T + B {\color{green} Z} + {\color{green} Z}^T B^T \prec 0, \; X \succ 0$$
where $Z = Y X$.

Then: $$Y_\text{opt} = Z_\text{opt} X^{-1}_\text{opt}$$

---

Some operations that preserve convexity
---------------------------------------

-   $-f$ is concave if and only if $f$ is convex.
-   Nonnegative weighted sums:
    -   if $w_1, \ldots, w_n \ge 0$ and $f_1, \ldots, f_n$ are all
        convex, then so is $w_1 f_1 + \cdots + w_n f_n.$ In particular,
        the sum of two convex functions is convex.
-   Composition:
    -   If $f$ and $g$ are convex functions and $g$ is non-decreasing
        over a univariate domain, then $h(x) = g(f(x))$ is convex. As an
        example, if $f$ is convex, then so is $e^{f(x)},$ because $e^x$
        is convex and monotonically increasing.
    -   If $f$ is concave and $g$ is convex and non-increasing over a
        univariate domain, then $h(x) = g(f(x))$ is convex.
    -   Convexity is invariant under affine maps.

---

Other thoughts
--------------

-   Minimizing any quasi-convex function subject to convex constraints
    can easily be transformed into a convex programming.
-   Replace a non-convex constraint with a sufficient condition (such as
    its lower bound). Less optimal.
-   Relaxation + heuristic
-   Decomposition

---

Unconstraint Techniques
-----------------------

-   Line search methods
-   Fixed or variable step size
-   Interpolation
-   Golden section method
-   Fibonacci's method
-   Gradient methods
-   Steepest descent
-   Quasi-Newton methods
-   Conjugate Gradient methods

---

General Descent Method
----------------------

1.  **Input**: a starting point $x \in$ dom $f$
2.  **Output**: $x^*$
3.  **repeat**
    1.  Determine a descent direction $p$.
    2.  Line search. Choose a step size $\alpha > 0$.
    3.  Update. $x := x + \alpha p$
4.  **until** stopping criterion satisfied.

---

Some Common Descent Directions
------------------------------

-   Gradient descent: $p = -\nabla f(x)^\mathsf{T}$
-   Steepest descent:
    -   $\triangle x_{nsd}$ =
        argmin$\{\nabla f(x)^\mathsf{T} v \mid \|v\|=1 \}$
    -   $\triangle x$ = $\|\nabla f(x)\| \triangle x_{nsd}$
        (un-normalized)
-   Newton's method:
    -   $p = -\nabla^2 f(x)^{-1} \nabla f(x)$
-   Conjugate gradient method:
    -   $p$ is "orthogonal" to all previous $p$'s
-   Stochastic subgradient method:
    -   $p$ is calculated from a set of sample data (instead of using
        all data)
-   Network flow problems:
    -   $p$ is given by a "negative cycle" (or "negative cut").

---

Approximation Under Constraints
-------------------------------

-   Penalization and barriers
-   Dual method
-   Interior Point method
-   Augmented Lagrangian method

---

📚 Books and Online Resources
----------------------------

-   Pablo Pedregal. Introduction to Optimization, Springer. 2003 (O224
    P371)
-   Stephen Boyd and Lieven Vandenberghe, Convex Optimization, Dec. 2002
-   Mittlemann, H. D. and Spellucci, P. Decision Tree for Optimization
    Software, World Wide Web, http://plato.la.asu.edu/guide.html, 2003

---

class: nord-dark, middle, center

.pull-left[

Q & A 🙋️
========

] .pull-right[

![image](figs/questions-and-answers.svg)

]

    </textarea>
    <script src="../js/remark.min.js"></script>
    <script src="../katex/katex.min.js" type="text/javascript"></script>
    <script
      src="../katex/contrib/auto-render.min.js"
      type="text/javascript"
    ></script>
    <script type="text/javascript">
      renderMathInElement(document.getElementById("source"), {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
        ],
      });
      var slideshow = remark.create({
        ratio: "4:3", // 窗口比例
        // 可选：arta, ascetic, dark, default, far, github, googlecode, idea,
        // ir-black, magula, monokai, rainbow, solarized-dark, solarized-light,
        // sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright,
        // tomorrow-night, tomorrow-night-eighties, vs, zenburn.
        highlightStyle: "tomorrow-night-eighties",
        highlightLines: true,
        countIncrementalSlides: false, // 增量内容是否算一页
        // slideNumberFormat: "", // 若将此参数设置为 ""，将不显示页码
        navigation: {
          scroll: false, // 是否允许使用鼠标滚轮翻页
          touch: true, // （如果是触摸屏）是否允许点击屏幕左边或右边前后翻页
          click: false, // 是否允许鼠标点击屏幕左边或右边前后翻页
        },
      });
    </script>
  </body>
</html>
