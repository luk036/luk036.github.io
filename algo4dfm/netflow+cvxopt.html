<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" type="text/css" href="../katex/katex.min.css"/>
    <link rel="stylesheet" type="text/css" href="slides.css"/>
    <style type="text/css">
      /* Slideshow styles */
      @import url(http://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
    </style>
  </head>
  <body>
    <textarea id="source">

class: middle, center

# When "Network Flow" Meets
# "Convex Optimization"

Wai-Shing Luk

2017-11-16

---

class: middle, center

Introduction
============

---

## Overview

-   Network flow problems can be solved efficiently and have a wide
    range of applications.

-   Unfortunately, some problems might has one additional constraints,
    making the current network flow techniques cannot be solved.

-   Moreover, in some problems, the objective functions is quasi-convex
    instead of the convex one.

-   In this lecture, we investigate some problems that still can be solved
    by network flow techniques with the help of convex optimization.

---

class: middle, center

Parametric potential problems
=============================

---

## Parametric potential problems

Consider:
$$
        \begin{array}{ll}
        \text{maximize}      & g(\beta), \\
        \text{subject to}    & y \leq d(\beta), \\
                             & A u = y,
    \end{array}
$$ 

where $g(\beta)$ and $d(\beta)$ are concave.

>   **Note:** parametric flow problems can be defined similarly.

---

## Network flow says:

-   For fixed $\beta$, the problem is feasible if and only if there
    exists no negative cycle

-   Negative cycle detection can be done efficiently using the
    Bellman-Ford-like methods

-   If a negative cycle $C$ is found, then
    $\sum_{(i,j)\in C} d_{ij}(\beta) < 0$

---

## Convex Optimization says:

-   If both sub-gradients of $g(\beta)$ and $d(\beta)$ are known, then
    the *bisection method* can be used for solving the problem
    efficiently.

-   Also, for multi-parameter problems, the *ellipsoid method* can be
    used.

---

## Quasi-convex Minimization

Consider: 
$$
    \begin{array}{ll}
        \text{maximize}      & f(\beta), \\
        \text{subject to}    & y \leq d(\beta), \\
                             & A u = y,
    \end{array}
$$ 

where $f(\beta)$ is *quasi-convex* and $d(\beta)$ are concave.

---

## Example of Quasi-Convex Functions

-   $\sqrt{|y|}$ is quasi-convex on $\mathbb{R}$

-   $\log(y)$ is quasi-linear on $\mathbb{R}_{++}$

-   $f(y_1,y_2) = y_1 y_2$ is quasi-concave on $\mathbb{R}_{++}^2$

-   Linear-fractional function:

    -   $f(x)$ = $(a^\mathrm{T} x + b)/(c^\mathrm{T} x + d)$

    -   dom $f$ = $\{x \,|\, c^\mathrm{T} x + d > 0 \}$

-   Distance ratio function:

    -   $f(x)$ = $\| x - a\|_2 / \| x - b \|_2$

    -   dom $f$ = $\{x \,|\, \| x - a\|_2 \le \| x - b \|_2 \}$

---

## Convex Optimization says:

If $f$ is quasi-convex, there exists a family of functions $\phi_t$ such
that:

-   $\phi_t(\beta)$ is convex w.r.t. $\beta$ for fixed $t$

-   $\phi_t(\beta)$ is non-increasing w.r.t. $t$ for fixed $\beta$

-   $t$-sublevel set of $f$ is $0$-sublevel set of $\phi_t$, i.e.,
    $f(\beta) \le t$ iff $\phi_t(\beta) \le 0$

For example:

-   $f(\beta) = p(\beta)/q(\beta)$
    with $p$ convex, $q$ concave $p(\beta) \ge 0$, $q(\beta) > 0$ on
    dom $f$,

-   can take $\phi_t(\beta)$ = $p(\beta) - t \cdot q(\beta)$

---

## Convex Optimization says:

Consider a convex feasibility problem:
$$
    \begin{array}{ll}
        \text{find}      & f(\beta), \\
        \text{s. t.}     & \phi_t(\beta) \le 0, \\
                         & y \leq d(\beta),  A u = y,
    \end{array}
$$

-   If feasible, we conclude that $t \ge p^*$;

-   If infeasible, $t < p^*$.

Binary search on $t$ can be used for obtaining $p^*$.

---

## Quasi-convex Network Problem

-   Again, the feasibility problem ([eq:quasi]) can be solved
    efficiently by the bisection method or the ellipsoid method,
    together with the negatie cycle detection technique.

-   Any EDA’s applications ???

---

## Monotonic Minimization

-   Consider the following problem: 
  $$\begin{array}{ll}
    \text{minimize}      & \max_{ij} f_{ij}(y_{ij}), \\
    \text{subject to}    & A u = y,
  \end{array}$$

  where $f_{ij}(y_{ij})$ is non-decreasing.

-   The problem can be recast as: 
  $$\begin{array}{ll}
    \text{maximize}      & \beta, \\
    \text{subject to}    & y \leq f_{ij}^{-1}(\beta), \\
    & A u = y,
  \end{array}$$

  where $f_{ij}^{-1}(\beta)$ is non-deceasing w.r.t. $\beta$.

---

## E.g. Yield-driven Optimization

-   Consider the following problem:

  $$\begin{array}{ll}
    \text{maximize}      & \min_{ij} \Pr(y_{ij} \leq \tilde{d}_{ij}) \\
    \text{subject to}    & A u = y,
  \end{array}$$ 

    where $\tilde{d}_{ij}$ is a random variables.

-   Equivalent to the problem:

  $$\begin{array}{ll}
    \text{maximize}      & \beta, \\
    \text{subject to}    & \beta \leq \Pr(y_{ij} \leq \tilde{d}_{ij}), \\
    & A u = y,
  \end{array}$$

    where $f_{ij}^{-1}(\beta)$ is non-deceasing w.r.t. $\beta$.

---

## E.g. Yield-driven Optimization

- Let $F_{ij}(x)$ is the cdf of $\tilde{d}_{ij}$.

- Then:
    $$\begin{array}{lll}
	&             & \beta \leq \Pr(y_{ij} \leq \tilde{d}_{ij}) \leq t \\
    & \Rightarrow & \beta \leq 1 - F_{ij}(y_{ij}) \\
    & \Rightarrow & y_{ij} \leq F_{ij}^{-1}(1 - \beta)
    \end{array}$$

- The problem becomes:

  $$\begin{array}{ll}
    \text{maximize}      & \beta, \\
    \text{subject to}    & y_{ij} \leq F_{ij}^{-1}(1 - \beta), \\
    & A u = y,
  \end{array}$$

---

## Network flow says:

- Monotonic problem can be solved efficeintly using cycle-cancelling methods such as Howard's algorithm.

---

class: middle, center

Min-cost flow problems
======================

---

## Min-Cost Flow Problem (linear)

Consider:

$$\begin{array}{ll}
            \text{min}   & d^T x + p \\
            \text{s. t.} & c^- \leq x \leq c^+, \\
                         & A^T x = b, \; b(V)=0
\end{array}$$

- some $c^+$ could be $+\infty$ some $c^-$ could be $-\infty$.
- $A^T$ is the incidence matrix of a network $G$.

---

## Conventional Algorithms

- Augmenting path based
	- Start from an infeasible solution
	- Inject minimal flow to the augmenting path while maintaining the infeasibility at every iteration
	- Stop when no flow can be injected to the path.
- Cycle cancelling based
	- Start from a feasible solution $x_0$
	- find a better sol'n $x_1 = x_0 + \alpha \triangle x$, where $\alpha$ is positive and $\triangle x$ is a negative cycle indicator.

---

## General Descent Method

1. **Input**: a starting point $x \in$ dom $f$
2. **Output**: $x^*$
3. **repeat**
    1. Determine a descent direction $p$.
    2. Line search. Choose a step size $\alpha > 0$.
    3. Update. $x := x + \alpha p$
4. **until** stopping criterion satisfied.

---

## Some Common Descent Directions

- For convex problems, the search direction must 
  satisfy $\nabla f(x)^T p < 0$.
- Gradient descent:
	- $p = -\nabla f(x)^T$
- Steepest descent: 
	- $\triangle x_{nsd}$ = argmin$\{\nabla f(x)^T v \mid \|v\|=1 \}$.
	- $\triangle x_{sd}$ = $\|\nabla f(x)\| \triangle x_{nsd}$ (un-normalized)
- Newton's method:
	- $p = -\nabla^2 f(x)^{-1} \nabla f(x)$

---

## Network flow says:
- Here, there is a better way to choose $p$!
- Let $x := x + \alpha p$, then we have:
  $$\begin{array}{lll}
    \text{min}   & d^T x_0 + \alpha d^T p  & \Rightarrow d^T < 0 \\
    \text{s. t.} & -x_0 \leq \alpha p \leq c-x_0 & \Rightarrow \text{residual graph} \\
    & A^T p = 0 & \Rightarrow p \text{ is a cycle!}
  \end{array}$$

- In other words, choose $p$ to be a negative cycle with cost $d$!
	- Simple negative cycle, or
	- Minimum mean cycle

---

## Network flow says:

- Step size is limited by the capacity constraints:
	- $\alpha_1 = \min_{ij} \{c^+ - x_0\}$, for $\triangle x_{ij} > 0$
	- $\alpha_2 = \min_{ij} \{x_0 - c^-\}$, for $\triangle x_{ij} < 0$
	- $\alpha_\mathrm{lin}$ = min$\{\alpha_1, \alpha_2\}$ 
- If $\alpha_\mathrm{lin} = +\infty$, the problem is unbounded.

---

## Network flow says:

- Initial feasible solution can be obtained by a similar construction of residual graph and cost vector.
- LEMON package implements this cycle cancelling algorithm.

---

## Min-Cost Flow Convex Problem

- Problem Formulation:
$$\begin{array}{ll}
            \text{min}   & f(x)  \\
            \text{s. t.} & 0 \leq x \leq c, \\
                         & A^T x = b, \; b(V)=0
\end{array}$$

---

## Common Line Search Types

- Exact line search: $t$ = argmin$_{t>0} f(x + t\triangle x)$
- Backtracking line search (with parameters 
  $\alpha \in (0,1/2), \beta \in (0,1)$)
	- starting at $t = 1$, repeat $t := \beta t$ until
	  $$f(x + t\triangle x) < f(x) + \alpha t \nabla f(x)^T \triangle x$$
	- graphical interpretation: backtrack until $t \leq t_0$

---

## Network flow says:

- Step size is further limited by:
	- $\alpha_\mathrm{cvx} = \min\{\alpha_\mathrm{lin}, t\}$
- At each iteration, choose $\triangle x$ to be a negative cycle of $G_x$, with cost $\nabla f(x)$ such that $\nabla f(x)^T \triangle x < 0$

---

## Quasi-convex Minimization (new)

- Problem Formulation:
  $$\begin{array}{ll}
    \text{min}   & f(x) \\
    \text{s. t.} & 0 \leq x \leq c, \\
     & A^T x = b, \; b(V)=0
  \end{array}$$

- The problem can be recast as:
  $$\begin{array}{ll}
    \text{min}   & t \\
    \text{s. t.} & f(x) \leq t, \\
      & 0 \leq x \leq c, \\
      & A^T x = b, \; b(V)=0
  \end{array}$$

---

## Convex Optimization says:

-   Consider a convex feasibility problem:
  $$\begin{array}{ll}
            \text{find}   & x \\
            \text{s. t.} & \phi_t(x) \leq 0, \\
                         & 0 \leq x \leq c, \\
                         & A^T x = b, \; b(V)=0
  \end{array}$$
	-   If feasible, we conclude that $t \ge p^*$;
	-   If infeasible, $t < p^*$.
-   Binary search on $t$ can be used for obtaining $p^*$.

---

## Network flow says:

- Choose $\triangle x$ to be a negative cycle of 
  $G_x$ with cost $\nabla \phi_t(x)$
- If no negative cycle is found, and $\phi_t(x) > 0$,
  we conclude that the problem is infeasible.
- Iterate until $x$ becomes feasible, i.e. $\phi_t(x) \leq 0$. 

---

## E.g. Linear-Fractional Cost

- Problem Formulation:
  $$\begin{array}{ll}
    \text{min}   & (e^T x + f) / (g^T x + h) \\
    \text{s. t.} & 0 \leq x \leq c, \\
      & A^T x = b, \; b(V)=0
  \end{array}$$

- The problem can be recast as:
  $$\begin{array}{ll}
    \text{min}   & t \\
    \text{s. t.} & (e^T x + f) - t(g^T x + h) \leq 0 \\
    & 0 \leq x \leq c, \\
    & A^T x = b, \; b(V)=0
  \end{array}$$

---

##  Convex Optimization says:

- Consider a convex feasibility problem:
  $$\begin{array}{ll}
            \text{find}   & x \\
            \text{s. t.} & (e - t\cdot g)^T x + (f - t\cdot h) \leq 0, \\
                         & 0 \leq x \leq c, \\
                         & A^T x = b, \; b(V)=0
  \end{array}$$
	-   If feasible, we conclude that $t \ge p^*$;
	-   If infeasible, $t < p^*$.
-   Binary search on $t$ can be used for obtaining $p^*$.


---

## Network flow says:

- Choose $\triangle x$ to be a negative cycle of $G_x$ with cost
  $(e - t\cdot g)$, i.e. $(e - t\cdot g)^T\triangle x < 0$
- If no negative cycle is found, and 
  $(e - t\cdot g)^T x_0 + (f - t\cdot h) > 0$,
  we conclude that the problem is infeasible.
- Iterate until $(e - t\cdot g)^T x_0 + (f - t\cdot h) \leq 0$. 

---

## E.g. Statistical Optimization

- Consider the quasi-convex problem:

  $$\begin{array}{ll}
    \text{min}   & \Pr(\mathbf{d}^T x > \alpha)  \\
    \text{s. t.} & 0 \leq x \leq c, \\
    & A^T x = b, \; b(V)=0
  \end{array}$$

  * $\mathbf{d}$ is random vector with mean $d$ and covariance $\Sigma$.
  * Hence, $\mathbf{d}^T x$ is a random variable with mean $d^T x$ and variance $x^T \Sigma x$.


---

## Statistical Optimization

- The problem can be recast as:
  $$\begin{array}{ll}
    \text{min}   & t \\
    \text{s. t.} & \Pr(\mathbf{d}^T x > \alpha) \leq t \\
    & 0 \leq x \leq c, \\
    & A^T x = b, \; b(V)=0
  \end{array}$$

- Note:
    $$\begin{array}{lll}
	&             & \Pr(\mathbf{d}^T x > \alpha) \leq t \\
    & \Rightarrow & d^T x  + F^{-1}(1-t) \| \Sigma^{1/2} x \|_2 \leq \alpha
    \end{array}$$
   (convex quadratic constraint w.r.t $x$)

---

## Recall...

Recall that the gradient of 
$d^T x  + F^{-1}(1-t) \| \Sigma^{1/2} x \|_2$ is
$d  + F^{-1}(1-t) (\| \Sigma^{1/2} x \|_2)^{-1} \Sigma x$.

---

## Problem w/ additional Constraints (new)

- Problem Formulation:
  $$\begin{array}{ll}
            \text{min}   & f(x) \\
            \text{s. t.} & 0 \leq x \leq c, \\
                         & A^T x = b, \; b(V)=0 \\
                         & \color{green}{s^T x \leq \gamma}
  \end{array}$$

---


## E.g. Yield-driven Delay Padding

- Consider the following problem:
$$\begin{array}{ll}
     \text{maximize}      & \gamma\,\beta - c^T p, \\
     \text{subject to}    & \beta \leq \Pr(y_{ij} \leq \mathbf{d}_{ij} + p_{ij}), \\
                                 & A u = y, \; p \geq 0
\end{array}$$ 
	- $p$: delay padding
	- $\gamma$: weight (determined by a trade-off curve of yield and buffer cost)
	- $\mathbf{d}_{ij}$: Gaussian random variable with mean $d_{ij}$ and variance $s_{ij}$. 

---

## E.g. Yield-driven Delay Padding

.pull-left[
- The problem is equivalent to:
$$\begin{array}{ll}
     \text{max}      & \color{green}{\gamma\,\beta} - c^T p, \\
     \text{s. t.}    & y \leq d \color{green}{- \beta s} + p, \\
                     & A u = y, p \geq 0
\end{array}$$ 

]

.pull-right[

- or its dual:
$$\begin{array}{ll}
    \text{min}   & d^T x  \\
    \text{s. t.} & 0 \leq x \leq c, \\
                 & A^T x = b, \; b(V)=0 \\
                 & \color{green}{s^T x \leq \gamma}
\end{array}$$

]

---

## Recall ...

- Yield drive CSS:
  $$\begin{array}{ll}
     \text{max}      & \beta, \\
     \text{s. t.}    & y \leq d - \beta s, \\
                     & A u = y,
  \end{array}$$ 
- Delay padding
$$\begin{array}{ll}
     \text{max}      & -c^T p, \\
     \text{s. t.}    & y \leq d + p, \\
                     & A u = y, \; p \geq 0
\end{array}$$ 

---

## Considering Barrier Method

- Approximation via logarithmic barrier:

  $$\begin{array}{ll}
    \text{min}   & f(x) + (1/t) \phi(x)\\
    \text{s. t.} & 0 \leq x \leq c, \\
    & A^T x = b, \; b(V)=0 \\
  \end{array}$$

	* where $\phi(x) = -\log (\gamma - s^T x)$
	* Approximation improves as $t \rightarrow \infty$
	* Here, $\nabla \phi(x) =  s / (\gamma - s^T x)$

---

## Barrier Method

- **Input**: a feasible $x$, $t := t^{(0)}$, $\mu > 1$, 
  tolerance $\varepsilon > 0$
- **Output**: $x^*$
- **repeat**
	1. Centering step. Compute $x^*(t)$ by minimizing $t\,f  + \phi$
	2. Update $x := x^*(t)$.
	3. Increase $t$. $t := \mu t$.
- **until** $1/t < \varepsilon$.

> Note: Centering usually done using Newton's method in general.

---

## Network flow says:

In the centering step, instead of using the Newton descent direction, we can replace it with a negative cycle on the residual graph.

</textarea>
<script src="../remark-latest.min.js"></script>
<script src="../katex/katex.min.js" type="text/javascript"></script>
<script src="../katex/contrib/auto-render.min.js" type="text/javascript"></script>
<script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
<script type="text/javascript">
  var hljs = remark.highlighter.engine;
</script>
<script src="terminal.language.js" type="text/javascript"></script>

<script type="text/javascript">
  renderMathInElement(
      document.getElementById("source"),
      {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      }
  );
  var slideshow = remark.create({
    highlightStyle: 'github'
  });
  // extract the embedded styling from ansi spans
  $('code.terminal span.hljs-ansi').replaceWith(function(i, x) {
    return x.replace(/<(\/?(\w+).*?)>/g, '<$1>')
  });
</script>
</body>
</html>