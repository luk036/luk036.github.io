<!DOCTYPE html>
<html>

<head>
  <title>When "Convex Optimization" Meets "Network Flow"</title>
  <meta charset="utf-8" />
  <meta
    name="viewport"
    content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"
  />
  <link rel="stylesheet" type="text/css" href="../katex/katex.min.css"/>
  <link rel="stylesheet" type="text/css" href="../css/quasar.min.css" />
  <link rel="stylesheet" type="text/css" href="../css/pure-min.css" />
  <link rel="stylesheet" type="text/css" href="../css/spaces.css" />
  <link rel="stylesheet" type="text/css" href="../css/typo.css" />
  <link rel="stylesheet" type="text/css" href="../css/devices.min.css" />
  <link rel="stylesheet" type="text/css" href="../css/gh-fork-ribbon.css" />
  <link rel="stylesheet" type="text/css" href="../css/nord-dark.css" />
  <link rel="stylesheet" type="text/css" href="../css/nord-light.css" />
  <link rel="stylesheet" type="text/css" href="../css/font-nord.css" />
  <link rel="stylesheet" type="text/css" href="../css/bg-nord.css" />
  <link rel="stylesheet" type="text/css" href="../css/font-open-color.css" />
  <link rel="stylesheet" type="text/css" href="../css/bg-open-color.css" />
  <link rel="stylesheet" type="text/css" href="../css/material-icons.css" />
  <link rel="stylesheet" type="text/css" href="../css/abs-layout.css" />
  <link rel="stylesheet" type="text/css" href="../css/border-layout.css" />
  <link rel="stylesheet" type="text/css" href="../css/text-rect.css" />
  <link rel="stylesheet" type="text/css" href="../css/text-circle.css" />
  <link rel="stylesheet" type="text/css" href="../css/card.css" />
  <link rel="stylesheet" type="text/css" href="../css/lines.css" />
  <link rel="stylesheet" type="text/css" href="../css/filters.css" />
  <link rel="stylesheet" type="text/css" href="../fonts/remixicon.css" />
  <link rel="stylesheet" type="text/css" href="../css/style.css" />
  <link rel="stylesheet" type="text/css" href="../slides.css" />
</head>
<body>
  <textarea id="source">

layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

When "Convex Optimization" Meets "Network Flow"
===============================================

@luk036
--------

2021-11-10

---

class: nord-light, middle, center

Introduction
============

---

Overview
--------

-   Network flow problems can be solved efficiently and have a wide range of applications.

-   Unfortunately, some problems may have other additional constraints that make them impossible to solve with current network flow techniques.

-   In addition, in some problems, the objective function is quasi-convex rather than convex.

-   In this lecture, we will investigate some problems that can still be solved by network flow techniques with the help of convex optimization.

---

class: nord-light, middle, center

Parametric Potential Problems
=============================

---

Parametric potential problems
-----------------------------

Consider: $$
    \begin{array}{ll}
        \text{maximize}      & g(\beta), \\
        \text{subject to}    & y \leq d(\beta), \\
                             & A u = y,
    \end{array}
$$

where $g(\beta)$ and $d(\beta)$ are concave.

**Note**: the parametric flow problems can be defined in a similar way.

---

Network flow says:
------------------

-   For fixed $\beta$, the problem is feasible precisely when there exists no negative cycle

-   Negative cycle detection can be done efficiently using the Bellman-Ford-like methods

-   If a negative cycle $C$ is found, then $\sum_{(i,j)\in C} d_{ij}(\beta) < 0$

---

Convex Optimization says:
-------------------------

-   If both sub-gradients of $g(\beta)$ and $d(\beta)$ are known, then the *bisection method* can be used for solving the problem efficiently.

-   Also, for multi-parameter problems, the *ellipsoid method* can be used.

---

Quasi-convex Minimization
-------------------------

Consider: $$
    \begin{array}{ll}
        \text{maximize}      & f(\beta), \\
        \text{subject to}    & y \leq d(\beta), \\
                             & A u = y,
    \end{array}
$$

where $f(\beta)$ is *quasi-convex* and $d(\beta)$ are concave.

---

Example of Quasi-Convex Functions
---------------------------------

-   $\sqrt{|y|}$ is quasi-convex on $\mathbb{R}$

-   $\log(y)$ is quasi-linear on $\mathbb{R}_{++}$

-   $f(x, y) = x y$ is quasi-concave on $\mathbb{R}_{++}^2$

-   Linear-fractional function:

    -   $f(x)$ = $(a^\mathsf{T} x + b)/(c^\mathsf{T} x + d)$

    -   dom $f$ = $\{x \,|\, c^\mathsf{T} x + d > 0 \}$

-   Distance ratio function:

    -   $f(x)$ = $\| x - a\|_2 / \| x - b \|_2$

    -   dom $f$ = $\{x \,|\, \| x - a\|_2 \le \| x - b \|_2 \}$

---

Convex Optimization says:
-------------------------

If $f$ is quasi-convex, there exists a family of functions $\phi_t$ such that:

-   $\phi_t(\beta)$ is convex w.r.t. $\beta$ for fixed $t$

-   $\phi_t(\beta)$ is non-increasing w.r.t. $t$ for fixed $\beta$

-   $t$-sublevel set of $f$ is $0$-sublevel set of $\phi_t$, i.e., $f(\beta) \le t$ iff $\phi_t(\beta) \le 0$

For example:

-   $f(\beta) = p(\beta)/q(\beta)$ with $p$ convex, $q$ concave $p(\beta) \ge 0$, $q(\beta) > 0$ on domÂ $f$,

-   can take $\phi_t(\beta)$ = $p(\beta) - t \cdot q(\beta)$

---

Convex Optimization says:
-------------------------

Consider a convex feasibility problem: $$
    \begin{array}{ll}
        \text{find}      & f(\beta), \\
        \text{s. t.}     & \phi_t(\beta) \le 0, \\
                         & y \leq d(\beta),  A u = y,
    \end{array}
$$

-   If feasible, we conclude that $t \ge p^*$;

-   If infeasible, $t < p^*$.

Binary search on $t$ can be used for obtaining $p^*$.

---

Quasi-convex Network Problem
----------------------------

-   Again, the feasibility problemÂ ([eq:quasi]) can be solved efficiently by the bisection method or the ellipsoid method, together with the negatie cycle detection technique.

-   Any EDA's applications ???

---

Monotonic Minimization
----------------------

-   Consider the following problem: $$\begin{array}{ll}
      \text{minimize}      & \max_{ij} f_{ij}(y_{ij}), \\
      \text{subject to}    & A u = y,
    \end{array}$$

    where $f_{ij}(y_{ij})$ is non-decreasing.

-   The problem can be recast as: $$\begin{array}{ll}
      \text{maximize}   & \beta, \\
      \text{subject to} & y \leq f^{-1}(\beta), \\
      & A u = y,
    \end{array}$$

    where $f^{-1}(\beta)$ is non-deceasing w.r.t. $\beta$.

---

E.g. Yield-driven Optimization
------------------------------

-   Consider the following problem:

    $$\begin{array}{ll}
      \text{maximize}      & \min_{ij} \Pr(y_{ij} \leq \tilde{d}_{ij}) \\
      \text{subject to}    & A u = y,
    \end{array}$$

    where $\tilde{d}_{ij}$ is a random variables.

-   Equivalent to the problem:

    $$\begin{array}{ll}
      \text{maximize}      & \beta, \\
      \text{subject to}    & \beta \leq \Pr(y_{ij} \leq \tilde{d}_{ij}), \\
      & A u = y,
    \end{array}$$

    where $f_{ij}^{-1}(\beta)$ is non-deceasing w.r.t. $\beta$.

---

E.g. Yield-driven Optimization (II)
------------------------------

-   Let $F(x)$ is the cdf of $\tilde{d}$.

-   Then: $$\begin{array}{lll}
      &             & \beta \leq \Pr(y_{ij} \leq \tilde{d}_{ij}) \leq t \\
      & \Rightarrow & \beta \leq 1 - F_{ij}(y_{ij}) \\
      & \Rightarrow & y_{ij} \leq F_{ij}^{-1}(1 - \beta)
      \end{array}$$

-   The problem becomes:

    $$\begin{array}{ll}
      \text{maximize}      & \beta, \\
      \text{subject to}    & y_{ij} \leq F_{ij}^{-1}(1 - \beta), \\
      & A u = y,
    \end{array}$$

---

Network flow says
------------------

-   Monotonic problem can be solved efficeintly using cycle-cancelling methods such as Howard's algorithm.

---

class: nord-light, middle, center

Min-cost flow problems
======================

---

Min-Cost Flow Problem (linear)
------------------------------

Consider:

$$\begin{array}{ll}
            \text{min}   & d^\mathsf{T} x + p \\
            \text{s. t.} & c^- \leq x \leq c^+, \\
                         & A^\mathsf{T} x = b, \; b(V)=0
\end{array}$$

-   some $c^+$ could be $+\infty$ some $c^-$ could be $-\infty$.
-   $A^\mathsf{T}$ is the incidence matrix of a network $G$.

---

Conventional Algorithms
-----------------------

-   Augmented-path based:
    -   Start with an infeasible solution
    -   Inject minimal flow into the augmented path while maintaining infeasibility in each iteration
    -   Stop when there is no flow to inject into the path.
-   Cycle cancelling based:
    -   Start with a feasible solution $x_0$
    -   find a better sol'n $x_1 = x_0 + \alpha \triangle x$, where
        $\alpha$ is positive and $\triangle x$ is a negative cycle indicator.

---

General Descent Method
----------------------

1.  **Input**: a starting $x \in$ dom $f$
2.  **Output**: $x^*$
3.  **repeat**
    1.  Determine a descent direction $p$.
    2.  Line search. Choose a step size $\alpha > 0$.
    3.  Update. $x := x + \alpha p$
4.  **until** a stopping criterion is satisfied.

---

Some Common Descent Directions
------------------------------

-   For convex problems, the search direction must satisfy $\nabla f(x)^\mathsf{T} p < 0$.
-   Gradient descent:
    -   $p = -\nabla f(x)^\mathsf{T}$
-   Steepest descent:
    -   $\triangle x^{nsd}$ = argmin$\{\nabla f(x)^\mathsf{T} v \mid \|v\|=1 \}$.
    -   $\triangle x^{sd}$ = $\|\nabla f(x)\| \triangle x^{nsd}$ (un-normalized)
-   Newton's method:
    -   $p = -\nabla^2 f(x)^{-1} \nabla f(x)$

---

Network flow says (II)
------------------

-   Here, there is a better way to choose $p$!
-   Let $x := x + \alpha p$, then we have: $$\begin{array}{lll}
      \text{min}   & d^\mathsf{T} x_0 + \alpha d^\mathsf{T} p  & \Rightarrow d^\mathsf{T} < 0 \\
      \text{s. t.} & -x_0 \leq \alpha p \leq c-x_0 & \Rightarrow \text{residual graph} \\
      & A^\mathsf{T} p = 0 & \Rightarrow p \text{ is a cycle!}
    \end{array}$$
-   In other words, choose $p$ to be a negative cycle with cost $d$!
    -   Simple negative cycle, or
    -   Minimum mean cycle

---

Network flow says (III)
------------------

-   Step size is limited by the capacity constraints:
    -   $\alpha_1 = \min_{ij} \{c^+ - x_0\}$, for $\triangle x_{ij} > 0$
    -   $\alpha_2 = \min_{ij} \{x_0 - c^-\}$, for $\triangle x_{ij} < 0$
    -   $\alpha_\mathrm{lin}$ = min$\{\alpha_1, \alpha_2\}$
-   If $\alpha_\mathrm{lin} = +\infty$, the problem is unbounded.

---

Network flow says (IV)
------------------

-   An initial feasible solution can be obtained by a similar construction of the residual graph and cost vector.
-   The LEMON package implements this cycle cancelling algorithm.

---

Min-Cost Flow Convex Problem
----------------------------

-   Problem Formulation: $$\begin{array}{ll}
              \text{min}   & f(x)  \\
              \text{s. t.} & 0 \leq x \leq c, \\
                           & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$

---

Common Types of Line Search
------------------------

-   Exact line search: $t$ = argmin$_{t>0} f(x + t\triangle x)$
-   Backtracking line search (with parameters $\alpha \in (0,1/2), \beta \in (0,1)$)
    -   starting from $t = 1$, repeat $t := \beta t$ until $$f(x + t\triangle x) < f(x) + \alpha t \nabla f(x)^\mathsf{T} \triangle x$$
    -   graphical interpretation: backtrack until $t \leq t_0$

---

Network flow says (V)
------------------

-   The step size is further limited by the following:
    -   $\alpha_\mathrm{cvx} = \min\{\alpha_\mathrm{lin}, t\}$
-   In each iteration, choose $\triangle x$ as a negative cycle of $G_x$, with cost $\nabla f(x)$ such that $\nabla f(x)^\mathsf{T} \triangle x < 0$

---

Quasi-convex Minimization (new)
-------------------------------

-   Problem Formulation: $$\begin{array}{ll}
      \text{min}   & f(x) \\
      \text{s. t.} & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$

-   The problem can be recast as: $$\begin{array}{ll}
      \text{min}   & t \\
      \text{s. t.} & f(x) \leq t, \\
      & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$

---

Convex Optimization says (II)
-------------------------

-   Consider a convex feasibility problem: $$\begin{array}{ll}
      \text{find}  & x \\
      \text{s. t.} & \phi_t(x) \leq 0, \\
      & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$
    -   If feasible, we conclude that $t \ge p^*$;
    -   If infeasible, $t < p^*$.
-   Binary search on $t$ can be used for obtaining $p^*$.

---

Network flow says (VI)
------------------

-   Choose $\triangle x$ as a negative cycle of $G_x$ with cost $\nabla \phi_t(x)$
-   If no negative cycle is found, and $\phi_t(x) > 0$, we conclude that the problem is infeasible.
-   Iterate until $x$ becomes feasible, i.e. $\phi_t(x) \leq 0$.

---

E.g. Linear-Fractional Cost
---------------------------

-   Problem Formulation: $$\begin{array}{ll}
      \text{min}   & (e^\mathsf{T} x + f) / (g^\mathsf{T} x + h) \\
      \text{s. t.} & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$

-   The problem can be recast as: $$\begin{array}{ll}
      \text{min}   & t \\
      \text{s. t.} & (e^\mathsf{T} x + f) - t(g^\mathsf{T} x + h) \leq 0 \\
      & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$

---

Convex Optimization says (III)
-------------------------

-   Consider a convex feasibility problem: $$\begin{array}{ll}
      \text{find}  & x \\
      \text{s. t.} & (e - t\cdot g)^\mathsf{T} x + (f - t\cdot h) \leq 0, \\
                   & 0 \leq x \leq c, \\
                   & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$
    -   If feasible, we conclude that $t \ge p^*$;
    -   If infeasible, $t < p^*$.
-   Binary search on $t$ can be used for obtaining $p^*$.

---

Network flow says (VII)
------------------

-   Choose $\triangle x$ to be a negative cycle of $G_x$ with cost $(e - t\cdot g)$, i.e. $(e - t\cdot g)^\mathsf{T}\triangle x < 0$
-   If no negative cycle is found, and $(e - t\cdot g)^\mathsf{T} x_0 + (f - t\cdot h) > 0$, we conclude that the problem is infeasible.
-   Iterate until $(e - t\cdot g)^\mathsf{T} x_0 + (f - t\cdot h) \leq 0$.

---

E.g. Statistical Optimization
-----------------------------

-   Consider the quasi-convex problem:

    $$\begin{array}{ll}
      \text{min}   & \Pr(\mathbf{d}^\mathsf{T} x > \alpha)  \\
      \text{s. t.} & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$

    -   $\mathbf{d}$ is random vector with mean $d$ and covariance
        $\Sigma$.
    -   Hence, $\mathbf{d}^\mathsf{T} x$ is a random variable with mean
        $d^\mathsf{T} x$ and variance $x^\mathsf{T} \Sigma x$.

---

Statistical Optimization
------------------------

-   The problem can be recast as: $$\begin{array}{ll}
      \text{min}   & t \\
      \text{s. t.} & \Pr(\mathbf{d}^\mathsf{T} x > \alpha) \leq t \\
      & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0
    \end{array}$$

ğŸ‘‰ Note: $$\begin{array}{lll}
      &             & \Pr(\mathbf{d}^\mathsf{T} x > \alpha) \leq t \\
      & \Rightarrow & d^\mathsf{T} x  + F^{-1}(1-t) \| \Sigma^{1/2} x \|_2 \leq \alpha
    \end{array}$$ (convex quadratic constraint w.r.t $x$)

---

Recall...
---------

Recall that the gradient of $d^\mathsf{T} x + F^{-1}(1-t) \| \Sigma^{1/2} x \|_2$ is $d + F^{-1}(1-t) (\| \Sigma^{1/2} x \|_2)^{-1} \Sigma x$.

---

Problem w/ additional Constraints (new)
---------------------------------------

-   Problem Formulation: $$\begin{array}{ll}
              \text{min}   & f(x) \\
              \text{s. t.} & 0 \leq x \leq c, \\
                           & A^\mathsf{T} x = b, \; b(V)=0 \\
                           & \color{green}{s^\mathsf{T} x \leq \gamma}
    \end{array}$$

---

E.g. Yield-driven Delay Padding
-------------------------------

-   Consider the following problem: $$\begin{array}{ll}
      \text{maximize}   & \gamma\,\beta - c^\mathsf{T} p, \\
      \text{subject to} & \beta \leq \Pr(y_{ij} \leq \mathbf{d}_{ij} + p_{ij}), \\
       & A u = y, \; p \geq 0
    \end{array}$$

    -   $p$: delay padding
    -   $\gamma$: weight (determined by a trade-off curve of yield and buffer cost)
    -   $\mathbf{d}_{ij}$: Gaussian random variable with mean $d_{ij}$ and variance $s_{ij}$.

---

E.g. Yield-driven Delay Padding (II)
-------------------------------

.pull-left[

-   The problem is equivalent to: $$\begin{array}{ll}
       \text{max}  & {\color{green}\gamma\,\beta} - c^\mathsf{T} p, \\
       \text{s.t.} & y \leq d {\color{green}- \beta s} + p, \\
          & A u = y, p \geq 0
    \end{array}$$

]

.pull-right[

-   or its dual: $$\begin{array}{ll}
      \text{min}  & d^\mathsf{T} x  \\
      \text{s.t.} & 0 \leq x \leq c, \\
          & A^\mathsf{T} x = b, \; b(V)=0 \\
          & {\color{green}s^\mathsf{T} x \leq \gamma}
    \end{array}$$

]

---

Recall ...
----------

-   Yield drive CSS: $$\begin{array}{ll}
      \text{max}  & \beta, \\
      \text{s.t.} & y \leq d - \beta s, \\
      & A u = y,
    \end{array}$$

-   Delay padding $$\begin{array}{ll}
      \text{max}  & -c^\mathsf{T} p, \\
      \text{s.t.} & y \leq d + p, \\
      & A u = y, \; p \geq 0
    \end{array}$$

---

Considering Barrier Method
--------------------------

-   Approximation via logarithmic barrier:

    $$\begin{array}{ll}
      \text{min}  & f(x) + (1/t) \phi(x)\\
      \text{s.t.} & 0 \leq x \leq c, \\
      & A^\mathsf{T} x = b, \; b(V)=0 \\
    \end{array}$$

    -   where $\phi(x) = -\log (\gamma - s^\mathsf{T} x)$
    -   Approximation improves as $t \rightarrow \infty$
    -   Here, $\nabla \phi(x) = s / (\gamma - s^\mathsf{T} x)$

---

Barrier Method
--------------

-   **Input**: a feasible $x$, $t := t^{(0)}$, $\mu > 1$, tolerance $\varepsilon > 0$
-   **Output**: $x^*$
-   **repeat**
    1.  Centering step. Compute $x^*(t)$ by minimizing $t\,f + \phi$
    2.  Update $x := x^*(t)$.
    3.  Increase $t$. $t := \mu t$.
-   **until** $1/t < \varepsilon$.

ğŸ‘‰ Note: Centering is usually done by Newton's method in general.

---

Network flow says (VIII)
------------------

In the centering step, instead of using the Newton descent direction, we can replace it with a negative cycle on the residual graph.

---

class: nord-dark, middle, center

Q & A ğŸ—£ï¸
=====

    </textarea>
    <script src="../js/remark.min.js"></script>
    <script src="../katex/katex.min.js" type="text/javascript"></script>
    <script src="../katex/contrib/auto-render.min.js" type="text/javascript"></script>
    <script src="terminal.language.js" type="text/javascript"></script>
    <script type="text/javascript">
      renderMathInElement(
          document.getElementById("source"),
          {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false}
              ]
          }
      );
      var slideshow = remark.create({
        ratio: "4:3", // çª—å£æ¯”ä¾‹
        // å¯é€‰ï¼šarta, ascetic, dark, default, far, github, googlecode, idea,
        // ir-black, magula, monokai, rainbow, solarized-dark, solarized-light,
        // sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright,
        // tomorrow-night, tomorrow-night-eighties, vs, zenburn.
        highlightStyle: "github",
        highlightLines: true,
        countIncrementalSlides: false, // å¢é‡å†…å®¹æ˜¯å¦ç®—ä¸€é¡µ
        // slideNumberFormat: "", // è‹¥å°†æ­¤å‚æ•°è®¾ç½®ä¸º ""ï¼Œå°†ä¸æ˜¾ç¤ºé¡µç 
        navigation: {
          scroll: false, // æ˜¯å¦å…è®¸ä½¿ç”¨é¼ æ ‡æ»šè½®ç¿»é¡µ
          touch: true, // ï¼ˆå¦‚æœæ˜¯è§¦æ‘¸å±ï¼‰æ˜¯å¦å…è®¸ç‚¹å‡»å±å¹•å·¦è¾¹æˆ–å³è¾¹å‰åç¿»é¡µ
          click: false, // æ˜¯å¦å…è®¸é¼ æ ‡ç‚¹å‡»å±å¹•å·¦è¾¹æˆ–å³è¾¹å‰åç¿»é¡µ
        },
      });
      // extract the embedded styling from ansi spans
      $('code.terminal span.hljs-ansi').replaceWith(function (i, x) {
        return x.replace(/<(\/?(\w+).*?)>/g, '<$1>')
      });
    </script>
  </body>
</html>
