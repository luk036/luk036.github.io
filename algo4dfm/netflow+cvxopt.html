<!doctype html>
<html>
  <head>
    <title>When "Convex Optimization" Meets "Network Flow"</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"
    />
    <link rel="stylesheet" type="text/css" href="../katex/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../css/spaces.css" />
    <link rel="stylesheet" type="text/css" href="../css/slides.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-dark.css" />
    <link rel="stylesheet" type="text/css" href="../css/nord-light.css" />
    <link rel="stylesheet" type="text/css" href="../css/font-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/bg-nord.css" />
    <link rel="stylesheet" type="text/css" href="../css/style.css" />
  </head>
  <body>
    <textarea id="source">

layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

# When "Convex Optimization" Meets "Network Flow" ğŸ”„

@luk036 ğŸ‘¨â€ğŸ’»

---

class: nord-light, middle, center

ğŸ¬ Introduction
============

---

Overview ğŸ“‹
--------

-   Network flow problems can be solved efficiently and have a wide range of applications. ğŸŒ

-   Unfortunately, some problems may have other additional constraints that make them impossible to solve with current network flow techniques. ğŸš§

-   In addition, in some problems, the objective function is quasi-convex rather than convex. ğŸ“ˆ

-   In this lecture, we will investigate some problems that can still be solved by network flow techniques with the help of convex optimization. ğŸ”

---

class: nord-light, middle, center

# Parametric Potential Problems

---

## Parametric potential problems ğŸ§®

Consider: 
$$
\begin{array}{ll}
    \text{maximize} & g({\color{darkmagenta}z}), \\
    \text{subject to} & {\color{blue}y} \leq d({\color{darkmagenta}z}), \\
                         & A {\color{red}u} = {\color{blue}y},
\end{array}
$$

where $g({\color{darkmagenta}z})$ and $d({\color{darkmagenta}z})$ are concave.

ğŸ‘‰ Note: the parametric flow problems can be defined in a similar way. ğŸ”„

---

## Network Flow insights: ğŸ”

-   For fixed ${\color{darkmagenta}z}$, the problem is feasible precisely when there exists no negative cycle ğŸ”„

-   Negative cycle detection can be done efficiently using the Bellman-Ford-like methods âš¡

-   If a negative cycle $C$ is found, then $\sum_{(i,j)\in C} d_{ij}({\color{darkmagenta}z}) < 0$

---

## Convex Optimization insights: ğŸ§ 

-   If both sub-gradients of $g({\color{darkmagenta}z})$ and $d({\color{darkmagenta}z})$ are known, then the *bisection method* can be used for solving the problem efficiently. âœ‚ï¸

-   Also, for multi-parameter problems, the *ellipsoid method* can be used. ğŸˆ

---

## Quasi-convex Minimization

Consider:
$$
\begin{array}{ll}
    \text{minimize}      & f({\color{darkmagenta}z}), \\
    \text{subject to}    & {\color{blue}y} \leq d({\color{darkmagenta}z}), \\
                         & A {\color{red}u} = {\color{blue}y},
\end{array}
$$

where $f({\color{darkmagenta}z})$ is *quasi-convex* and $d({\color{darkmagenta}z})$ are concave.

---

## ğŸ“š Example of Quasi-Convex Functions ğŸ“ˆ

-   $\sqrt{|y|}$ is quasi-convex on $\mathbb{R}$ â—

-   $\log(y)$ is quasi-linear on $\mathbb{R}_{++}$

-   $f(x, y) = x y$ is quasi-concave on $\mathbb{R}_{++}^2$ âœ–ï¸

-   Linear-fractional function:

    -   $f(x)$ = $(a^\mathsf{T} x + b)/(c^\mathsf{T} x + d)$ â—

    -   dom $f$ = $\{x \,|\, c^\mathsf{T} x + d > 0 \}$ âœ…

-   Distance ratio function:

    -   $f(x)$ = $\| x - a\|_2 / \| x - b \|_2$ ğŸ“

    -   dom $f$ = $\{x \,|\, \| x - a\|_2 \le \| x - b \|_2 \}$ ğŸ¯

---

## Convex Optimization insights: ğŸ§ 

If $f$ is quasi-convex, there exists a family of functions $\phi_{\color{darkorange}\gamma}$ such that:

-   $\phi_{\color{darkorange}\gamma}({\color{darkmagenta}z})$ is convex w.r.t. ${\color{darkmagenta}z}$ for fixed ${\color{darkorange}\gamma}$ ğŸ“ˆ

-   $\phi_{\color{darkorange}\gamma}({\color{darkmagenta}z})$ is non-increasing w.r.t. ${\color{darkorange}\gamma}$ for fixed ${\color{darkmagenta}z}$

-   ${\color{darkorange}\gamma}$-sublevel set of $f$ is $0$-sublevel set of $\phi_{\color{darkorange}\gamma}$, 

    - i.e., $f({\color{darkmagenta}z}) \le {\color{darkorange}\gamma}$ iff $\phi_{\color{darkorange}\gamma}({\color{darkmagenta}z}) \le 0$ â†”ï¸

For example:

-   $f({\color{darkmagenta}z}) = p({\color{darkmagenta}z})/q({\color{darkmagenta}z})$ with $p$ convex, $q$ concave $p({\color{darkmagenta}z}) \ge 0$, $q({\color{darkmagenta}z}) > 0$ on domÂ $f$,

-   can take $\phi_{\color{darkorange}\gamma}({\color{darkmagenta}z})$ = $p({\color{darkmagenta}z}) - {\color{darkorange}\gamma} \cdot q({\color{darkmagenta}z})$ â–

---

## Convex Optimization insights: ğŸ§ 

Consider a convex feasibility problem:
$$
    \begin{array}{ll}
        \text{find}      & {\color{darkmagenta}z}, \\
        \text{s. t.}     & \phi_{\color{darkorange}\gamma}({\color{darkmagenta}z}) \le 0, \\
                         & {\color{blue}y} \leq d({\color{darkmagenta}z}),  A {\color{red}u} = {\color{blue}y},
    \end{array}
$$

-   If feasible, we conclude that ${\color{darkorange}\gamma} \ge p^*$; âœ…
-   If infeasible, ${\color{darkorange}\gamma} < p^*$. âŒ

Binary search on ${\color{darkorange}\gamma}$ can be used for obtaining $p^*$. ğŸ”

---

## Quasi-convex Network Problem ğŸŒ

-   Again, the feasibility problem ([eq:quasi]) can be solved efficiently by the bisection method or the ellipsoid method, together with the negative cycle detection technique. âš¡

-   Any EDA's applications ??? ğŸ’»

---

## Monotonic Minimization

-   Consider the following problem: $$\begin{array}{ll}
      \text{minimize}      & \max_{ij} f_{ij}({\color{blue}y}_{ij}), \\
      \text{subject to}    & A {\color{red}u} = {\color{blue}y},
    \end{array}$$

    where $f_{ij}({\color{blue}y}_{ij})$ is non-decreasing. ğŸ“ˆ

-   The problem can be recast as: $$\begin{array}{ll}
      \text{maximize} & {\color{darkorange}\beta}, \\
      \text{subject to} & {\color{blue}y} \leq f^{-1}({\color{darkorange}\beta}), \\
      & A {\color{red}u} = {\color{blue}y},
    \end{array}$$

    where $f^{-1}({\color{darkorange}\beta})$ is non-decreasing w.r.t. ${\color{darkorange}\beta}$. ğŸ”„

---

## E.g. Yield-driven Optimization ğŸ¯

-   Consider the following problem:

    $$\begin{array}{ll}
      \text{maximize} & \min_{ij} \Pr({\color{blue}y}_{ij} \leq \tilde{d}_{ij}) \\
      \text{subject to} & A {\color{red}u} = {\color{blue}y},
    \end{array}$$

    where $\tilde{d}_{ij}$ is a random variable. ğŸ²

-   Equivalent to the problem:

    $$\begin{array}{ll}
      \text{maximize} & {\color{darkorange}\beta}, \\
      \text{subject to} & {\color{darkorange}\beta} \leq \Pr({\color{blue}y}_{ij} \leq \tilde{d}_{ij}), \\
      & A {\color{red}u} = {\color{blue}y},
    \end{array}$$

    where $f_{ij}^{-1}({\color{darkorange}\beta})$ is non-decreasing w.r.t. ${\color{darkorange}\beta}$. ğŸ“ˆ

---

## E.g. Yield-driven Optimization (II) ğŸ¯

-   Let $F(x)$ is the cdf of $\tilde{d}$.

-   Then: $$\begin{array}{lll}
      & & {\color{darkorange}\beta} \leq \Pr({\color{blue}y}_{ij} \leq \tilde{d}_{ij}) \\
      & \Rightarrow & {\color{darkorange}\beta} \leq 1 - F_{ij}({\color{blue}y}_{ij}) \\
      & \Rightarrow & {\color{blue}y}_{ij} \leq F_{ij}^{-1}(1 - {\color{darkorange}\beta})
      \end{array}$$

-   The problem becomes:

    $$\begin{array}{ll}
      \text{maximize} & {\color{darkorange}\beta}, \\
      \text{subject to} & {\color{blue}y}_{ij} \leq F_{ij}^{-1}(1 - {\color{darkorange}\beta}), \\
      & A {\color{red}u} = {\color{blue}y},
    \end{array}$$

---

## Network Flow insights ğŸŒ

-   Monotonic problem can be solved efficiently
    using cycle-cancelling methods such as Howard's algorithm. âš¡

---

## ğŸ“š Example - clock period & yield-driven co-optimization â°ğŸ¯

$$
\begin{array}{cll}
   \text{minimize} & {\color{darkmagenta}T_\text{CP} } / {\color{darkmagenta}\beta} \\
   \text{subject to} & {\color{blue}y_{ij} } \le {\color{darkmagenta}T_\text{CP} } - F_{ij}^{-1}({\color{darkmagenta}\beta}), & \forall (i,j) \in E_s \,,\\
                     & {\color{blue}y_{ji} } \le F_{ij}^{-1}(1 - {\color{darkmagenta}\beta}), & \forall (j,i) \in E_h \,, \\
                     & {\color{darkmagenta}T_\text{CP} } \ge 0, \, 0 \le {\color{darkmagenta}\beta} \le 1 \, .
   \end{array}
$$

- ğŸ‘‰ Note that $F_{ij}^{-1}(x)$ is not concave in general in $[0, 1]$. âš ï¸
- Fortunately, we are most likely interested in optimizing circuits
  for high yield rather than the low one in practice. ğŸ¯
- Therefore, by imposing an additional constraint to ${\color{darkmagenta}\beta}$, say
  ${\color{darkmagenta}\beta} \geq 0.8$, the problem becomes convex. âœ…

---

## ğŸ“š Example - clock period & yield-driven co-optimization â°ğŸ¯

The problem can be reformulated as:

$$
\begin{array}{cll}
   \text{minimize}   & {\color{darkorange}\gamma} \\
   \text{subject to} & {\color{darkmagenta}T_\text{CP} } - {\color{darkorange}\gamma} {\color{darkmagenta}\beta} \le 0\\
                     & {\color{blue}y_{ij} } \le {\color{darkmagenta}T_\text{CP} } - F_{ij}^{-1}({\color{darkmagenta}\beta}), & \forall (i,j) \in E_s \,,\\
                     & {\color{blue}y_{ji} } \le F_{ij}^{-1}(1 - {\color{darkmagenta}\beta}), & \forall (j,i) \in E_h \,, \\
                     & {\color{darkmagenta}T_\text{CP} } \ge 0, \, 0.8 \le {\color{darkmagenta}\beta} \le 1 \, .
   \end{array}
$$

---

class: nord-light, middle, center

# Min-cost flow problems ğŸ’°ğŸŒŠ

---

## Min-Cost Flow Problem (linear) ğŸ“ˆ

Consider:

$$\begin{array}{ll}
  \text{min} & d^\mathsf{T} {\color{green}x} + p \\
  \text{s. t.} & c^- \leq {\color{green}x} \leq c^+, \\
    & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
\end{array}$$

-   some $c^+$ could be $+\infty$ some $c^-$ could be $-\infty$. âˆ
-   $A^\mathsf{T}$ is the incidence matrix of a network $G$. ğŸŒ

---

## Conventional Algorithms âš™ï¸

-   Augmented-path based:
    -   Start with an infeasible solution ğŸš§
    -   Inject minimal flow into the augmented path while maintaining infeasibility in each iteration ğŸ”„
    -   Stop when there is no flow to inject into the path. ğŸ›‘
-   Cycle cancelling based:
    -   Start with a feasible solution ${\color{green}x}_0$ âœ…
    -   find a better sol'n ${\color{green}x}_1 = {\color{green}x}_0 + \alpha \triangle {\color{green}x}$, where
        $\alpha$ is positive and $\triangle {\color{green}x}$ is a negative cycle indicator. ğŸ”„

---

## General Descent Method

1.  **Input**: a starting $x \in$ dom $f$ ğŸ
2.  **Output**: $x^*$ ğŸ¯
3.  **repeat**
    1.  Determine a descent direction $p$. ğŸ”½
    2.  Line search. Choose a step size $\alpha > 0$. ğŸ“
    3.  Update. $x := x + \alpha p$ ğŸ”„
4.  **until** a stopping criterion is satisfied. ğŸ›‘

---

## Common Descent Directions ğŸ§­

-   Gradient descent: $p = -\nabla f(x)^\mathsf{T}$
-   Steepest descent:
    -   $\triangle x_{nsd}$ =
        argmin$\{\nabla f(x)^\mathsf{T} v \mid \|v\|=1 \}$ ğŸ”ï¸
    -   $\triangle x$ = $\|\nabla f(x)\| \triangle x_{nsd}$
        (un-normalized) ğŸ“
-   Newton's method:
    -   $p = -\nabla^2 f(x)^{-1} \nabla f(x)$ ğŸ§®
-   Levenberg-Marquardt:
    - Hybrid of Newton and Gradient Descent ğŸ”€
    -   $p = -(\nabla^2 f(x) + \lambda I)^{-1} \nabla f(x)$ âš–ï¸
-   Conjugate gradient method:
    -   $p$ is "orthogonal" to all previous $p$'s â†”ï¸
-   Stochastic subgradient method:
    -   $p$ is calculated from a set of sample data (instead of using
        all data) ğŸ²

---

## Network Flow insights (II) ğŸŒ

-   Here, there is a better way to choose $p$! ğŸ’¡
-   Let ${\color{green}x} := {\color{green}x} + \alpha p$, then we have: $$\begin{array}{lll}
      \text{min}   & d^\mathsf{T} {\color{green}x}_0 + \alpha d^\mathsf{T} p  & \Rightarrow d^\mathsf{T} < 0 \\
      \text{s. t.} & -{\color{green}x}_0 \leq \alpha p \leq c-{\color{green}x}_0 & \Rightarrow \text{residual graph} \\
      & A^\mathsf{T} p = 0 & \Rightarrow p \text{ is a cycle!}
    \end{array}$$
-   In other words, choose $p$ to be a negative cycle with cost $d$! ğŸ”„
    -   Simple negative cycle, or
    -   Minimum mean cycle âš–ï¸

---

## Network Flow insights (III) ğŸŒ

-   Step size is limited by the capacity constraints: âš–ï¸
    -   $\alpha_1 = \min_{ij} \{c^+ - {\color{green}x}^0\}$, 
        for $\triangle {\color{green}x}_{ij} > 0$ â•
    -   $\alpha_2 = \min_{ij} \{{\color{green}x}^0 - c^-\}$,
        for $\triangle {\color{green}x}_{ij} < 0$ â–
    -   $\alpha_\mathrm{lin}$ = min$\{\alpha_1, \alpha_2\}$ â†”ï¸
-   If $\alpha_\mathrm{lin} = +\infty$, the problem is unbounded. âˆ

---

## Network Flow insights (IV) ğŸŒ

-   An initial feasible solution can be obtained by a similar construction of the residual graph and cost vector. ğŸ—ï¸
-   The LEMON package implements this cycle cancelling algorithm. ğŸ‹

---

## Min-Cost Flow Convex Problem ğŸ“ˆ

-   Problem Formulation: $$\begin{array}{ll}
      \text{min} & f({\color{green}x}) \\
      \text{s. t.} & 0 \leq {\color{green}x} \leq c, \\
       & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$

    where $f({\color{green}x})$ is now non-linear but convex.

---
 
## Common Types of Line Search ğŸ”

-   Exact line search: $t$ = argmin$_{t>0} f(x + t\triangle x)$ ğŸ¯
-   Backtracking line search (with parameters $\alpha \in (0,1/2), \beta \in (0,1)$) â†©ï¸
    -   starting from $t = 1$, repeat $t := \beta t$ until
        $$f(x + t\triangle x) < f(x) + \alpha t \nabla f(x)^\mathsf{T} \triangle x$$
    -   graphical interpretation: backtrack until $t \leq t_0$

---

## Network Flow insights (V) ğŸŒ

-   The step size is further limited by the following: âš–ï¸
    -   $\alpha_\mathrm{cvx} = \min\{\alpha_\mathrm{lin}, t\}$ â†”ï¸
-   In each iteration, choose $\triangle {\color{green}x}$ as a negative cycle of $G_x$,
    with cost $\nabla f({\color{green}x})$ such that $\nabla f({\color{green}x})^\mathsf{T} \triangle {\color{green}x} < 0$ ğŸ”„

---

## Quasi-convex Minimization

-   Problem Formulation: $$\begin{array}{ll}
      \text{min} & f({\color{green}x}) \\
      \text{s. t.} & 0 \leq {\color{green}x} \leq c, \\
      & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$

-   The problem can be recast as: $$\begin{array}{ll}
      \text{min} & {\color{darkorange}\gamma} \\
      \text{s. t.} & f({\color{green}x}) \leq {\color{darkorange}\gamma}, \\
      & 0 \leq {\color{green}x} \leq c, \\
      & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$

---

## Convex Optimization insights (II) ğŸ§ 

-   Consider a convex feasibility problem: $$\begin{array}{ll}
      \text{find} & {\color{green}x} \\
      \text{s. t.} & \phi_{\color{darkorange}\gamma}({\color{green}x}) \leq 0, \\
      & 0 \leq {\color{green}x} \leq c, \\
      & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$
    -   If feasible, we conclude that ${\color{darkorange}\gamma} \ge p^*$; âœ…
    -   If infeasible, ${\color{darkorange}\gamma} < p^*$. âŒ
-   Binary search on ${\color{darkorange}\gamma}$ can be used for obtaining $p^*$. ğŸ”

---

## Network Flow insights (VI) ğŸŒ

-   Choose $\triangle {\color{green}x}$ as a negative cycle of $G_x$
    - with cost $\nabla \phi_{\color{darkorange}\gamma}({\color{green}x})$ ğŸ”„
-   If no negative cycle is found, and $\phi_{\color{darkorange}\gamma}({\color{green}x}) > 0$, we conclude that the problem is infeasible. ğŸš«
-   Iterate until ${\color{green}x}$ becomes feasible, i.e. $\phi_{\color{darkorange}\gamma}({\color{green}x}) \leq 0$. âœ…

---

## E.g. Linear-Fractional Cost â—

-   Problem Formulation: $$\begin{array}{ll}
      \text{min}   & (e^\mathsf{T} {\color{green}x} + f) / (g^\mathsf{T} {\color{green}x} + h) \\
      \text{s. t.} & 0 \leq {\color{green}x} \leq c, \\
      & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$

-   The problem can be recast as: $$\begin{array}{ll}
      \text{min}   & {\color{darkorange}\gamma} \\
      \text{s. t.} & (e^\mathsf{T} {\color{green}x} + f) - {\color{darkorange}\gamma}(g^\mathsf{T} {\color{green}x} + h) \leq 0 \\
      & 0 \leq {\color{green}x} \leq c, \\
      & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$

---

## Convex Optimization insights (III) ğŸ§ 

-   Consider a convex feasibility problem: $$\begin{array}{ll}
      \text{find}  & {\color{green}x} \\
      \text{s. t.} & (e - {\color{darkorange}\gamma}\cdot g)^\mathsf{T} {\color{green}x} + (f - {\color{darkorange}\gamma}\cdot h) \leq 0, \\
                   & 0 \leq {\color{green}x} \leq c, \\
                   & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$
    -   If feasible, we conclude that ${\color{darkorange}\gamma} \ge p^*$; âœ…
    -   If infeasible, ${\color{darkorange}\gamma} < p^*$. âŒ
-   Binary search on ${\color{darkorange}\gamma}$ can be used for obtaining $p^*$. ğŸ”

---

## Network Flow insights (VII) ğŸŒ

-   Choose $\triangle {\color{green}x}$ to be a negative cycle of $G_x$
    - with cost $(e - {\color{darkorange}\gamma}\cdot g)$, i.e. $(e - {\color{darkorange}\gamma}\cdot g)^\mathsf{T}\triangle {\color{green}x} < 0$ ğŸ”„
-   If no negative cycle is found, and $(e - {\color{darkorange}\gamma}\cdot g)^\mathsf{T} {\color{green}x}_0 + (f - {\color{darkorange}\gamma}\cdot h) > 0$, we conclude that the problem is infeasible. ğŸš«
-   Iterate until $(e - {\color{darkorange}\gamma}\cdot g)^\mathsf{T} {\color{green}x}_0 + (f - {\color{darkorange}\gamma}\cdot h) \leq 0$. âœ…

---

## E.g. Statistical Optimization

-   Consider the quasi-convex problem:

    $$\begin{array}{ll}
      \text{min} & \Pr(\mathbf{d}^\mathsf{T} {\color{green}x} > \alpha) \\
      \text{s. t.} & 0 \leq {\color{green}x} \leq c, \\
      & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$

    -   $\mathbf{d}$ is random vector with mean $d$ and covariance
        $\Sigma$. ğŸ²
    -   Hence, $\mathbf{d}^\mathsf{T} {\color{green}x}$ is a random variable with mean
        $d^\mathsf{T} {\color{green}x}$ and variance ${\color{green}x}^\mathsf{T} \Sigma {\color{green}x}$.

---

## Statistical Optimization ğŸ“ˆ

-   The problem can be recast as: $$\begin{array}{ll}
      \text{min} & {\color{darkorange}\gamma} \\
      \text{s. t.} & \Pr(\mathbf{d}^\mathsf{T} {\color{green}x} > \alpha) \leq {\color{darkorange}\gamma} \\
      & 0 \leq {\color{green}x} \leq c, \\
      & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
    \end{array}$$

ğŸ‘‰ Note: $$\begin{array}{lll}
      & & \Pr(\mathbf{d}^\mathsf{T} {\color{green}x} > \alpha) \leq {\color{darkorange}\gamma} \\
      & \Rightarrow & d^\mathsf{T} {\color{green}x}  + F^{-1}(1-{\color{darkorange}\gamma}) \| \Sigma^{1/2} {\color{green}x} \|_2 \leq \alpha
    \end{array}$$ (convex quadratic constraint w.r.t ${\color{green}x}$)


-   Recall that the gradient of $d^\mathsf{T} {\color{green}x} + F^{-1}(1-{\color{darkorange}\gamma}) \| \Sigma^{1/2} {\color{green}x} \|_2$ is:

 $$d + F^{-1}(1-{\color{darkorange}\gamma}) (\| \Sigma^{1/2} {\color{green}x} \|_2)^{-1} \Sigma {\color{green}x}$$.

---

## Problem with additional Constraints ğŸš§

-   Problem Formulation: $$\begin{array}{ll}
              \text{min} & f({\color{green}x}) \\
              \text{s. t.} & 0 \leq {\color{green}x} \leq c, \\
                           & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0 \\
                           & \color{brown}{s^\mathsf{T} x \leq \gamma}
    \end{array}$$

---

## E.g. Yield-driven Delay Padding â³ğŸ¯

-   Consider the following problem: $$\begin{array}{ll}
      \text{maximize} & \gamma\,\beta - c^\mathsf{T} p, \\
      \text{subject to} & \beta \leq \Pr({\color{blue}y}_{ij} \leq \mathbf{d}_{ij} + p_{ij}), \\
       & A {\color{red}u} = {\color{blue}y}, \; p \geq 0
    \end{array}$$

    -   $p$: delay padding â³
    -   $\gamma$: weight âš–ï¸
    -   $\mathbf{d}_{ij}$: Gaussian random variable, with
        - mean $d_{ij}$, and
        - variance $s_{ij}$.

---

## E.g. Yield-driven Delay Padding (II) â³ğŸ¯

.pull-left[

-   The problem is equivalent to: $$\begin{array}{ll}
       \text{max} & {\color{brown}\gamma\,\beta} - c^\mathsf{T} p, \\
       \text{s.t.} & {\color{blue}y} \leq d {\color{brown}- \beta s} + p, \\
          & A {\color{red}u} = {\color{blue}y}, p \geq 0
    \end{array}$$

]

.pull-right[

-   or its dual: $$\begin{array}{ll}
      \text{min} & d^\mathsf{T} {\color{green}x}  \\
      \text{s.t.} & 0 \leq {\color{green}x} \leq c, \\
          & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0 \\
          & {\color{brown}s^\mathsf{T} x \leq \gamma}
    \end{array}$$

]

---

## Recall ... ğŸ”„

-   Yield drive CSS: $$\begin{array}{ll}
      \text{max} & \beta, \\
      \text{s.t.} & {\color{blue}y} \leq d - \beta s, \\
      & A {\color{red}u} = {\color{blue}y},
    \end{array}$$

-   Delay padding $$\begin{array}{ll}
      \text{max} & -c^\mathsf{T} p, \\
      \text{s.t.} & {\color{blue}y} \leq d + p, \\
      & A {\color{red}u} = {\color{blue}y}, \; p \geq 0
    \end{array}$$

---

## Considering Barrier Method ğŸš§

-   Approximation via logarithmic barrier:

$$
\begin{array}{ll}
  \text{min} & f({\color{green}x}) + (1/t) \phi({\color{green}x})\\
  \text{s. t.} & 0 \leq {\color{green}x} \leq c, \\
               & A^\mathsf{T} {\color{green}x} = b, \; b(V)=0
\end{array}
$$

  - where $\phi({\color{green}x}) = -\log (\gamma - s^\mathsf{T} {\color{green}x})$
  - Approximation improves as $t \rightarrow \infty$ âˆ
  - Here, $\nabla \phi({\color{green}x}) = s / (\gamma - s^\mathsf{T} {\color{green}x})$ â—

---

## Barrier Method âš™ï¸

-   **Input**: a feasible $x$, $t := t^{(0)}$, $\mu > 1$, tolerance $\varepsilon > 0$ ğŸ
-   **Output**: $x^*$ ğŸ¯
-   **repeat**
    1.  Centering step. Compute $x^*(t)$ by minimizing $t\,f + \phi$ âš–ï¸
    2.  Update $x := x^*(t)$. ğŸ”„
    3.  Increase $t$. $t := \mu t$ ğŸ“ˆ
-   **until** $1/t < \varepsilon$. ğŸ›‘

ğŸ‘‰ Note: Centering is usually done by Newton's method in general. ğŸ§®

---

## Network Flow insights (VIII) ğŸŒ

In the centering step, instead of using the Newton descent direction, we can replace it with a negative cycle on the residual graph. ğŸ”„

---

## Conclusion: A Synergistic Future ğŸ”„âœ¨

The intersection of convex optimization and network flow theory provides a powerful framework for tackling a wide range of optimization problems that exhibit both network structure and convex (or quasi-convex) characteristics. By leveraging the efficient algorithms and structural insights from network flow theory, combined with the generality and robustness of convex optimization techniques, we can address problems that would be intractable using either approach alone. ğŸ’ª

---

class: nord-dark, middle, center

Q & A ğŸ¤
=====
    </textarea>
    <script src="../js/remark.min.js"></script>
    <script src="../katex/katex.min.js" type="text/javascript"></script>
    <script
      src="../katex/contrib/auto-render.min.js"
      type="text/javascript"
    ></script>
    <script src="terminal.language.js" type="text/javascript"></script>
    <script type="text/javascript">
      renderMathInElement(document.getElementById("source"), {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
        ],
      });
      var slideshow = remark.create({
        ratio: "4:3", // çª—å£æ¯”ä¾‹
        // å¯é€‰ï¼šarta, ascetic, dark, default, far, github, googlecode, idea,
        // ir-black, magula, monokai, rainbow, solarized-dark, solarized-light,
        // sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright,
        // tomorrow-night, tomorrow-night-eighties, vs, zenburn.
        highlightStyle: "tomorrow-night-eighties",
        highlightLines: true,
        countIncrementalSlides: false, // å¢é‡å†…å®¹æ˜¯å¦ç®—ä¸€é¡µ
        // slideNumberFormat: "", // è‹¥å°†æ­¤å‚æ•°è®¾ç½®ä¸º ""ï¼Œå°†ä¸æ˜¾ç¤ºé¡µç 
        navigation: {
          scroll: false, // æ˜¯å¦å…è®¸ä½¿ç”¨é¼ æ ‡æ»šè½®ç¿»é¡µ
          touch: true, // ï¼ˆå¦‚æœæ˜¯è§¦æ‘¸å±ï¼‰æ˜¯å¦å…è®¸ç‚¹å‡»å±å¹•å·¦è¾¹æˆ–å³è¾¹å‰åç¿»é¡µ
          click: false, // æ˜¯å¦å…è®¸é¼ æ ‡ç‚¹å‡»å±å¹•å·¦è¾¹æˆ–å³è¾¹å‰åç¿»é¡µ
        },
      });
      // extract the embedded styling from ansi spans
      $("code.terminal span.hljs-ansi").replaceWith(function (i, x) {
        return x.replace(/<(\/?(\w+).*?)>/g, "<$1>");
      });
    </script>
  </body>
</html>
