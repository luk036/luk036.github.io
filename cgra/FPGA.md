layout: true
class: typo, typo-selection

---

count: false
class: nord-dark, middle, center

# ğŸ”Œ FPGA Architecture: Principles and Progression

@luk036 ğŸ‘¨â€ğŸ’»

2025-05-26 ğŸ“…

---

## ğŸ’¡ What are FPGAs? ğŸ¤”

*   Field-Programmable Gate Arrays (FPGAs) are **reconfigurable computer chips** ğŸ§ ğŸ’».
*   They can be programmed to implement **any digital hardware circuit** ğŸ› ï¸.
*   Consist of an array of different types of programmable blocks (logic, IO, others) ğŸ§©.
*   These blocks are **flexibly interconnected** using pre-fabricated routing tracks with programmable switches ğŸ›¤ï¸.
*   Functionality is controlled by millions of **SRAM cells programmed at runtime** ğŸ’¾.
*   User designs are typically described in HDL (Verilog, VHDL) or translated from high-level languages (C, OpenCL) using High-Level Synthesis (HLS) ğŸ“š.
*   Designs are compiled into a **bitstream file** that programs the SRAM cells âš™ï¸.

---

## âœ¨ Why Use FPGAs? Advantages ğŸŒŸ

*   **Low-level hardware reconfigurability** allows for much faster design cycles and lower development costs compared to custom-designed chips (ASICs) âš¡ğŸ’°.
*   **Much lower non-recurring engineering (NRE) cost** and **shorter time-to-market** compared to building a custom ASIC ğŸš€â±ï¸.
*   A pre-fabricated, off-the-shelf FPGA can implement a complete system in weeks ğŸ“….
*   **Skips physical design, layout, fabrication, and verification stages** required for ASICs ğŸ—ï¸â¡ï¸ğŸ¯.
*   Allows **continuous hardware upgrades** after deployment by loading a new bitstream ("field-programmable") ğŸ”„ğŸ’¡.
*   Compelling solution for **medium and small volume designs** ğŸ“Š.

---

## ğŸ“ˆ FPGAs vs. Alternatives âš–ï¸

*   **Bit-level reconfigurability** allows FPGAs to implement the exact hardware needed for an application (customizable datapath, pipeline, memory, etc.) ğŸ›ï¸.
*   This contrasts with the **fixed, one-size-fits-all architecture** of CPUs and GPUs ğŸ”„.
*   Consequently, FPGAs can achieve **higher efficiency** than CPUs or GPUs by implementing instruction-free streaming hardware or customized processor overlays ğŸï¸ğŸ’¨.
*   **However, flexibility comes at an efficiency cost vs. ASICs** âš ï¸.
*   Circuits using only programmable logic average **35x larger and 4x slower** than ASIC implementations ğŸ“â±ï¸.
*   For designs using other blocks (RAMs, DSPs), the area gap reduces but is still **~9x** ğŸ“‰.
*   FPGA architects constantly seek to reduce this gap while maintaining programmability ğŸ”.

---

## ğŸ—ºï¸ FPGA Architecture Components - Progression ğŸš€

*   FPGAs evolved from **simple arrays** of programmable logic and IO blocks ğŸ§±.
*   They are now **complex heterogeneous multi-die systems** ğŸ—ï¸.
*   Modern FPGAs include:
    *   Embedded Block RAMs (BRAMs) ğŸ’¾
    *   Digital Signal Processing (DSP) blocks ğŸ§®
    *   Processor subsystems
    *   Diverse high-performance external interfaces ğŸŒ
    *   System-level interconnect (Network-on-Chip) ğŸ›£ï¸
*   All blocks are interconnected using **bit-level programmable routing** ğŸ”Œ.

---

## ğŸ§ª Evaluating FPGA Architectures

*   Evaluating new FPGA architecture ideas involves a complex flow ğŸ”„.
*   Three main components:
    1.  **Benchmark applications**: Representative circuits to test the architecture ğŸ§ª (e.g., VTR, Titan suites)
    2.  **Architecture model**: Captures design decisions from high-level organization to transistor-level details ğŸ“. Includes architecture description file, area, timing, and power models.
    3.  **Computer-Aided Design (CAD) system**: Maps benchmark applications onto the architecture ğŸ—ºï¸. Involves synthesis, mapping, placement, and routing.
*   Key metrics evaluated: **Total area, maximum frequency (timing), and power consumption** ğŸ“Š.
*   Metrics are averaged across benchmarks.
*   Evaluation blends metrics based on architecture goals (e.g., high performance vs. low power) âš–ï¸.

---

count: false
class: nord-light, middle, center

# Programmable Logic

---

## ğŸ§± Programmable Logic - Evolution

*   Earliest reconfigurable devices: **Programmable Array Logic (PALs)** ğŸ›ï¸.
    *   Array of AND gates feeding OR gates â•.
    *   Implement two-level sum-of-products.
    *   Configurability via programmable switches selecting inputs ğŸ”˜.
    *   **Did not scale well** due to increasing wire length and quadratic growth in switches ğŸ“ˆ.
*   **Complex Programmable Logic Devices (CPLDs)**: Integrated multiple PALs with crossbar interconnect ğŸ”€. Solved some scalability but required more complex tools.
*   **Lookup-Table-based (LUT-based) FPGAs**: Pioneered by Xilinx in 1984 ğŸš€.
    *   Array of SRAM-based LUTs with programmable interconnect ğŸ§©.
    *   **Scaled very well** and achieved much higher area efficiency than PAL/CPLD logic ğŸ“Š.
    *   **LUTs are now the fundamental logic element** in all commercial FPGAs.
*   Alternative ideas like And-Inverter Cones (AICs) were investigated but found to have significantly larger area than LUTs âŒ.

---


## ğŸ§© LUTs and BLEs ğŸ§ 

*   A **K-LUT** implements any K-input Boolean function by storing its truth table in SRAM cells ğŸ’¾.
*   Inputs act as multiplexer select lines to choose an output from the 2^K truth table values ğŸ”¢.
*   Circuit implementation uses **pass-transistor logic** with internal and output buffers to mitigate delay â±ï¸.
*   **Basic Logic Element (BLE)**: A K-LUT coupled with an output register (Flip-Flop) and bypassing multiplexers ğŸ”„. Can implement just an FF or a LUT with registered/unregistered output.
*   BLEs are typically clustered in **Logic Blocks (LBs)** ğŸ¢.
    *   An LB contains N BLEs and **local interconnect** (multiplexers) ğŸ”Œ.
    *   Local interconnect forms a local full or partial crossbar.

---

## ğŸ“ Sizing LUTs and LBs ğŸ“

*   Over time, the size of LUTs (K) and LBs (N) has increased with device capacity ğŸ“ˆ.
*   Increasing K packs more functionality into a LUT, reducing LUT count and critical path logic levels, increasing performance âš¡.
*   Increasing N captures more connections into fast local interconnect, decreasing demand for inter-LB routing ğŸ›£ï¸.
*   **Trade-offs** âš–ï¸:
    *   LUT area increases exponentially with K (due to 2^K SRAMs) ğŸ“Š.
    *   LUT speed degrades linearly with K â±ï¸.
    *   Crossbar local interconnect size increases quadratically and speed degrades linearly with N ğŸ“ˆ.
*   Empirical studies suggest **4-6 LUTs and 3-10 BLEs per LB** offer the best area-delay product âš–ï¸. 4-LUTs for better area, 6-LUTs for higher speed.
*   Historically, early FPGAs had smaller LBs (e.g., Xilinx XC2000 had 2 3-LUTs, N=2, K=3) ğŸ›ï¸. Size gradually increased (e.g., Virtex family: 4 4-LUTs, N=4, K=4; Apex 20K: 10 4-LUTs, N=10, K=4).

---

## Fracturable LUTs ğŸ§©

*   Introduced by Altera in 2003 (Stratix II) ğŸš€.
*   Goal: Combine performance of larger LUTs with area efficiency of smaller ones âš–ï¸.
*   Problem: Traditional 6-LUTs often under-utilized (64% use <6 inputs) â—.
*   A **fracturable {K, M}-LUT** can be configured as a single K-LUT or fractured into two LUTs up to size K-1, using â‰¤ K+M distinct inputs ğŸ”€.
*   Example: A 6-LUT internally composed of two 5-LUTs plus a 2:1 multiplexer ğŸ”Œ.
    *   Without extra ports, two 5-LUTs share all 5 inputs (limited packing) âš ï¸.
    *   Adding extra input ports and steering muxes (Fig 5(b)) makes it easier to pack two functions (e.g., Stratix II ALM: {6, 2}-LUT, 8 inputs, 2 outputs) âœ…. Can implement 6-LUT or two 5-LUTs sharing 2 inputs (total 8 distinct inputs).

---

*   Fracturable LUTs improved performance and reduced logic/routing area ğŸ“ˆ.
*   Xilinx adopted a similar approach (Virtex-5), minimizing extra circuitry for lower area but harder packing âš–ï¸.
*   Recent studies suggest tightly coupled 4-LUTs can achieve performance near 6-LUTs with 4-LUT area/power benefits ğŸ’¡.
*   LB size continues to increase (e.g., Xilinx Versal: 32 BLEs) due to poor inter-LB wire delay scaling and to aid CAD tool runtime â±ï¸.

---

## Registers (FFs) â±ï¸

*   Early FPGAs coupled a non-fracturable LUT with a single FF ğŸ›ï¸.
*   With fracturable LUTs, FPGAs added a second FF per BLE to register both outputs of a fractured LUT ğŸ”„.
*   Stratix V increased FFs to four per BLE to support deeply pipelined designs for higher performance âš¡. Shared inputs between LUTs and FFs using low-cost multiplexing.
*   Stratix V also implemented FFs as **pulse latches** (Fig 6(b)) instead of edge-triggered FFs (Fig 6(a)) ğŸ”„.
    *   Removes one latch stage, reducing register delay and area ğŸ“‰.
    *   Acts as a cheaper FF with worse hold time âš ï¸.
    *   Pulse generators are shared within LBs; pulse width is programmable âš™ï¸.
    *   Allows limited time borrowing. Hold violations can be solved by routing ğŸ›£ï¸.
*   Xilinx also uses pulse latches in Ultrascale+.

---

## Arithmetic Hardening â—

*   Arithmetic operations (add, subtract) are very common (22% of logic elements in some designs) ğŸ”¢.
*   Implementing arithmetic with LUTs is inefficient (e.g., ripple carry adder bit needs two LUTs) âŒ. Leads to high logic utilization and slow critical paths due to series connections for carries.
*   **All modern FPGAs include hardened arithmetic circuitry** in logic blocks âœ….
*   Common principles for hardening arithmetic:
    *   Re-use existing LUT routing ports ğŸ”Œ.
    *   Carry bits propagated on **special, dedicated interconnect** for speed âš¡.

---

*   Hardening ripple carry structures yields large speed gains over LUTs (~3-4x for 32-bit adder) ğŸ“ˆ. Hardening carry skip adders further improves speed.
*   Xilinx Versal: Hardens carry logic for 8-bit look-ahead, implements sum/propagate/generate logic in LUTs [26, 29(a)]. 1-bit arithmetic per logic element ğŸ”¢.
*   Intel Agilex: Hardens 2-bit carry-skip adders fed by 4-LUTs within a 6-LUT [26, 29(b)]. 2-bits arithmetic per logic element ğŸ”¢.
*   Fracturable LUTs + 2 bits arithmetic is shown to be efficient. Dedicated arithmetic circuits increase average performance (75% for arithmetic microbenchmarks, 15% for general circuits) ğŸ“Š.

---

## Hardening for DL ğŸ§ 

*   Deep Learning (DL) workloads are increasingly important, with MAC operations as the core âš™ï¸.
*   Low-precision MACs (8-bit or narrower) can be implemented efficiently in programmable logic ğŸ¯. Uses LUTs for partial products and an adder tree for accumulation.
*   Increasing density of hardened adders in logic fabric enhances performance for arithmetic-heavy apps like DL acceleration ğŸš€.
*   Proposals incorporate **4 bits of arithmetic per logic element** ğŸ”¢.
*   These proposals use existing routing ports, leveraging input sharing in multiplier arrays ğŸ”Œ.
*   Promising proposals increase MAC density by ~1.7x and improve speed, also reducing logic/routing area for general benchmarks (8%). **More arithmetic density benefits applications beyond DL** ğŸŒŸ.

---

count: false
class: nord-light, middle, center

# Programmable Routing

---

## ğŸ›£ï¸ Programmable Routing - Overview ğŸ—ºï¸

*   Programmable routing is **critical**, commonly accounting for **over 50%** of both fabric area and critical path delay âš ï¸.
*   Composed of **pre-fabricated wiring segments** and **programmable switches** ğŸ”Œ.
*   Allows connecting any function block output to any input by programming switches âš™ï¸.

---

*   Two main classes of architecture:
    1.  **Hierarchical FPGAs**: Inspired by design hierarchy ğŸ—ï¸. Use short wires for nearby connections, longer wires/multiple switches for distant ones (Fig 8). Popular in earlier FPGAs (Altera 7K, Apex 20K). Long wires at upper levels became problematic with process scaling (increasingly resistive) âš ï¸. Less efficient for physically close blocks requiring multiple hops. Primarily used today for smaller FPGAs or embedded IP cores.
    2.  **Island-style FPGAs**: Pioneered by Xilinx ğŸï¸. Regular 2D layout of horizontal/vertical directed wire segments (Fig 9). Includes **routing wire segments, connection blocks** (block inputs to wires), and **switch blocks** (wires to wires). Most connections span small distances, implemented with few wires.

---

## Key Parameters & Switches âš™ï¸

*   Good routing architecture balances connectivity for routability with minimizing wasted area âš–ï¸.
*   Matches application needs: short wires for short connections (minimize capacitance/area), longer wires for long connections (avoid switch delay).
*   Parameters include:
    *   Connectivity from block pins to wires (Fc) ğŸ”Œ
    *   Connectivity from wire ends to other wires (Fs) ğŸ”€
    *   Wire segment lengths ğŸ“
    *   Routing switch pattern ğŸ›ï¸
    *   Electrical design of wires and switches âš¡
    *   Number of wires per channel ğŸ›£ï¸

---

*   **Switch Design (Fig 10)**:
    *   Early FPGAs: **Pass gate transistors** controlled by SRAM. Smallest, but delay grows quadratically in series âš ï¸.
    *   Adding **tri-state buffers** improves speed at area cost âš–ï¸.
    *   Most recent FPGAs: **Multiplexer built from pass gates followed by a buffer** (Fig 4(b)). Most efficient circuit design for most cases âœ…. Called **direct drive switch**. Pass transistors are small, buffer drives wire capacitance. Wire can only be driven at one point. Direct drive improves area and speed.
    *   Exceptions: Expensive/rare wires (long wires on wide metal, interposer wires) may use **multiple tri-state buffers** for more flexible usage ğŸ”Œ.

---

## Wire Segmentation & Delay â±ï¸

*   Early island-style used only short wires (spanning 1 logic block) ğŸ›ï¸.
*   Later research showed longer segments improved efficiency (4 logic blocks segment reduced delay by 40%, routing area by 25%) ğŸ“ˆ.
*   Modern architectures include **multiple lengths**, but most plentiful are moderate length (e.g., 4 logic blocks) ğŸ“.
*   Longer segments (e.g., 16 logic blocks) achieve lower delay for long connections but require wide/thick metal on upper layers, which are limited âš ï¸.
*   Some architectures (Intel Stratix) connect long wires only to short wires, creating a routing hierarchy within island-style ğŸ—ï¸.
*   Local routing within logic blocks also acts as a small hierarchical cluster within the larger island-style network ğŸ§©.

---

## Delay Challenges & Solutions âš¡

*   **Major Challenge**: Delay of long wires is **not improving** with process scaling ğŸ“‰. Chip-crossing delay is stagnating or increasing.
*   Leads designers to **increase pipelining** to allow multiple clock cycles for long routes â±ï¸.
*   Solutions to make pipelining more effective:
    *   **Integrating registers within the routing network** ğŸ”Œ.
    *   Intel Stratix 10: Routing drivers can be configured as pulse latches (Fig 6(b)), acting as registers with low delay but poor hold time âš ï¸. Allows deep interconnect pipelining without expensive logic. Cannot use consecutive pulse latches.
    *   Intel Agilex: Integrates actual registers (better hold time) on one-third of drivers (mitigates area cost) âš–ï¸.
    *   Xilinx Versal: Adds bypassable, full-featured registers (with clock enable/clear) only on inputs to function blocks âœ….

---

count: false
class: nord-light, middle, center

# Programmable IO

---

## ğŸŒ Programmable IO - Overview & Challenges ğŸ–¥ï¸

*   FPGAs include **unique programmable IO structures** to communicate with a wide variety of devices ğŸŒ. Make FPGAs the "communications hub".
*   Single set of physical IOs must support many different IO interfaces and standards (voltage levels, electrical characteristics, timing, protocols) âš™ï¸. This is challenging.
*   Programmable IO structures can take up a **large area** (e.g., Stratix II devoted 20-48% of die area to IO) ğŸ“.
*   Highest speed IOs implement **serial protocols** (PCIe, Ethernet) running at 28 Gb/s or more, embedding clock in data âš¡. Require separate differential-only IOs (serial transceivers) with less programmability.
*   IO design is challenging due to **competing demands for speed and programmability** âš–ï¸.
*   Distributing high IO bandwidth requires **wide soft buses** in the fabric, creating further challenges ğŸ›£ï¸.

---

## Techniques (Fig 11) ğŸ› ï¸

*   FPGAs use a combination of approaches for programmable IO:
    1.  **IO Buffers operating across a range of voltages** âš¡: Grouped into banks with separate Vddio rails. Different banks can operate at different voltages.
    2.  **Single-ended or Differential Standards** ğŸ”„: Each IO can be used separately for single-ended, or pairs configured for differential.
    3.  **Programmable Drive Strengths & Termination** ğŸ›ï¸: Multiple parallel pull-up/pull-down transistors allow adjusting drive strength. Programmable on-chip termination resistances minimize signal reflections.
    4.  **Programmable Delay Chains (PDC)** â±ï¸: Allow fine delay adjustments for signal timing.
    5.  **Hardened Digital Circuitry on Input Path** ğŸ’¾: Capture registers, double-to-single data rate (DDR) conversion registers, serial-to-parallel converters. This circuitry can often be bypassed.

---

*   Most FPGAs also contain bypassable higher-level blocks connecting to groups of IOs, implementing protocols like DDR memory controllers ğŸ§ .
*   These approaches allow general-purpose IOs to service many protocols up to ~3.2 Gb/s. Serial IOs have their own hardened circuits for higher speeds (28+ Gb/s) âš¡.

---

count: false
class: nord-light, middle, center

# ğŸ’¾ On-Chip Memory

---

## On-Chip Memory - The Need for Hard Blocks (BRAMs) ğŸ§ 

*   Early FPGAs used FFs in logic blocks for memory ğŸ›ï¸.
*   As logic capacity grew, larger systems needing memory became common ğŸ“ˆ.
*   Building large RAMs out of registers and LUTs is **over 100x less dense** than ASIC-style SRAM blocks âŒ.
*   Application memory needs are diverse (small storage, large buffers, caches, FIFOs, etc.) ğŸ—„ï¸. No single RAM configuration is universal.
*   Challenge: Deciding what kind(s) of RAM blocks to add for efficiency across diverse uses ğŸ¤”.
*   First FPGA with hard memory blocks (**Block RAMs or BRAMs**): Altera Flex 10K in 1995 ğŸš€. Included columns of small (2kb) BRAMs.
*   Gradually incorporated larger/diverse BRAMs.
*   Typical modern FPGA dedicates **~25% of area to BRAMs** ğŸ“Š.

---

## ğŸ›ï¸ BRAM Architecture & Decisions (Fig 12) ğŸ—ï¸

*   FPGA BRAMs consist of an **SRAM-based memory core** plus peripheral circuitry for configurability and routing interface âš™ï¸.
*   SRAM core is a 2D array of cells. Peripheral circuitry orchestrates access (row/column decoders, write drivers, sense amplifiers) ğŸ”.
*   All modern BRAMs **register all inputs** to simplify timing â±ï¸.
*   Main architectural decisions: **Capacity, data word width, number of read/write ports** ğŸ“Š.
*   Larger BRAMs: Lower area per bit (SRAM area grows linearly, periphery/ports sub-linearly) ğŸ“‰. But capacity might be wasted for small RAM needs.
*   Wider BRAMs: Higher data bandwidth, but cost more area due to more periphery (sense amps, write drivers) and routing ports âš–ï¸.
*   More ports: Increases SRAM cell area and periphery area, but increases bandwidth and allows more diverse uses (e.g., FIFOs need 1r+1w) ğŸ”„. Adding a second port to an SRAM cell can increase area by 33% ğŸ“ˆ.

---

## Configurable BRAMs & Routing Interface âš™ï¸

*   Extra configurability is added to BRAMs to adapt to diverse application needs ğŸ”„.
*   Configurable width and depth via **low-cost multiplexing circuitry** in peripherals ğŸ”Œ.
    *   Example: A 4x8-bit array can operate in 8x4, 16x2, 32x1 modes by adding muxes/decoding [53, Fig 12] ğŸ”¢.
    *   Width configurability cost is small relative to SRAM array. Doesn't require additional routing ports.
*   Interface to programmable routing fabric: Designed similarly to logic blocks ğŸ—ï¸.
    *   Connection block muxes (and local crossbars) form input ports ğŸ”Œ.
    *   Read outputs drive switch block muxes for output ports ğŸ”€.
*   Routing interfaces are **costly**, especially for smaller BRAMs (5%-35% of tile area) ğŸ’°. Motivates minimizing routing ports.

---

*   Table I shows port requirements for different BRAM modes. True dual-port (2r/w) requires significantly more ports than simple dual-port (1r+1w) ğŸ“Š.
*   Common approach: True dual-port SRAM core, but only enough routing interfaces for simple-dual port at full width (W) and true dual-port at half width (W/2) âš–ï¸.
*   Multi-pumping (operating BRAMs faster than logic) can implement logical multi-porting with single physical ports, but makes timing closure challenging â±ï¸.

---

## ğŸ’¡ LUT-RAMs ğŸ§ 

*   FPGA vendors add circuitry to repurpose **LUTs in the logic fabric** as additional RAM blocks ğŸ”„.
*   LUTs are naturally read-only memories (truth tables written by bitstream).
*   Adding **designer-controlled write circuitry** turns them into small, distributed **LUT-based RAMs (LUT-RAMs)** âœï¸.
*   Major concern: **Additional routing ports needed for write functionality**.
    *   Example: Altera ALM (6-LUT fracturable into two 5-LUTs, 8 input ports). Can be 64x1 or 32x2 memory. Only 2-3 unused ports, not enough for simple dual-port write signals (8 total) âš ï¸.
*   Solution: Configure an **entire logic block** as LUT-RAM to amortize control circuitry and address bits across multiple ALMs/LUTs ğŸ—ï¸.
    *   Write address/enable signals are broadcast within the block ğŸ“¢.
    *   Limits: A single logic block cannot mix logic and LUT-RAM âŒ.
*   To reduce area cost, only **half of logic blocks** in recent architectures are LUT-RAM capable âš–ï¸.
*   Avoids extra routing ports but still adds some area.

---

## Mapping Logical RAMs (Fig 13) ğŸ—‚ï¸

*   Designs require many different RAMs; must be implemented using fixed BRAM and LUT-RAM resources ğŸ§©.
*   Designers describe logical memories; **vendor CAD tools perform RAM mapping** ğŸ—ºï¸.
*   RAM mapper chooses physical implementation (type, width, depth, ports) ğŸ”.
*   Generates logic to combine multiple BRAMs or LUT-RAMs ğŸ”—.
*   Example (Fig 13): Mapping a 2048x32-bit 2r+1w logical RAM to 1024x8-bit 1r+1w physical BRAMs ğŸ”¢.
    *   Four physical BRAMs in parallel for width â†”ï¸.
    *   Depth-wise stitching of two sets of BRAMs using soft logic ğŸ§µ.
    *   Replicate structure or double-pump for extra read port ğŸ”„.
*   RAM mapping is a complex optimization problem ğŸ¤”.

---

## Trends & Efficiency ğŸ“ˆ

*   FPGA memory architecture has evolved significantly; ratio of memory to logic has grown ğŸ”„.
*   Trend of increasing **memory bits per logic element** (Fig 14) ğŸ“ˆ.
*   Modern BRAMs have larger capacities (20kb) than early ones (2kb) ğŸ—ï¸.
*   Some FPGAs have highly heterogeneous BRAM architectures (e.g., Stratix had 3 types: 512b, 4kb, 512kb) ğŸ§©.
*   LUT-RAM introduction reduced need for small BRAMs ğŸ“‰.
*   Recent Intel/Xilinx architectures converge on LUT-RAM + medium-sized BRAMs (20kb/18kb), sometimes with larger blocks (Xilinx UltraRAM 288kb). No general agreement on best architecture ğŸ¤·.

---

*   **Efficiency Comparison (Table II)** for 2048x72-bit 1r+1w RAM on Stratix IV âš–ï¸.
    *   Mapped to one 144kb BRAM: Best area (0.22 mmÂ²), reasonable Freq (336 MHz) âœ….
    *   Mapped to eighteen 9kb BRAMs: 1.9x larger area, but 1.5x faster (497 MHz) âš¡.
    *   Mapped to LUT-RAM: 12.8x larger area, 40% frequency âš ï¸.
    *   Mapped to Registers: >300x larger area, 40% frequency âŒ.
*   Confirms importance of on-chip RAM. Diversity of RAM resources is needed for different design needs ğŸï¸.
*   Future: Exploring emerging memory technologies (MTJs) for denser BRAMs (~2.95x capacity increase) ğŸ”®.

---

count: false
class: nord-light, middle, center

# ğŸ§® DSP Blocks

---

## ğŸ§® DSP Blocks - Evolution & Multipliers ğŸ”„

*   Initially, only hard arithmetic was carry chains for adders â•. Multipliers were in soft logic (LUTs + carry chains), incurring large area/delay penalty âš ï¸.
*   Signal processing/communications needed high multiplier density. Techniques like multiplier-less distributed arithmetic were used ğŸ›ï¸.
*   **Hardening multipliers** became necessary due to inefficiency in soft logic âœ….
*   Xilinx Virtex-II (2000s): Introduced industry's first **18x18 bit hard multiplier blocks** ğŸš€. Arranged in columns next to BRAMs. Shared interconnect resources to reduce cost, limiting BRAM width.
*   Multiple hard multipliers could be combined for bigger multipliers or FIR filters using soft logic ğŸ”—.

---

## DSP Blocks for Communications/Signal Processing (Fig 15) ğŸ“¡

*   Altera Stratix (2002): Introduced **full-featured DSP blocks** targeting comms/signal processing ğŸ¯.
*   Philosophy: Minimize soft logic for DSP algorithms by hardening more functionality. Enhance flexibility ğŸ› ï¸.
*   Stratix DSP block was highly configurable: eight 9x9, four 18x18, or one 36x36 multiplier ğŸ”¢.
*   Increasing configurability/utility by adding low-cost circuitry to existing structures (Fig 16 showing fracturing an 18x18 into two 9x9s) âœ‚ï¸. Avoids adding costly routing interfaces.

---

*   Stratix DSP also incorporated:
    *   Adder/output block for summation/accumulation â•.
    *   Hardened input registers configurable as shift registers with dedicated cascade interconnect for FIR filters ğŸ”„.
*   Xilinx Virtex-4: Adopted full-featured approach with **DSP48 tiles** (two fixed 18x18 multipliers, input cascades, adder/subtractor/accumulator) ğŸ—ï¸.
*   Virtex-4 introduced cascading adders/accumulators with dedicated interconnect for high-speed **systolic FIR filters** (Fig 17) âš¡.

---

## Efficiency Gains (Table III) ğŸ“Š

*   Hard DSP blocks offer significant efficiency gains over soft logic implementations ğŸ“ˆ.
*   Example: 51-tap symmetric FIR filter on Stratix IV ğŸ”.
    *   With DSPs: Area 0.49 mmÂ², Freq 510 MHz âœ….
    *   Without DSPs (fixed coeff): 3.0x larger area, 0.5x Freq âš ï¸.
    *   Without DSPs (input coeff): 6.2x larger area, 0.4x Freq âŒ.
*   Example: 51-tap asymmetric FIR filter ğŸ”.
    *   With DSPs: Area 0.63 mmÂ², Freq 510 MHz âœ….
    *   Without DSPs (fixed coeff): 3.9x larger area, 0.5x Freq âš ï¸.
    *   Without DSPs (input coeff): 8.5x larger area, 0.4x Freq âŒ.
*   Gains are large but less than the 35x FPGA vs ASIC gap. Due to remaining soft logic, and the programmable routing interfaces/general interconnect in the DSP tile âš–ï¸.
*   Using advanced DSP block features (accumulation, systolic registers) often requires manual instantiation, making designs less portable ğŸ—ï¸.

---

## ğŸ“ˆ Recent DSP Block Evolution ğŸ”„

*   Subsequent generations saw minor changes, focus on fine-tuning for key domains without costly routing additions ğŸ› ï¸.
*   Stratix V simplified DSP block: Two 18x18 or one 27x27 multiplication. Added input pre-adders, embedded coefficient banks for FIR filters (allows implementing whole symmetric FIR filter in DSP) ğŸ›ï¸.
*   Xilinx shifted multiplier widths (18x18 to 25x18 in Virtex-5, 27x18 in Ultrascale). Added input pre-adders, enhanced ALU â•.
*   Evolution was initially driven by communications needs (wireless base stations) ğŸ“¡.

---

## ğŸ§  DSP Blocks for Deep Learning & HPC ğŸ§ 

*   Recent adoption in datacenters and the rise of DL drove DSP block evolution in two directions ğŸ”„.
*   **Direction 1: HPC**: Adding native support for **single-precision floating-point (fp32)** multiplication ğŸ—ï¸.
    *   Before this, fp32 used soft logic or IP cores on fixed-point DSPs + soft logic, a barrier vs CPUs/GPUs âš ï¸.
    *   Intel Arria 10: First native fp32, limited block area increase (10%) by reusing interfaces, maximizing fixed-point hardware reuse âœ….
    *   Xilinx Versal: Will also support floating-point in DSP58 tiles ğŸ”®.

---

*   **Direction 2: DL Inference**: Increasing density of **low-precision integer multiplication** (8-bit and below) ğŸ”¢.
    *   Low-precision arithmetic effective for DL with little accuracy loss ğŸ¯.
    *   FPGAs attractive for custom precision datapaths, energy efficiency, lower development cost ğŸ’°.
    *   Academic work enhanced fracturability for int9/int4 MACs. Increased performance/reduced resource use for DL accelerators ğŸš€.
    *   Intel Agilex: Added int9, fp16, bfloat16 support ğŸ”¢.
    *   Xilinx Versal: Natively supports int8 multiplications ğŸ”¢.

---

## ğŸ¯ Application-Specific DSP Block Architectures ğŸ¯

*   Divergence in requirements (high-precision FP for HPC, medium-precision fixed for comms, low-precision fixed for DL) makes universal DSP blocks harder ğŸ¤”.
*   Leads to **AI-optimized FPGAs** ğŸ§ .
*   Intel Stratix 10 NX: Replaces conventional DSPs with **AI tensor blocks** ğŸš€. Drops legacy support for comms, adopts new modes for DL. Significantly increases int8/int4 MAC density (30/60 per block) ğŸ”¢.
*   Feeding many multipliers without adding ports is key. NX tensor block introduces a **double-buffered data reuse register network** ğŸ”„.
*   Achronix Speedster7t: Includes **Machine Learning Processing (MLP) block** ğŸ—ï¸. Supports wide range of precisions (int16-int3, fp24, fp16, bfloat16) ğŸ”¢.
*   MLP block features tightly coupled BRAM and circular register file for data reuse. Reduces required routing ports ğŸ”Œ.

---

## ğŸŒ System-Level Interconnect: Network-on-Chip (NoC) ğŸ›£ï¸

*   FPGAs increased capacity and external IO bandwidth (DDR, PCIe, Ethernet) ğŸ“ˆ.
*   **Challenge**: Distributing data traffic between high-speed interfaces and the large soft fabric ğŸš¦.
*   Traditionally: **Soft buses** implemented in FPGA logic/routing (multiplexing, arbitration, pipelining, wiring) ğŸ—ï¸.
*   High-speed interfaces run faster than fabric, require **wide soft buses** to match bandwidth. (e.g., 128-bit HBM at 1 GHz needs 1024-bit bus at 250 MHz) âš¡.
*   Wide, physically long soft buses use significant logic/routing resources ğŸ“.
*   Timing closure is challenging; requires **deep pipelining**, further increasing resource use â±ï¸.
*   Problem is worsening with advanced nodes due to poor wire parasitics scaling ğŸ“‰.

---

## ğŸ›£ï¸ Hard NoC Design & Implementations (Fig 18) ğŸ—ï¸

*   Proposed solution: Embedding a **hard, packet-switched network-on-chip (NoC)** in the fabric ğŸš€.
*   A hard NoC with hardened routers and links is **23x more area efficient, 6x faster, and consumes 11% less power** than a soft NoC ğŸ“Š.
*   Challenge: Hardening requires committing choices (routers, link width, topology) while maintaining FPGA flexibility âš–ï¸.
*   Academic work suggests mesh topology, moderate routers (e.g., 16), wide links (128-bit). Keep area <2%, ease layout, carry DDR bandwidth ğŸ—ï¸.
*   **Fabric port**: Interfaces hard NoC routers to programmable fabric, handles width adaptation, clock domain crossing, voltage translation âš™ï¸. Decouples NoC from fabric speed.
*   Hard NoCs well-suited for datacenter FPGAs (shell+role architecture). Significantly improve resource utilization, frequency, routing congestion âœ….
*   Optimized soft NoCs exist but still trail hard NoCs in most respects (bandwidth, latency, area, congestion) âš ï¸.

---

## ğŸš„ Next-Gen Hard NoCs ğŸš€ğŸ’»

*   Recent **Xilinx (Versal) and Achronix (Speedster7t) FPGAs integrate hard NoCs** âœ…ğŸ”Œ ğŸ†•
*   **Xilinx Versal (Fig 18a)**: Uses hard NoC for system-level communication between various endpoints (transceivers, processor, AI subsys, fabric). Only way for external memory to communicate with rest of device ğŸ§©
    *   128-bit links at 1 GHz (matches DDR) âš¡ ğŸ”¢
    *   Topology related to mesh, horizontal links pushed to top/bottom for layout ğŸ“
    *   Multiple rows (horizontal) and columns (vertical) â•
    *   Programmable routing tables configured at boot. Provides standard AXI interfaces âš™ï¸

---

*   **Achronix Speedster7t (Fig 18b)**: Optimized for external interface to fabric transfers ğŸ”„
    *   Peripheral ring around fabric with rows/columns over fabric ğŸ”µ
    *   Peripheral ring can operate independently without configuring fabric ğŸŒ€
    *   No direct row-column connectivity; packets pass through peripheral ring ğŸš«

---

## ğŸ—ï¸ Interposers & Multi-Die Integration ğŸ§©ğŸ”—

*   FPGAs adopted **interposer technology** early for dense interconnection of multiple silicon dice ğŸ›ï¸
*   **Passive interposer**: Silicon die (often trailing process) with metal layers and microbumps connecting dice on top [87, Fig 19a] ğŸ”©
*   Motivations: ğŸ’¡
    1.  **Higher logic capacity**: Large monolithic dice have poor yield. Combining smaller dice on interposer yields better ğŸ“ˆ
    2.  **Heterogeneous Integration**: Integrate different specialized chiplets (different processes) into one system. FPGA fabric bridges functionality/protocols ğŸŒ

---

*   Xilinx Virtex-7/Ultrascale: Use passive silicon interposers to integrate 3-4 FPGA dice. Provide >2x logic elements of largest monolithic devices ğŸ”§
*   Challenge: Interposer microbumps are larger/slower than conventional routing. Do they limit routability? âš ï¸
*   Solutions: CAD tools place logic to minimize interposer crossings. Architecture changes increase switch flexibility to crossing tracks ğŸ› ï¸
*   Versal: Vertical NoC bandwidth crosses between dice, using limited interposer wires efficiently due to high frequency/packet switching ğŸ”„

---

## ğŸ¤ Interposers - EMIB & Chiplets ğŸ”©ğŸš€

*   Intel FPGAs use smaller interposers called **embedded multi-die interconnect bridges (EMIB)** carved into package substrate [89, Fig 19b] ğŸ§Š
*   Intel Stratix 10: Uses EMIB to integrate large FPGA fabric die with smaller IO transceiver or HBM chiplets. Decouples design/process choices ğŸ”Œ
*   Recent studies used EMIB to tightly couple FPGA fabric with specialized **ASIC accelerator chiplets for DL** ğŸ§ 
*   Offloads computation kernels (matrix multiplication) to more efficient ASICs âš¡
*   Leverages FPGA fabric for external interfacing and implementing rapidly changing DL model components ğŸ¤–

---

## âš™ï¸ Other Important Components ğŸ› ï¸ğŸ”§

*   **Configuration Circuitry**: Loads the bitstream into millions of SRAM cells controlling logic, routing, hard blocks. Controller loads serially (flash, PCIe), writes in parallel ğŸ”„
    *   Allows **partial re-configuration** of parts of the device while others run ğŸ”„
    *   **Security**: Bitstream encryption/decryption key programmed by manufacturer to protect IP ğŸ”’
*   **Clock Networks**: Handle dozens of clocks operating at different speeds. Generated by PLLs, DLLs, CDRs â°
    *   Special interconnect networks for clocks ğŸŒ
    *   Use topologies for **low-skew networks** (H-trees) ğŸŒ³
    *   Implemented with wider metal and shielding to reduce crosstalk/jitter ğŸ›¡ï¸

---

## â­ï¸ Conclusion - Progression & Future ğŸš€ğŸ”®

*   FPGAs have progressed from simple programmable arrays to complex heterogeneous multi-die systems with embedded BRAMs, DSPs, IOs, NoCs, and more ğŸ“ˆ
*   Recent adoption in HPC/Datacenters and demand from applications like DL are driving new architecture design â˜ï¸
*   New applications and multi-user datacenter paradigms create opportunities for innovation ğŸ’¡
*   Process technology scaling changes are fundamental: ğŸ”¬
    *   Poor wire delay scaling requires rethinking routing âš¡
    *   Interposers & 3D integration enable new heterogeneous systems ğŸ§©
    *   Controlling power is key; likely leads to more power-gating and heterogeneous hard blocks ğŸ”‹
*   The future of FPGA architecture will be interesting and different! ğŸŒŸ

---

count: false
class: nord-dark, middle, center

.pull-left[
## Q & A ğŸ¤
] .pull-right[
![Discussion](figs/questions-and-answers.svg)
]
