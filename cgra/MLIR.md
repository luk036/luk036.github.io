

# ğŸš€ Understanding Compilers: LLVM IR & MLIR ğŸŒŒ

---

## Welcome! ğŸ‘‹ğŸ‰

*   **Title:** Understanding Compilers: LLVM IR & MLIR
*   **Goal:** Demystify Intermediate Representations (IRs) and how they power modern compilers. ğŸ§ 
*   **Key takeaway:** Reading IR reveals how the compiler *thinks* about your code. ğŸ’­

---

## Why Care About Compiler IRs? ğŸ¤”ğŸ”

*   **For Performance Critical Code:** Understanding your optimizer's IR is a useful tool. âš¡
*   **Beyond Assembly:** ğŸ—ï¸
    *   Reading assembly helps understand what your *processor* is doing. ğŸ’»
    *   Learning LLVM IR helps understand what your *compiler* is doing to create highly optimized code. ğŸ› ï¸
*   **Developer Productivity:** Compilers are fundamental tools that boost developer productivity by automating the conversion between different levels of abstraction. ï¿½

---

## What is LLVM? ğŸ› ï¸ğŸ—ï¸

*   **Umbrella Name:** "LLVM" is an umbrella name for a number of software components used to build compilers. ğŸŒ‚
*   **Flagship Product:** Clang, a high-end C/C++/Objective-C compiler. ğŸ†
*   **Compiler Architecture:** Clang follows the orthodox compiler architecture: ğŸ”„
    1.  **Frontend (Parser):** Parses source code into an Abstract Syntax Tree (AST). ğŸŒ³
    2.  **Lowering:** Converts AST into an **Intermediate Representation (IR)**. â¬‡ï¸
    3.  **Optimizer (Middle-end):** Transforms IR into "better" IR. âœ¨
    4.  **Backend (llc):** Converts IR into machine code for a particular platform. ğŸ’¾

```mermaid
graph LR
    A["Source Code (.c file)"]:::source --> B[AST]:::ast;
    B --> C[LLVM IR]:::ir;
    C --> D[Assembly]:::asm;
    subgraph Clang Frontend
        A --parser:::parse--> B;
        B --lowering:::lower--> C;
    end
    subgraph LLVM
        C -- optimizer (opt):::optimize --> C;
        C -- backend (llc):::compile --> D;
    end

    classDef source fill:#FFD700,stroke:#FFA500,stroke-width:2px,color:#000;
    classDef ast fill:#87CEFA,stroke:#1E90FF,stroke-width:2px;
    classDef ir fill:#98FB98,stroke:#3CB371,stroke-width:2px;
    classDef asm fill:#FFA07A,stroke:#FF6347,stroke-width:2px;
    classDef parse stroke:#9370DB,stroke-width:2px,stroke-dasharray:5,5;
    classDef lower stroke:#40E0D0,stroke-width:2px;
    classDef optimize stroke:#FF69B4,stroke-width:2px;
    classDef compile stroke:#FF8C00,stroke-width:2px;
```
*Diagram based on.* ğŸ“Š

---

## LLVM IR: The Core Language ğŸ“–ğŸ”§

*   **Popular & Well-Documented:** LLVM IR is very popular, well-documented, and reasonably well-specified, allowing it to be treated as a programming language. ğŸ“š
*   **Compilation Target:** It's a very good compilation target due to its documentation and stability, allowing language implementers to reuse thousands of engineering hours. â³
*   **Forms:** LLVM IR exists as a binary format (bitcode) and a human-readable text format (using `.ll` extension). ğŸ’½ğŸ“
    *   Compilers like Clang (`clang++ -S -emit-llvm foo.cc`) and Rust (`rustc --emit=llvm-ir foo.rs`) can emit LLVM IR. ğŸ¯
*   **Strongly Typed:** Unlike most assembly languages, LLVM IR is strongly typed and requires explicit type annotations almost everywhere. ğŸ·ï¸

---

## LLVM IR: Basic Structure - Functions & Types ğŸ“ğŸ§©

*   **Function Definitions:** Introduced with `define`. ğŸ—ï¸
    *   Example: `define void @do_nothing () { ret void }`. ğŸ­
*   **External Symbols:** `declare` brings external symbols (functions without a body) into scope, similar to C. ğŸŒ
*   **Sigils:** User-defined symbols start with a sigil indicating type: ğŸ”£
    *   `@`: Global and functions (addressable, `ptr`-typed when used as a value). ğŸŒ
    *   `%`: Local variables, often called "registers" (LLVM IR is like assembly for an abstract machine with infinite registers). ğŸ’¾
*   **Primitive Types:** ğŸ§±
    *   `iN`: Arbitrary-bit integers (e.g., `i32`, `i64`). No inherent signed/unsigned, defined by instructions. `i1` is used for booleans. ğŸ”¢
    *   `void`: Only used as a return value. ğŸ•³ï¸
    *   `ptr`: Untyped pointer. ğŸ‘‰
*   **Arithmetic Operations:** `add`, `sub`, `mul`, `and`, `or`, `xor`, `shl`, `sdiv`, `udiv`, `srem`, `urem`, `ashr`, `lshr`. â•â–âœ–ï¸
*   **Type Conversion:** `trunc`, `zext` (zero-extend), `sext` (sign-extend). ğŸ”„

---

## LLVM IR: Control Flow & SSA ğŸ”„ğŸ”„

*   **Function Body:** Resembles assembly â€“ a list of labels and instructions. ğŸ“œ
*   **Basic Blocks:** A sequence of non-control flow operations, ending with a control flow instruction. They form the Control Flow Graph (CFG). ğŸ§±
*   **Branch Instruction (`br`):** General branch operation. ğŸŒ¿
    *   Conditional: `br i1 %cond, label %a, label %b` (if/else). ğŸ¤”
    *   Unconditional: `br label %a` (simple goto). ğŸƒ
*   **Switch Instruction (`switch`):** Similar to C switch, easier for LLVM to generate jump tables. ğŸ”€
*   **`unreachable` Instruction:** Represents a codepath the compiler assumes is never executed. Triggers undefined behavior (UB) upon being called. â˜ ï¸
    *   Used in dead code elimination (DCE): `unreachable`s "bubble upwards" in the CFG, dissolving parts of it. ğŸ’€
*   **Static Single Assignment (SSA):** Every register is assigned by at most one instruction per function. âœ‹
    *   **Advantages:** Simplifies optimizations like global value numbering and constant-folding. ğŸ¯
    *   Mutation is reinterpreted as "versions" of a variable (e.g., `%x.1 = add i32 %x.0, %y.0`). ğŸ”„
*   **Phi Nodes (`phi`):** Handle values where control flow merges (e.g., in loops). ğŸ”„
    *   Selects a value based on which basic block was jumped from. ğŸšï¸
    *   Can refer to values not defined in all dominating blocks, allowing dynamic versions. ğŸ­

---

## LLVM IR: Aggregates & Other Operations ğŸ—ï¸ğŸ§©

*   **Aggregate Types:** ğŸ§±
    *   `[n x T]`: Arrays (e.g., `[1024 x i8]`). ğŸ“¦
    *   `{T1, T2, ...}`: Structs (fields indexed, no names). `<{...}>` for packed structs. ğŸ›ï¸
    *   `<n x T>`: Vectors (for SIMD operations). â•
*   **Aggregate Operations:** ğŸ› ï¸
    *   `insertvalue`, `extractvalue`: Statically access/change struct/array fields (does *not* mutate in-place due to SSA). ğŸ‘
*   **`getelementptr` (GEP):** Pointer arithmetic instruction. ğŸ‘‰
    *   Calculates an offset pointer into a struct or array. ğŸ“
    *   Takes an untyped pointer, has an extra index parameter (even for single elements), and explicit types for indices. ğŸ”¢
*   **Function Calls:** `call`, `invoke` (for C++ `try {}` blocks). ğŸ“
*   **Synchronization:** `load atomic`, `store atomic`, `fence`, `cmpxchg` (CAS), `atomicrmw` (read-modify-write). âš›ï¸
    *   Can be marked `volatile` (compiler-invisible side-effects). ğŸ’¥
*   **Reinterpret Operations:** ğŸ”„
    *   `bitcast`: Converts non-aggregate types of same bit width (e.g., `double` to `i64`). Used to cast pointer types before opaque pointers were standard. ğŸ­
    *   `inttoptr`, `ptrtoint`: Convert between pointer and integer data (sketchy semantics). ğŸ¤¹
*   **Intrinsics:** Functions starting with `llvm.` (e.g., `llvm.memcpy`). ğŸ©

---

## LLVM IR: Undefined Behavior (UB) & Poison â˜ ï¸ğŸ’€

*   **Purpose of UB:** LLVM declares certain machine states "impossible" to enable optimizations and simplify code. ğŸ­
*   **Examples:** `unreachable` execution, division by zero, out-of-bounds memory access. ğŸš«
*   **Poisoned Values:** Many LLVM UBs factor through "poisoned values". â˜£ï¸
    *   A poison value can "take on every value at once," convenient for optimization. ğŸ­
    *   Using a poison value (e.g., as a pointer in `load`/`store`/`call`, or as a denominator in `udiv`, or in `br`/`switch`) *must* be UB. â˜ ï¸
    *   LLVM can propagate UB forward through dataflow analysis (e.g., "time traveling UB"). â³
*   **Poison Generation:** Many operations generate poison instead of defined behavior (e.g., `add nsw` for signed overflow). â—
    *   `udiv exact`, `getelementptr inbounds`, `nnan`/`ninf` floating point operations. ğŸŒŠ
*   **Key Distinction:** Creating poison is *not* UB; *only using it is*. This is weaker than C's UB, allowing more reordering and simplification of operational semantics. âš–ï¸

---

## LLVM IR: The Other Side of the Coin ğŸª™âš–ï¸

*   **Centralization:** LLVM IR's role as the central IR for frontends and backends creates a high bar for changing the IR itself. ğŸ¯
    *   **Forks:** Leads to many different flavors of LLVM forks, which are costly to maintain and update. ğŸ”„
*   **Evolution vs. Compatibility:** LLVM IR co-evolves with analyses and transformations, resulting in weak compatibility guarantees. â³
    *   **Hardware Drivers:** Not ideal for device drivers (e.g., in cell phones) where updates are infrequent, leading to compatibility issues. ğŸ“±
    *   **SPIR Example:** Standard Portable Intermediate Representation (SPIR) tried to use LLVM IR but found it unsuitable for bridging software and hardware, leading to SPIR-V. ğŸŒ‰
*   **Domain Specificity:** The fundamental problem is strong coupling across application domains and compiler transformation paths. ğŸ§©
    *   LLVM's design, while good for general languages, struggles to provide top performance for *all* applications and specialized hardware. ğŸ¯

---

## Enter MLIR: Multi-Level IR ğŸŒŸğŸš€

*   **What is MLIR?** An open-source compiler infrastructure project developed as a sub-project of LLVM. ğŸ—ï¸
*   **Purpose:** Provides a modular and extensible IR framework to facilitate building domain-specific compilers and improve compilation for heterogeneous computing platforms. ğŸ§©
*   **Origin:** Developed by Chris Lattner at Google in 2018 to address challenges in building compilers for modern workloads like machine learning, hardware acceleration, and high-level synthesis. ğŸ“…
    *   It was open-sourced as part of the LLVM monorepository in 2019. ğŸ
*   **Multi-Level:** MLIR models computations at various abstraction levels and progressively lowers them toward machine code. â¬‡ï¸
*   **Beyond ML:** While it serves machine learning, MLIR is a general-purpose compiler infrastructure for any domain ("ML" stood for "everything but Machine Learning" initially). ğŸŒ

---

## MLIR Core Concepts: Operations & Regions ğŸ’¡ğŸ§©

*   **Operations (Ops):** The fundamental unit in MLIR. âš™ï¸
    *   Fully extensible; no fixed list of operations. âˆ
    *   Represent concepts from high-level (function definitions, ML graphs) to low-level (target-specific instructions). ğŸ“Š
    *   **SSA-based:** Values are results of exactly one operation or block argument. MLIR avoids traditional PHI nodes by using block arguments in conjunction with control-flow operation operands. ğŸ”„
    *   Can return zero or more results, take zero or more operands, have attributes and properties, and contain regions. ğŸ›ï¸
*   **Regions:** Ordered lists of MLIR Blocks. ğŸ—ï¸
    *   Contained within operations, enabling hierarchical structures. ğŸ›ï¸
    *   **Encapsulation:** Values defined in a region do not escape to the enclosing region. ğŸ›¡ï¸
    *   **Types of Regions:** ğŸ”„
        *   **SSACFG Regions:** Describe control flow between blocks (like traditional basic blocks in LLVM IR). Control flow enters via the "entry" block. ğŸšª
        *   **Graph Regions:** For concurrent semantics without control flow, or modeling generic directed graphs. Only contain a single basic block. ğŸ•¸ï¸

```mermaid
graph TD
    A[ModuleOp]:::module --> B{"Operation (e.g., FuncOp)"}:::func;
    B --> C["Region (e.g., Function Body)"]:::region;
    C --> D["Block (Entry Block)"]:::block;
    D --> E[Operation]:::op;
    E --> F[...]:::ellipsis;
    F --terminator:::term--> G{"Block (Successor)"}:::block;
    G --> E;
    B --can contain:::contain--> C;
    C --contains:::contain--> D & G;
    D & G --contain:::contain--> E & F;

    classDef module fill:#FFD700,stroke:#FFA500,stroke-width:2px,color:#000;
    classDef func fill:#87CEFA,stroke:#1E90FF,stroke-width:2px,color:#000;
    classDef region fill:#98FB98,stroke:#3CB371,stroke-width:2px;
    classDef block fill:#FFA07A,stroke:#FF6347,stroke-width:2px;
    classDef op fill:#9370DB,stroke:#6A5ACD,stroke-width:2px,color:#FFF;
    classDef ellipsis fill:#DDA0DD,stroke:#BA55D3,stroke-width:2px;
    classDef term stroke:#FF69B4,stroke-width:2px;
    classDef contain stroke:#40E0D0,stroke-width:2px;
```
*Diagram based on.* ğŸ“Š

---

## MLIR Core Concepts: Dialects âœ¨ğŸ—£ï¸

*   **The Key to Extensibility:** Dialects are MLIR's primary mechanism for extensibility. ğŸ”‘
*   **Logical Grouping:** A dialect is a self-contained namespace of: ğŸ—‚ï¸
    *   **Operations** âš™ï¸
    *   **Types** ğŸ·ï¸
    *   **Attributes** ğŸ¨
    *   **Other constructs** ğŸ§©
*   **Defining Rules and Semantics:** Dialects provide: ğŸ“œ
    *   **Verifiers** for operation invariants (e.g., `toy.print` must have a single operand). âœ”ï¸
    *   **Semantics** (e.g., `has-no-side-effects`, `constant-folding`, `CSE-allowed`). ğŸ­
    *   **Passes** (analysis, transformations, dialect conversions). ğŸ”„
    *   Possibly custom parser and assembly printer for readability. ğŸ“–
*   **Co-existence:** Multiple dialects can co-exist within one module. ğŸ¤
*   **Examples of Built-in Dialects:** `arith`, `memref`, `affine` (for polyhedral optimization), `scf` (structured control flow), `func`, `gpu`, `tosa`, and even `llvm` (mapping to LLVM IR). ğŸ“š

---

## MLIR: Traits & Interfaces ğŸ¤ğŸ­

*   **Traits:** Mixins that define additional functionality, properties, and verification on an Attribute/Operation/Type. ğŸ¨
    *   Indicate that an operation satisfies certain properties (e.g., `Commutative`, `Terminator`, `ZeroOperands`). âœ”ï¸
    *   Presence checked opaquely by analyses/transformations. ğŸ”
*   **Interfaces:** Abstract classes that allow opaque manipulation of MLIR entities. ğŸ–‡ï¸
    *   Group of methods implemented by an attribute/dialect/operation/type. ğŸ§©
    *   **Cornerstone of Extensibility & Reusability:** Allow generic transformation passes to operate on operations without knowing their actual implementation, relying only on interface properties. â™»ï¸
    *   Example: `CallOpInterface` for callgraph modeling, `ShapeInferenceOpInterface`. ğŸ“
    *   Dialects implement interfaces to enable and reuse generic transformations. ğŸ”„

---

## MLIR: Progressive Lowering â¬‡ï¸ğŸ—ï¸

*   **Multi-level Abstraction:** MLIR enables dialects and operations to model concepts at different abstraction levels (vertically). ğŸ“Š
*   **Seamless Flow:** Different levels can be represented using the same infrastructure, making the flow between them seamless. ğŸŒŠ
*   **Lowering Process:** ğŸ”„
    *   **Dialect Conversion:** The primary mechanism for lowering, converting operations from source dialects into "legal" target dialects. ğŸ”§
        *   **Conversion Target:** Defines legal operations/dialects for the conversion. ğŸ¯
        *   **Operation Conversion:** Dag-to-Dag rewrite patterns transform illegal ops to legal ones. ğŸ› ï¸
        *   **Type Conversion:** Transforms illegal types to legal ones. ğŸ”„
    *   **Modes:** Can be `Partial` (subset of operations converted) or `Full` (all operations converted). â†”ï¸
*   **Benefits:** âœ¨
    *   Separates concerns, allowing each layer to focus on a dedicated task. ğŸ¯
    *   Avoids lowering too early and losing high-level information. â³
    *   Allows mixing levels of abstraction (e.g., Toy dialect ops cohabiting with affine and other ops in the same function). ğŸ¤

---

```mermaid
graph TD
    A["High-Level Lang (e.g., Toy AST)"]:::highLevel --> B(ToyIR Dialect):::toyDialect;
    B --"Inlining, Shape Inference":::optimize--> B;
    B --"ToyToAffineLoweringPass":::lower--> C(Affine Dialect):::affineDialect;
    C --"Standard Affine optimization":::optimize--> C;
    C --"Lowering passes (e.g., AffineToStd, LoopToStd)":::lower--> D(Standard/SCF Dialect):::stdDialect;
    D --"ToyToLLVMLoweringPass":::lower--> E(LLVM Dialect):::llvmDialect;
    E --"translateModuleToLLVMIR":::translate--> F[LLVM IR]:::llvmIR;
    F --"LLVM Backends":::backend--> G[Machine Code]:::machineCode;

    subgraph Compiler Flow
        B -- "MLIR Dialects":::dialectFlow --> E;
    end
    subgraph LLVM Toolchain
        E --> F --> G;
    end

    classDef highLevel fill:#FFD1DC,stroke:#FF69B4,stroke-width:2px;
    classDef toyDialect fill:#B5EAD7,stroke:#2E8B57,stroke-width:2px;
    classDef affineDialect fill:#C7CEEA,stroke:#6A5ACD,stroke-width:2px;
    classDef stdDialect fill:#FFDAC1,stroke:#FF7F50,stroke-width:2px;
    classDef llvmDialect fill:#E2F0CB,stroke:#32CD32,stroke-width:2px;
    classDef llvmIR fill:#B2E2FF,stroke:#1E90FF,stroke-width:2px;
    classDef machineCode fill:#FF9AA2,stroke:#DC143C,stroke-width:2px;
    classDef optimize stroke:#9370DB,stroke-width:3px,stroke-dasharray:5,5;
    classDef lower stroke:#FFA500,stroke-width:3px;
    classDef translate stroke:#40E0D0,stroke-width:3px;
    classDef backend stroke:#FF6347,stroke-width:3px;
    classDef dialectFlow stroke:#BA55D3,stroke-width:3px,stroke-dasharray:3,3;
```
*Diagram inspired by.* ğŸ“Š

---

## MLIR in Action: Ecosystem & Applications ğŸŒğŸš€

*   **Growing Ecosystem:** MLIR has fostered a vibrant open-source ecosystem, production compilers, and experimental toolchains. ğŸŒ±
*   **Key Applications:** ğŸ¯
    *   **Machine Learning:** TensorFlow/XLA, IREE, torch-mlir, ONNX-MLIR, TPU-MLIR, Triton-MLIR. ğŸ¤–
    *   **Systems Programming Languages:** Mojo (integrates Python syntax with low-level performance, built on MLIR). ğŸ
    *   **Hardware Design:** CIRCT project for high-level synthesis. ğŸ–¥ï¸
    *   **Runtimes:** TFRT. âš¡
    *   **Research:** Quantum computing, homomorphic encryption. ğŸ”¬
*   **"Dialects, dialects, dialects":** The infrastructure helps define operations and form logical groups (dialects) based on functionality. ğŸ—£ï¸

---

## The Future: Democratizing AI Compute? ğŸ’­ğŸŒ

*   **Unbundling Compilers & IRs:** MLIR aims to further decouple compilers, breaking down monolithic IRs into mixable dialects. ğŸ§©
    *   Enables defining new "partial" IRs easily. â•
    *   Developing domain-specific compilers could become as simple as choosing, customizing, and mixing existing dialects. ğŸ› ï¸
*   **Challenges:** âš ï¸
    *   Early explosion of AI-specific dialects created fragmentation and made some designs not ideal for evolving requirements (e.g., GenAI, PyTorch support). ğŸ’¥
    *   Identity crisis: Is MLIR a general compiler framework or an AI solution? ğŸ¤”
    *   Governance and corporate rivalries have complicated its evolution towards a unified end-to-end AI solution. ğŸ¢
*   **The Vision:** MLIR seeks to harmonize compute across many different hardware makers and accelerate development of domain-specific solutions. ğŸŒˆ

---

## Key Takeaways & Discussion ğŸ¯ğŸ’¡

*   **LLVM IR:** Powerful, well-documented, SSA-based, but its monolithic nature and weak compatibility can be limiting for specialized domains. âš–ï¸
*   **MLIR:** A framework for *building* compilers, offering multi-level abstraction, extensible dialects, and reusable infrastructure (operations, regions, traits, interfaces). ğŸ—ï¸
*   **The Evolution:** From single, monolithic IRs to modular, decentralized approaches, allowing greater flexibility and domain-specific optimizations. ğŸ”„

---

## Thank You! Questions? ğŸ™â“

*   **MLIR Project Website:** [https://mlir.llvm.org/](https://mlir.llvm.org/) ğŸŒ
*   **LLVM Project Website:** [https://llvm.org/](https://llvm.org/) ğŸŒ
*   **Sources Consulted:** ğŸ“š
    *   "A Gentle Introduction to LLVM IR - mcyoung" ğŸ“–
    *   "Compilers and IRs: LLVM IR, SPIR-V, and MLIR - Lei.Chat()" ğŸ’¬
    *   "MLIR (software) - Wikipedia" ğŸ§ 
    *   "MLIR For Beginners: A series of articles on the MLIR framework - Hacker News" ğŸ†•
    *   "MLIR Language Reference" ğŸ“–
    *   "MLIR dialect design best practise? : r/Compilers - Reddit" ğŸ’¬
    *   "MLIR: Multi-Level Intermediate Representation - LLVM" ğŸ—ï¸
    *   "Open Projects - MLIR - LLVM" ğŸšª
    *   "What about the MLIR compiler infrastructure? (Democratizing AI Compute, Part 8) - Modular" ğŸ¤–
    *   "Writing an LLVM Pass â€” LLVM 21.0.0git documentation" ğŸ“
